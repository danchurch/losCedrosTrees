
## clean gps points

## Ana's GPS points are not working for me. Going to keep some notes on what I think is going on here, 
## and what I do to try to fix.

## problem: when I load the first page of Ana's spread entitled "GPSpointsCedros.xls", 
## the points are far away from where I think they should be. 

## They sort of appear like they are UTM zone 17S, but I think perhaps that (1) E and N are flipped, 
## and (2) that the E column is off by 10^7 meters.

## #2 may be due to the use of a different UTM zone, not sure. But we'll try bringin them into a 
## range that makes sense here by adding 10^7 to all of them 

## #1 we'll just try flipping these axes, E becomes N, vice versa. 


### works! we're in business. 

################################################

## Ana's data is too big for me to see with my 
## spreadsheet software
## Get ana's data out the xls files. Maybe if we can 
## get it into individual sheets, we can manage 
## a little better with our software.
## or even better, get the various sheets into 
## dataframes

sudo pip3 install openpyxl

## py

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## read one of the wkbks in:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']


wb.sheetnames
sheet['A1'].value

## alternatively:

sheet.cell(row=1, column=2).value

## are we missing data?
sheet.max_row
sheet.max_column

openpyxl.utils.get_column_letter(40)

## doesn't look like it. Can we go to a pandas dataframe?

## probably best to delete header, for this exercise

sheet['A1'].value

sheet.delete_rows(1)

sheet['A22'].value

## looks okay. convert to a df?

df = pd.DataFrame(sheet.values)
df.columns = df.iloc[0]
df.drop(0,0, inplace=True)

## yup looks good. 

### try this with the larger excel document 

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb['Cedros bosq Circ plots']

sheet.max_row
sheet.max_column

df2 = pd.DataFrame(sheet.values)

df2.columns = df2.iloc[0]
df2.drop(0,0, inplace=True)

## definitely more reliable than the spreadsheet gui, libreoffice. 
## something really funny is going on with the libreoffice software on this. 
## anyway now  we gotta start really studying what Ana did...

## it's messy, a big pile of spreadsheets. If I had more time I'd 
## maybe try to make a proper database out of all this...

## but I don't
## so, what did they do?

########################################################

## what I can understand about Ana's spreadsheets:

####### BaseDeDatosCedrosEnvaSarah7012012.xlsx ########

#### Sheet 1 - Cedros bosq Circ plots ####

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

sheet = wb['Cedros bosq Circ plots']
cbcpDF = pd.DataFrame(sheet.values)
cbcpDF.columns = cbcpDF.iloc[0]
cbcpDF.drop(0,0, inplace=True)

## this looks like trees, shrubs, woody stuff. Each species is given a count, 

## all species unique? Any repeats?

cbcpDF.ESPECIE.duplicated()

## yup, most. So what does the count indicate?  
## those are not counts. "No de individuo" is a unique identifier for
## each tree 
## good, that will make the SAC easier

## can we isolate this into a single workbook? Then libreoffice might 
## digest it a little better...

#cbcpDF.to_csv('CedrosBosqCircPlots.csv')

## works, libreoffice wasn't loading the full sheet originally. 
## okay, so the circular plots are basically the "large woody plants" data,
## for one of the forest plots? But which type of forest?
## from Ana's report, it seems like there should be one for 
## bosques cerrados and bosques secandarios
## so there should also be a sheet for the gaps and the other type of forest...
## the habitat column all says "BC", so this is probably the 
## closed forest tree data.

cbcpDF.columns

cbcpDF.Hábitat.unique()

cbcpDF.Hábitat.duplicated()

cbcpDF.Hábitat.duplicated()

cbcpAllSpp

## check for number of unique species
## to do this for real, we'll need to deal with all the random white spaces
cbcpAllSpp = cbcpDF['Genero '] + " " + cbcpDF['Especie']
len(cbcpAllSpp.unique())  ## ~139 spp

######## sheet 2, "Cedros reg circ plots " #######

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros reg circ plots "] ## note they included a space in this name
crcpDF = pd.DataFrame(sheet.values)
crcpDF.columns = crcpDF.iloc[0]
crcpDF.drop(0,0, inplace=True)

## whoah, this has 1216 rows... that's a lot of plants...

crcpDF.to_csv("CedrosRegCircPlots.csv") 

crcpDF.Hábitat == "RG"

crcpDF.Hábitat == "BC"

crcpDF.Hábitat.unique()

## the habitat column indicates just two values, "RG" and "RCA"
## what do these mean?

crcpAllSpp = crcpDF['Genero '] + " " + crcpDF['Especie']
len(crcpAllSpp.unique())  ## 197

######## sheet 3, "Cedros Arb juv bosq" ####################

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
sheet = wb["Cedros Arb juv bosq"]

cajbDF = pd.DataFrame(sheet.values)
cajbDF.columns = cajbDF.iloc[0]
cajbDF.drop(0,0, inplace=True)
#cajbDF.to_csv("CedrosArbJuvBosq.csv") 
cajbDF.shape 
## 657 rows, 26 columns, different data
## spreadsheet only picks up ~509 rows
## all of these are from bosque cerrado

cajbDF.Hábitat.unique()

## all "BC"

## so this is the juvenile trees from closed forests, methinks

## it seems like these are botanical collections
## according to Ana's report, collections of juvenile trees 
## were only made in the inner, 5m x 5m parcels. 
## so it seems like this is the 5x5 closed forest plant collections.

cajbAllSpp = cajbDF['Genero '] + " " + cajbDF['Especie']
len(cajbAllSpp.unique())  ## 92

######## sheet 4, "Cedros Arb juv reg " ####################

## again, note the space in the name

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros Arb juv reg "] ## note they included a space in this name
cajrDF = pd.DataFrame(sheet.values)
cajrDF.columns = cajrDF.iloc[0]
cajrDF.drop(0,0, inplace=True)
#cajrDF.to_csv("CedrosArbJuvReg.csv") 

## this looks like the complement to the above (the juvenile trees 
## from the closed forest). It looks like the unique identifiers 
## for parcel are continuous:

cajrDF.columns

## six regular parcels
cajrDF['Parcela/Plot'].unique() ## 1,2,3,4,5,10
cajbDF['Parcela/Plot'].unique() ## 7,6,8,9

cajrDF['Genero ']  ## fucking spaces. I will clean all these up if I do this project

## can we check for number of unique species?
## to do this for real, we'll need to deal with all the random white spaces



cajrAllSpp = cajrDF['Genero '] + " " + cajrDF['Especie']
len(cajrAllSpp.unique()) ## 89


######## sheet 5, "Cedros Arb juv reg " ####################

## this is not a data. But some good stuff here. Some of the habitat types are
## explained:

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

## BC Bosque cerrado
## BS Bosque secundario
## CLB Claro de bosque
## RG regeneración fincas agricultura y ganadería
## RCA Regeneración cañaveral

########## combine observations from 

## can we estimate the total unique tree species collected/observed?

help(pd.concat)

pd.concat

spp = [
cbcpAllSpp,
crcpAllSpp,
cajbAllSpp,
cajrAllSpp
]

sppC = pd.concat(spp)

len(sppC.unique())

type(sppC)
len(sppC)

## 315 species of tree recorded from these plots. 

## we need to start thinking about the proper unit for 
## construct sampling effort curves...

####### other workbook: "CedrosFinalReview.7022012.xlsx" #######

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
#bdfDF.to_csv("CedrosFinalReview.csv") 

bdfDF.columns

## number of unique species here?

bdfSG = bdfDF['Genero '] + ' ' + bdfDF['ESPECIE']

len(bdfSG.unique()) ##337


len(bdfDF.ESPECIE.unique()) ##337

bdfDF.Hábitat.unique() 

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

bdfDF['Final site '].unique()

bdfDF['Parcela'].unique()

bdfDF['Subparcela'].unique()

## pretty similar to above. 

## so again, how do we make a SAC out of this? What are our rows?

## I'm going to guess that the most useful sample unit here is the 30x30
## plot, of which there should be 27, according to the report from 
## Mika. 

## how/where are these labeled in the spreadsheet?

## actually, it looks more like there are 61 sub-parcel/plot

## to get a unique id for these, we need to index both 
## plot and subplot together.

## but what are these sub-plots? the terminology is loose in the 
## explanation, but it looks like a "subplot" corresponds to one
## of the 30x30 events, with both of its smaller, nested plots. 

## I don't understand the placement of the plots. When I color code
## by sub-block, they mostly group together spatially but not 
## entirely. A few random points are far removed in each group.

## doesn't matter for the sac, but it will haunt us later if we don't
## figure that out, methinks.  

## for the moment, let's create a unique ID column and go to vegan

bdfDF.head(2)

PsubP = bdfDF.Parcela.astype(str) + "." + bdfDF.Subparcela.astype(str)

bdfDF['PsubP'] = PsubP

bdfDF.head(2)

## now, we need a simple community matrix, where rows are 
## unique IDs from this PsubP, and columns are the ESPECIE column.

## so maybe let's drop to these, ESPECIE and PsubP:

bdfDF['PsubP']

smallbdf = bdfDF[['PsubP','ESPECIE']]

bdfGroup = smallbdf.groupby('PsubP')
bdfGroup.agg(np.size)
## neat, that's basically the species richness of each subparcel


bdfDummy = pd.get_dummies(smallbdf['ESPECIE'])
## wow, that was easy.

## sanity checks
bdfDummy.iloc[0,] ## yup

bdfDummy.loc[:,'vs. Perebea angustifolia'] ## yup

bdfDummy['Leandra subseriata']

[0:5] ##yup

bdfDummy.iloc[0:3,0:5]

bdfDummy['Turpinia occidentalis (Sw.) G. Don '][2347:2351] 
bdfDummy.iloc[2347:2351,0:5]

## there are some serious whitespace issues with this data. 
## will need some time to clean up species names, etc.
## but for now, let's just get a curve for folks to look at.

## okay, add our PsubP back in there, make it our first column

##bdfDummy['PsubP'] = PsubP
## better:
bdfDummy.insert(0, 'PsubP', PsubP)

## ugh, my head has been out of pandas for a long time...I think we 
## need the grouby function, check this site:
## <https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm>

bdfDummyGroup = bdfDummy.groupby('PsubP')

subParcelComm = bdfDummyGroup.agg(np.sum)

##subParcelComm.to_csv("subParcelComm.csv")

## looks promising. 
## to vegan!



###################################

## R/vegan

R

library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)

## fix rownames 
rownames(subParcelComm) <- subParcelComm$PsubP
subParcelComm <- subParcelComm[,-1]


anaSAC <- specaccum(subParcelComm, method = "exact")

pdf(file="anaSubparcelSAC.pdf")
plot(anaSAC)
dev.off()

specpool(subParcelComm)

##########
    Species     chao  chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     343 516.3946 39.15056 483.6557 21.60728 566.8038 404.4403 10.54889 61
##########

## neat. lots of tree species

## what about some sort of turnover chart?

################

## what do want out of this data set?

## what is ana's hypothesis/goal
## what is mine?

## next steps:

## 1 - clean data 

## and then, not necessarily in this order:

## 2 - ordination - does gap/forest-age explain community?
## 3 - create vegetation zones 
## 4 - show community turnover. This will be perhaps the most important,
## this is pretty much just BC by distance classes. 
## also some sort of mantal's r correlogram
##     because the cloudforest will probably 
## 5 - compare to other places

#####################

## clean data ##

## what spreadsheets do we need? 

## so far, it looks like we can do with just the 
## 2012 data sheet: "Base dat final 6 29 2012"

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)

## basically, we need to hunt for white space. 

bdfDF.columns

## typos here:

## 'Final site ' 
## 'N. Actual '
## 'N. anterior '
## 'Genero '

## from this data, we want to retain:

specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
##rename, without white spaces in the names
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.head(1)

## we got some empty rows. Can we get rid of these?
specObs.tail(3)

aa = specObs.dropna(how='all', axis=0)
aa.shape
## drops 11 rows, is that right? 
## looks right, keep it:

specObs.shape
specObs.tail(15)

specObs.dropna(how='all', axis=0, inplace=True)
specObs.shape
specObs.tail(10)

## how do we search our columns for white space?

## can we apply strip to all the elements of Df?
 
specObs.genero.str.strip()


## that seems to work okay. do this for all string columns

strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()


specObs.head(5)
specObs.head(5)

## looks good. 
## we may need to get rid of accents, etc, in the text,
## but let's wait and see if they cause problems

## let's make a clean species/genus column, and see if our 
## old numbers still match, or if we had a bunch of duplicates
## that typos were making unique:



## what next? we want a physical distance matrix, and a 
## a BC distance matrix. 

specObs['genusSpecies'] = specObs.genero + " " + specObs.especie

specObs.head(2)

specObs['genusSpecies'].unique().shape

## yup, lost like thirty species. 

## this is getting complicated...
## let's get all the specObs creation commands here, at once:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.dropna(how='all', axis=0, inplace=True)
strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()

specObs.especie[specObs.especie.isnull()] = ""
specObs['genusSpecies'] = specObs.familia + " " + specObs.genero + " " + specObs.especie
specObs['genusSpecies'] = specObs['genusSpecies'].str.strip()
specObs['PsubP'] = specObs.parcela.astype('str') + "." + specObs.subparcela.astype('str')
specObs = specObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","habito","elevacion","habitat",]]
specObs.genusSpecies.unique()

specObs.to_csv('specObs.csv')
specObs.to_pickle('specObs.p')

specObs.familia.isnull().any() ## nope
specObs.genero.isnull().any() ## also no empty genero cells. The problem is with the names of species


## we need to 

## there is a species that starts with "vs. "
## wtf?

## okay, let's regenerate our site x species matrix
## with the cleaned data:

smallSpecObs = specObs[['PsubP','genusSpecies']]
smallSpecObsGrouped = smallSpecObs.groupby('PsubP')

spRich=smallSpecObsGrouped.agg(np.size)
## ugh, keeps abbreviating, want to look at all the values
## looks like a max of 86 species at one subparcel
## can we plot this?

plt.ion()
plt.bar(spRich.index, spRich.iloc[:,0])
spRich.iloc[:,0].max() ## 86
spRich.iloc[:,0].min() ## 11
## we'll want to group that by gap/regen groups


## make matrix for SAC
smallSpecObs['genusSpecies']

bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv("subParcelComm.csv")
#subParcelComm.to_pickle('subParcelComm.p')

####### find x,y for all the PsubP values ##########

## read in the csv for the gps data:

spDF = pd.read_csv('GPSpointsCedrosHoja1.csv', parse_dates=True)

## for now, we only want a PsubP column, and N, E coordinates

spDF['PsubP'] = spDF['Parcela/Plot'].astype('str') + "." + spDF['Subparcela  /Subplot'].astype('str') 
spDF = spDF[['PsubP', 'E', 'N']]
spDF.set_index('PsubP', inplace=True)

## does that match? 
#subParcelComm = pd.read_csv("subParcelComm.csv")

subParcelComm = pd.read_pickle("subParcelComm.p")

spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
## indices are not in same order...


spDF.sort_index(inplace=True)
subParcelComm.sort_index(inplace=True)

## check matching order

spDF.index == subParcelComm.index
spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
pd.concat([spDF.PsubP, subParcelComm.PsubP], axis=1)
spDF.set_index('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
len(subParcelComm.PsubP)
len(spDF.PsubP)
## looks okay. 
spDF.to_csv('subParcelSpatial.csv')
spDF.to_pickle('subParcelSpatial.p')

## to R? or can we do our BC distances without vegan, here in python?

import scipy.spatial as sp

spDF.head(2)
subParcelComm.head(2)

## this is really what we need, I think:
physDist = sp.distance.pdist(spDF, metric='euclidean')

sqPhysDist = sp.distance.squareform(physDist)

bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')

physDist.shape
bcDist.shape

## plot these?

plt.scatter(physDist, bcDist)

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

from sklearn.linear_model import LinearRegression

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)


plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## okay, that's cool. 
## we should see if it is more dramatic if we focus 
## on the primary forest plots only



########################################

#### adding to Bitty's list ####

## Bitty maintains a plant list for Los Cedros.
## let's see how many of these are new 
## species for los Cedros

## read in Bitty's spreadsheet:

wb = openpyxl.load_workbook("BittyLosCedrosPlants.xlsx")
wb.sheetnames
sheet = wb["Plants"]
bittyDF = pd.DataFrame(sheet.values)

## it looks like we only need the scientific name and family from this. 
## go to just these:

bittyDF = pd.DataFrame(sheet.values)
bittyDF.columns = bittyDF.iloc[0,:]
bittyDF.drop(0,0, inplace=True) 
bittyDF[['Scientific name', 'Family']].head()
bittyDF=bittyDF[['Scientific name', 'Family']]
bittyDF.dropna(how='all', axis=0, inplace=True)
bittyDF['Scientific name'] = bittyDF['Scientific name'].str.strip()
bittyDF['Family'] = bittyDF['Family'].str.strip()

## check the unique values:

len(bittyDF['Scientific name'].unique())
bittyDF.shape

## how do we compare the Ana data to this?

specObs['genero'].unique()

## this is going to be an imperfect process. But can we whittle the problem 
## down to something manageable?

## split up bitty's scientific name into genus and species:

aa = bittyDF['Scientific name']
bitGen = aa.str.split(pat=' ', expand=True).iloc[:,0]
bitGen.isin(specObs['genero'].unique())

sum(bitGen.isin(specObs['genero'].unique())) ## ~100 genera that are in Bitty's data are in Ana's?

## What are they?

mask = bitGen.isin(specObs['genero'].unique())
len(bitGen[mask])
bitGen[mask]

## and vice versa?
anaGen = pd.Series(specObs['genero'].unique())

## can we get rid of 'sp.' genera?

mask = anaGen.str.contains("sp. *[0-9]+", case=False, regex=True)
anaGen[mask]

## that regex may be useful above for cleaning...

## how to edit this in place...
anaGen = anaGen.str.replace("sp. *[0-9]+", "unidentified", regex=True)

## kick out the unIDs
anaGen = pd.Series(anaGen.unique())

## okay, so are there genera that are represented in Bitty's data?
anaGen.isin(bitGen)

sum(anaGen.isin(bitGen)) ## 64 genera that are in Ana's data are in Bit

## 
~anaGen.isin(bitGen)

sum(~anaGen.isin(bitGen)) ## 74 more genera? can that be true?

## sanity checks
mask = (~anaGen.isin(bitGen)) 
#anaGen[mask]
anaGenNotBit = anaGen[mask]
anaGenNotBit.reset_index(inplace=True, drop=True)

## so we now for certain that all species within these are not on bitty's list.

## now we need to recover the full scientific name and family from Ana's list, 
## and family, and add these to Bitty's data, or at least put it in a form that 
## is easy to add.

## for now, keep it separate to make this easier for her?

specObs[recNotBit] ## NOT in bitty's genera, but yes in ana's

specObs[~recNotBit] ## in bitty's genera and ana's

## how to recover the full records from ana data by genus name:

recNotBit = (~specObs['genero'].isin(bitGen))
newGen = specObs[recNotBit]

## cleanout the genera with just "sp." values:
mask = ~(newGen['genero'].str.contains("sp. *[0-9]*", case=False, regex=True))
newGen[mask].shape
newGen = newGen[mask]

## so this the spreadsheet of new genera, 
## with all of the species within these.

## check 
## this should be ana's but not Bitty's
"Conostegia" in specObs.genero.values
"Conostegia" in bitGen.values
"Conostegia" in newGen.genero.values

## this should be in both
"Dacryodes" in specObs.genero.values
"Dacryodes" in bitGen.values
"Dacryodes" in newGen.genero.values

## seems to work. From newGen, we need the family, genus, species columns:
newGen = newGen[['familia', 'genero', 'especie']]

## how to we subset to just unique species values?

newGenGrouped = newGen.groupby(['genero','especie']).agg(pd.Series.unique)

newGen.head()
newGenGrouped.head()
newGenGrouped.shape

## what does a csv look like if we use this?

newGenGrouped.to_csv('newGeneraForBitty.csv')

#### ok, so how do we get the other plants? #####

## these other species will not be in the new genera, 
## so we need to look in the genera that are already
## covered. 

## This gets messy, probably, but the start is simple:

## these are the records that are of genera that are 
## in both Bitty's plant list and Ana's:

sharedGen = specObs[~recNotBit] 

## subset to family, genus, species
sharedGen = sharedGen[['familia', 'genero', 'especie']]

## collapse it:
sharedGen = sharedGen.groupby(['genero','especie']).agg(pd.Series.unique)

## now how to avoid duplicates with bitty's list?

## let's try joining our genus+species column, and matching to
## Bitty's scientific names:

sharedGen = sharedGen.reset_index()[["familia","genero","especie"]]

sharedGen['GenSpec'] = sharedGen.genero.str.cat(sharedGen.especie, sep=' ')

## search Bitty's df:

sharedGen['GenSpec'].isin(bittyDF['Scientific name'])

sum(sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 44 matches.

sum(~sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 153 spp not listed in Bitty's list

## sanity checks
"Rhodostemonodaphne cyclops" in bittyDF['Scientific name'].values
"Trichilia glabra" in bittyDF['Scientific name'].values

## okay, so how do we subset specObs to these?
newSpp = sharedGen[(~sharedGen['GenSpec'].isin(bittyDF['Scientific name']))]
mask = ~(newSpp['GenSpec'].str.contains("sp. *[0-9]*", case=False, regex=True))
newSpp = newSpp[mask]

## I think this is what bitty needs?
newSpp.to_csv('oldGeneraDifferentSpp4bitty.csv')

## bitty has noticed a particular record of interest. Get her the full story:



############## ridgetop paper data ########################

## there was an additional paper about biodiversity at los Cedros,
## and elsewhere, Wilson and Rhemtulla (2018).

## their data is public, let's look at it:

wb = openpyxl.load_workbook("doi_10.5061_dryad.3nr41__v1/prescence_abscence.xlsx")

wb.sheetnames
sheet = wb["Sheet1"]
ridgeDF = pd.DataFrame(sheet.values)

ridgeDF.rename(columns=ridgeDF.iloc[1,:], inplace=True)

ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.reset_index(inplace=True, drop=True)

columns=ridgeDF.iloc[1,:], inplace=True)

#aa = ridgeDF.columns.values
aa = ridgeDF.iloc[1,:], inplace=True)aa[0] = "family"
aa[1] = "gnusSpecies"

ridgeDF.columns[0]='Family'

## which is LC?

## and never mind, Bitty just informed me she has done all this work.

######################################

## okay, back to the turnover graphs...

## can we back up and redo the turnover graphs, split out 
## by habitat type?

## we need to split up both the specObs and spDF dataframes by 
## habitat. How

spDF = pd.read_pickle('subParcelSpatial.p')
specObs = pd.read_pickle('specObs.p')

## we want a set of smaller data frames from specObs, based on their habitat:
soGrouped = specObs.groupby('habitat')

soGrouped.groups.keys()

oldForestObs = soGrouped.get_group('BC')

oldForestObs.head()

oldForestObs.PsubP.unique() ## 12 subparcels of old forest. Is this enough?

## how can we add spatial info?
ofoN = [ spDF.loc[i].N for i in oldForestObs.PsubP ]
ofoE = [ spDF.loc[i].E for i in oldForestObs.PsubP ]
oldForestObs['N'] = ofoN
oldForestObs['E'] = ofoE

oldForestObs.head()

oldForestObs.tail()

## we need a species matrix for just this set of plots:

subParcelComm = pd.read_pickle("subParcelComm.p")
aa = subParcelComm.index.isin(oldForestObs.PsubP)
oldForestComm = subParcelComm[aa]

## any rows all-zeros?

(oldForestComm == 0).all(axis=0) ## lots of species that are not present in any of these plots
sum((oldForestComm == 0).all(axis=0)) ## 190, out of 343, species not represented in these
## but that's not bad, considering the ratio, this is just 1/5 of all plots, but 45% of tree spp
(oldForestComm == 0).all(axis=1) 
## no rows that have zero in all species. Makes sense, that would mean a 
## plot with no trees, classified as bosque cerrado

## I think it makes sense to remove these all-zero species columns:

(oldForestComm == 0).all(axis=0).values

notObserved=~((oldForestComm == 0).all(axis=0).values)

oldForestComm = oldForestComm.iloc[:,notObserved]

oldForestComm.shape ## 12,153 . Makes sense. 

(oldForestComm == 0).all(axis=1).any() ## no species that are not present at least somewhere

## okay, I think we can do our turnover graph

oldForestDF = spDF.loc[oldForestComm.index.values]

## got to get a spatial matrix just for the oldforest..


physDist = sp.distance.pdist(oldForestDF, metric='euclidean')
bcDist = sp.distance.pdist(oldForestComm, metric='brayCurtis')

physDist.shape
bcDist.shape

plt.ion()

plt.scatter(physDist, bcDist)

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

plt.xlabel('distance between subplots')
plt.ylabel('BC dissimilarity')
plt.title('old Forest')
plt.savefig('oldForestTurn.png')

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## works. can this be generalized to the other habitat types?

###### general turnover pipeline #########

specObs = pd.read_pickle('specObs.p')
spDF = pd.read_pickle('subParcelSpatial.p')
subParcelComm = pd.read_pickle("subParcelComm.p")

## get our original in there first:
fig, axes = plt.subplots(nrows=2, ncols=3, sharey=True)
axes = axes.flatten()

physDist = sp.distance.pdist(spDF, metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
axes[0].scatter(physDist, bcDist)
axes[0].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## now look at these individually:
soGrouped = specObs.groupby('habitat')
habs = list(soGrouped.groups.keys())


for h,i in enumerate(habs): 
    print(i)
    obs_i = soGrouped.get_group(i)
    obs_iN = [ spDF.loc[a].N for a in obs_i.PsubP ]
    obs_iE = [ spDF.loc[a].E for a in obs_i.PsubP ]
    obs_i['N'] = obs_iN
    obs_i['E'] = obs_iE
    subParcelInObs_i = subParcelComm.index.isin(obs_i.PsubP)
    comm_i = subParcelComm[subParcelInObs_i]
    notObserved=~((comm_i == 0).all(axis=0).values)
    comm_i = comm_i.iloc[:,notObserved]
    spDF_i = spDF.loc[comm_i.index.values]
    physDist_i = sp.distance.pdist(spDF_i, metric='euclidean')
    bcDist_i = sp.distance.pdist(comm_i, metric='brayCurtis')
    axes[h+1].scatter(physDist_i, bcDist_i)
    X, Y = physDist_i.reshape(-1,1), bcDist_i.reshape(-1,1)
    axes[h+1].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 
    axes[h+1].set_xlabel('distance between subplots')
    axes[h+1].set_ylabel('BC dissimilarity')
    axes[h+1].set_title(f'{i} habitat turnover')

plt.tight_layout()

plt.savefig('turnoverByHabitat.png')

## well, withing the scale of comparisons that we have,
## this is not that interesting, except to say, damn, 
## these habitats are not good predictors of the 
## massive biodiversity that we are seeing here. 

## is it worth our time to do some mem analysis on this?
## what would be the advantages..?

## something to chew on...

#############################################################

## while we're here, SAC for each, to compare
## alpha diversity?

## what do we need for this?

## we need the community matrix for each habitat type, 
## exported as a csv, so we can head over to R and get into 
## vegan

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## grab the pipeline for the community matrix above:

specObs = pd.read_pickle('specObs.p')
spDF = pd.read_pickle('subParcelSpatial.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
smallDF = specObs[['PsubP','genusSpecies']]
dfDummy = pd.get_dummies(smallDF['genusSpecies'])
bdfDummy.insert(0, 'PsubP', specObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)

## to do one habitat type, we want a column that shows 
## the habitat type for each PsubP

aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)

## now groupby this?
bbGrouped = bb.groupby('habitat')

bbGrouped.groups.keys()


cc = bbGrouped.get_group('BC')

~(cc == 0).all(axis=0) ## or...

(cc != 0).any(axis=0)

observed = (cc != 0).any(axis=0).values
dd = cc.iloc[:,observed]

dd.drop('habitat', axis=1, inplace=True)

dd.to_csv('BC_comm.csv')


## let's generalize that to create community dataframes for each habitat
## type:


aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)
bbGrouped = bb.groupby('habitat')
habs = list(bbGrouped.groups.keys())

for i in habs:
    cc = bbGrouped.get_group(i)
    observed = (cc != 0).any(axis=0).values
    dd = cc.iloc[:,observed]
    dd.drop('habitat', axis=1, inplace=True)
    dd.to_csv(f'{i}_comm.csv')

## in R

library(vegan)
library(RColorBrewer)

subParcelComm <- read.csv('BC_comm.csv', header=TRUE, row.names=1)
SAC <- specaccum(subParcelComm, method = "exact")
#pdf(file="BC_SAC.pdf")



plot(SAC)
#dev.off()
specpool(subParcelComm)

    Species     chao chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     137 250.2274  36.241 207.5833    22.91 253.5682 167.1898 10.28604 12

## so ~half of the estimated species will come from this type of forest.

## okay, can we repeat for the others, and build a single diagram out of 
## this?

#pdf("comboSACs.pdf")
par(mfrow = c(2,3))
files <- list.files()
comm <- grep('_comm.csv', list.files())
comms <- files[comm]
comms <- c("subParcelComm.csv", comms)
comtitles <- sub("_comm.csv", "", comms)
comtitles[1] <- "All habs"
j <- 0
for (i in comms){
    j <- j + 1
    comm.i <- read.csv(i, header=TRUE, row.names=1)
    print(i)
    SAC <- specaccum(comm.i, method = "exact")
    plot(SAC, main=comtitles[j], ylab='No. of Spp')
    capture.output(print(paste('Species estimates for', comtitles[j], sep=" ")), 
                    file="habSRestimates.txt", append = TRUE)
    capture.output(specpool(comm.i), 
                    file="habSRestimates.txt",
                    append = TRUE)
    sacDF <- data.frame(cbind(SAC$richness, SAC$sd), row.names=SAC$sites)
    colnames(sacDF) <- c('richness', 'sd')
    write.csv(sacDF, file=paste(comtitles[j], "SAC.csv", sep="_"))
}
#dev.off()

## so now we can go back to python, plot these together. 
## so we need to export appropriate data. These will be CSVs 
## of the sites, richness, and SD.

######## side note - NMS #########
## while we're in here, can we do an NMS of the habitat types,
## see if they group together?

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

## standard discrete color palette will work?

nms <- metaMDS(sPC, try=40)

## make plotting dataframe
specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat) 
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

head(nmsInfo)

## can we see what this looks like?

pdf(file='primenetNMSbyHabitat.pdf')
plot(nondups$MDS1, nondups$MDS2, 
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
## looks okay. How do we add hulls again?
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)
dev.off()

names(table(nmsInfo$habitat))

names(table(nmsInfo$colrs))

## do some checks

nmsInfo[identify(nondups$MDS1, nondups$MDS2),]


data(dune)
attach(dune.env)
ord <- metaMDS(dune)
plot(ord)
ordihull(ord, Management, col=1:4, lwd=3)
detach(dune.env)



names(table(nmsInfo$habitat))
names(table(nmsInfo$colrs))

nmsInfo$colrs

plot(1, col="#FF7F00", pch=19, cex=5)
##################################

## back to python, plot SAC curves

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
plt.ion()

## find our csvs of interest
SACs = [ i for i in os.listdir() if "SAC.csv" in i ]
sacDF = pd.read_csv('RCA_SAC.csv', index_col=0)

## plot this

fig, ax = plt.subplots(1,1)

def oneSacPlot(DF, clr):
    sacDF = pd.read_csv(DF, index_col=0)
    plt.plot(sacDF['richness'],
                color=clr)
    plt.fill_between(x=sacDF.index,
                     y1=sacDF.richness - sacDF.sd,
                     y2=sacDF.richness + sacDF.sd,
                    alpha=0.4,
                    color=clr,
                    )

colors = ['blue', 'green', 'red', 'yellow', 'orange', 'brown']

for colNu,DF in enumerate(SACs):
    oneSacPlot(DF, colors[colNu])


## the goal now is to plot these 

## make SACs

## make some useful SAC combo plots...then what? 

## indicator species for each type of habitat?

## the general story - deforestation knocks back a landscape into
## a pluripotent state? You may well end up with a very different

## wehere did the bosque secondario come from? Is it natural disturbance?
## blowdown, etc?
## or is it from deforestation? 

## If it is natural, then I think the story here is that deforestation
## changes the trajectory of succession...you may not recover your
## old growth, even with time, if you cut your forest. You will with 
## "natural" disturbance 

## would be nice to get the ages on

