
## clean gps points

## Ana's GPS points are not working for me. Going to keep some notes on what I think is going on here, 
## and what I do to try to fix.

## problem: when I load the first page of Ana's spread entitled "GPSpointsCedros.xls", 
## the points are far away from where I think they should be. 

## They sort of appear like they are UTM zone 17S, but I think perhaps that (1) E and N are flipped, 
## and (2) that the E column is off by 10^7 meters.

## #2 may be due to the use of a different UTM zone, not sure. But we'll try bringin them into a 
## range that makes sense here by adding 10^7 to all of them 

## #1 we'll just try flipping these axes, E becomes N, vice versa. 


### works! we're in business. 

################################################

## Ana's data is too big for me to see with my 
## spreadsheet software
## Get ana's data out the xls files. Maybe if we can 
## get it into individual sheets, we can manage 
## a little better with our software.
## or even better, get the various sheets into 
## dataframes

sudo pip3 install openpyxl

## py

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

## read one of the wkbks in:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']


wb.sheetnames
sheet['A1'].value

## alternatively:

sheet.cell(row=1, column=2).value

## are we missing data?
sheet.max_row
sheet.max_column

openpyxl.utils.get_column_letter(40)

## doesn't look like it. Can we go to a pandas dataframe?

## probably best to delete header, for this exercise

sheet['A1'].value

sheet.delete_rows(1)

sheet['A22'].value

## looks okay. convert to a df?

df = pd.DataFrame(sheet.values)
df.columns = df.iloc[0]
df.drop(0,0, inplace=True)

## yup looks good. 

### try this with the larger excel document 

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb['Cedros bosq Circ plots']

sheet.max_row
sheet.max_column

df2 = pd.DataFrame(sheet.values)

df2.columns = df2.iloc[0]
df2.drop(0,0, inplace=True)

## definitely more reliable than the spreadsheet gui, libreoffice. 
## something really funny is going on with the libreoffice software on this. 
## anyway now  we gotta start really studying what Ana did...

## it's messy, a big pile of spreadsheets. If I had more time I'd 
## maybe try to make a proper database out of all this...

## but I don't
## so, what did they do?

########################################################

## what I can understand about Ana's spreadsheets:

####### BaseDeDatosCedrosEnvaSarah7012012.xlsx ########

#### Sheet 1 - Cedros bosq Circ plots ####

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

sheet = wb['Cedros bosq Circ plots']
cbcpDF = pd.DataFrame(sheet.values)
cbcpDF.columns = cbcpDF.iloc[0]
cbcpDF.drop(0,0, inplace=True)

## this looks like trees, shrubs, woody stuff. Each species is given a count, 

## all species unique? Any repeats?

cbcpDF.ESPECIE.duplicated()

## yup, most. So what does the count indicate?  
## those are not counts. "No de individuo" is a unique identifier for
## each tree 
## good, that will make the SAC easier

## can we isolate this into a single workbook? Then libreoffice might 
## digest it a little better...

#cbcpDF.to_csv('CedrosBosqCircPlots.csv')

## works, libreoffice wasn't loading the full sheet originally. 
## okay, so the circular plots are basically the "large woody plants" data,
## for one of the forest plots? But which type of forest?
## from Ana's report, it seems like there should be one for 
## bosques cerrados and bosques secandarios
## so there should also be a sheet for the gaps and the other type of forest...
## the habitat column all says "BC", so this is probably the 
## closed forest tree data.

cbcpDF.columns

cbcpDF.Hábitat.unique()

cbcpDF.Hábitat.duplicated()

cbcpDF.Hábitat.duplicated()

cbcpAllSpp

## check for number of unique species
## to do this for real, we'll need to deal with all the random white spaces
cbcpAllSpp = cbcpDF['Genero '] + " " + cbcpDF['Especie']
len(cbcpAllSpp.unique())  ## ~139 spp

######## sheet 2, "Cedros reg circ plots " #######

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros reg circ plots "] ## note they included a space in this name
crcpDF = pd.DataFrame(sheet.values)
crcpDF.columns = crcpDF.iloc[0]
crcpDF.drop(0,0, inplace=True)

## whoah, this has 1216 rows... that's a lot of plants...

crcpDF.to_csv("CedrosRegCircPlots.csv") 

crcpDF.Hábitat == "RG"

crcpDF.Hábitat == "BC"

crcpDF.Hábitat.unique()

## the habitat column indicates just two values, "RG" and "RCA"
## what do these mean?

crcpAllSpp = crcpDF['Genero '] + " " + crcpDF['Especie']
len(crcpAllSpp.unique())  ## 197

######## sheet 3, "Cedros Arb juv bosq" ####################

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
sheet = wb["Cedros Arb juv bosq"]

cajbDF = pd.DataFrame(sheet.values)
cajbDF.columns = cajbDF.iloc[0]
cajbDF.drop(0,0, inplace=True)
#cajbDF.to_csv("CedrosArbJuvBosq.csv") 
cajbDF.shape 
## 657 rows, 26 columns, different data
## spreadsheet only picks up ~509 rows
## all of these are from bosque cerrado

cajbDF.Hábitat.unique()

## all "BC"

## so this is the juvenile trees from closed forests, methinks

## it seems like these are botanical collections
## according to Ana's report, collections of juvenile trees 
## were only made in the inner, 5m x 5m parcels. 
## so it seems like this is the 5x5 closed forest plant collections.

cajbAllSpp = cajbDF['Genero '] + " " + cajbDF['Especie']
len(cajbAllSpp.unique())  ## 92

######## sheet 4, "Cedros Arb juv reg " ####################

## again, note the space in the name

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros Arb juv reg "] ## note they included a space in this name
cajrDF = pd.DataFrame(sheet.values)
cajrDF.columns = cajrDF.iloc[0]
cajrDF.drop(0,0, inplace=True)
#cajrDF.to_csv("CedrosArbJuvReg.csv") 

## this looks like the complement to the above (the juvenile trees 
## from the closed forest). It looks like the unique identifiers 
## for parcel are continuous:

cajrDF.columns

## six regular parcels
cajrDF['Parcela/Plot'].unique() ## 1,2,3,4,5,10
cajbDF['Parcela/Plot'].unique() ## 7,6,8,9

cajrDF['Genero ']  ## fucking spaces. I will clean all these up if I do this project

## can we check for number of unique species?
## to do this for real, we'll need to deal with all the random white spaces



cajrAllSpp = cajrDF['Genero '] + " " + cajrDF['Especie']
len(cajrAllSpp.unique()) ## 89


######## sheet 5, "Cedros Arb juv reg " ####################

## this is not a data. But some good stuff here. Some of the habitat types are
## explained:

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

## BC Bosque cerrado
## BS Bosque secundario
## CLB Claro de bosque
## RG regeneración fincas agricultura y ganadería
## RCA Regeneración cañaveral

########## combine observations from 

## can we estimate the total unique tree species collected/observed?

help(pd.concat)

pd.concat

spp = [
cbcpAllSpp,
crcpAllSpp,
cajbAllSpp,
cajrAllSpp
]

sppC = pd.concat(spp)

len(sppC.unique())

type(sppC)
len(sppC)

## 315 species of tree recorded from these plots. 

## we need to start thinking about the proper unit for 
## construct sampling effort curves...

####### other workbook: "CedrosFinalReview.7022012.xlsx" #######

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
#bdfDF.to_csv("CedrosFinalReview.csv") 

bdfDF.columns

## number of unique species here?

bdfSG = bdfDF['Genero '] + ' ' + bdfDF['ESPECIE']

len(bdfSG.unique()) ##337


len(bdfDF.ESPECIE.unique()) ##337

bdfDF.Hábitat.unique() 

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

bdfDF['Final site '].unique()

bdfDF['Parcela'].unique()

bdfDF['Subparcela'].unique()

## pretty similar to above. 

## so again, how do we make a SAC out of this? What are our rows?

## I'm going to guess that the most useful sample unit here is the 30x30
## plot, of which there should be 27, according to the report from 
## Mika. 

## how/where are these labeled in the spreadsheet?

## actually, it looks more like there are 61 sub-parcel/plot

## to get a unique id for these, we need to index both 
## plot and subplot together.

## but what are these sub-plots? the terminology is loose in the 
## explanation, but it looks like a "subplot" corresponds to one
## of the 30x30 events, with both of its smaller, nested plots. 

## I don't understand the placement of the plots. When I color code
## by sub-block, they mostly group together spatially but not 
## entirely. A few random points are far removed in each group.

## doesn't matter for the sac, but it will haunt us later if we don't
## figure that out, methinks.  

## for the moment, let's create a unique ID column and go to vegan

bdfDF.head(2)

PsubP = bdfDF.Parcela.astype(str) + "." + bdfDF.Subparcela.astype(str)

bdfDF['PsubP'] = PsubP

bdfDF.head(2)

## now, we need a simple community matrix, where rows are 
## unique IDs from this PsubP, and columns are the ESPECIE column.

## so maybe let's drop to these, ESPECIE and PsubP:

bdfDF['PsubP']

smallbdf = bdfDF[['PsubP','ESPECIE']]

bdfGroup = smallbdf.groupby('PsubP')
bdfGroup.agg(np.size)
## neat, that's basically the species richness of each subparcel


bdfDummy = pd.get_dummies(smallbdf['ESPECIE'])
## wow, that was easy.

## sanity checks
bdfDummy.iloc[0,] ## yup

bdfDummy.loc[:,'vs. Perebea angustifolia'] ## yup

bdfDummy['Leandra subseriata']

[0:5] ##yup

bdfDummy.iloc[0:3,0:5]

bdfDummy['Turpinia occidentalis (Sw.) G. Don '][2347:2351] 
bdfDummy.iloc[2347:2351,0:5]

## there are some serious whitespace issues with this data. 
## will need some time to clean up species names, etc.
## but for now, let's just get a curve for folks to look at.

## okay, add our PsubP back in there, make it our first column

##bdfDummy['PsubP'] = PsubP
## better:
bdfDummy.insert(0, 'PsubP', PsubP)

## ugh, my head has been out of pandas for a long time...I think we 
## need the grouby function, check this site:
## <https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm>

bdfDummyGroup = bdfDummy.groupby('PsubP')

subParcelComm = bdfDummyGroup.agg(np.sum)

##subParcelComm.to_csv("subParcelComm.csv")

## looks promising. 
## to vegan!



###################################

## R/vegan

R

library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)

## fix rownames 
rownames(subParcelComm) <- subParcelComm$PsubP
subParcelComm <- subParcelComm[,-1]


anaSAC <- specaccum(subParcelComm, method = "exact")

pdf(file="anaSubparcelSAC.pdf")
plot(anaSAC)
dev.off()

specpool(subParcelComm)

##########
    Species     chao  chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     343 516.3946 39.15056 483.6557 21.60728 566.8038 404.4403 10.54889 61
##########

## neat. lots of tree species

## what about some sort of turnover chart?

################

## what do want out of this data set?

## what is ana's hypothesis/goal
## what is mine?

## next steps:

## 1 - clean data 

## and then, not necessarily in this order:

## 2 - ordination - does gap/forest-age explain community?
## 3 - create vegetation zones 
## 4 - show community turnover. This will be perhaps the most important,
## this is pretty much just BC by distance classes. 
## also some sort of mantal's r correlogram
##     because the cloudforest will probably 
## 5 - compare to other places

#####################

## clean data ##

## what spreadsheets do we need? 

## so far, it looks like we can do with just the 
## 2012 data sheet: "Base dat final 6 29 2012"

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)

## basically, we need to hunt for white space. 

bdfDF.columns

## typos here:

## 'Final site ' 
## 'N. Actual '
## 'N. anterior '
## 'Genero '

## from this data, we want to retain:

specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
##rename, without white spaces in the names
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.head(1)

## we got some empty rows. Can we get rid of these?
specObs.tail(3)

aa = specObs.dropna(how='all', axis=0)
aa.shape
## drops 11 rows, is that right? 
## looks right, keep it:

specObs.shape
specObs.tail(15)

specObs.dropna(how='all', axis=0, inplace=True)
specObs.shape
specObs.tail(10)

## how do we search our columns for white space?

## can we apply strip to all the elements of Df?
 
specObs.genero.str.strip()


## that seems to work okay. do this for all string columns

strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()


specObs.head(5)
specObs.head(5)

## looks good. 
## we may need to get rid of accents, etc, in the text,
## but let's wait and see if they cause problems

## let's make a clean species/genus column, and see if our 
## old numbers still match, or if we had a bunch of duplicates
## that typos were making unique:



## what next? we want a physical distance matrix, and a 
## a BC distance matrix. 

specObs['genusSpecies'] = specObs.genero + " " + specObs.especie

specObs.head(2)

specObs['genusSpecies'].unique().shape

## yup, lost like thirty species. 

## this is getting complicated...
## let's get all the specObs creation commands here, at once:
wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.dropna(how='all', axis=0, inplace=True)
strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()

specObs.especie[specObs.especie.isnull()] = ""
specObs['genusSpecies'] = specObs.familia + " " + specObs.genero + " " + specObs.especie
specObs['genusSpecies'] = specObs['genusSpecies'].str.strip()
specObs['PsubP'] = specObs.parcela.astype('str') + "." + specObs.subparcela.astype('str')
specObs = specObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","habito","elevacion","habitat",]]

#specObs.to_csv('specObs.csv')
specObs.to_pickle('specObs.p')

specObs.familia.isnull().any() ## nope
specObs.genero.isnull().any() ## also no empty genero cells. The problem is with the names of species


## we need to 

## there is a species that starts with "vs. "
## wtf?

## okay, let's regenerate our site x species matrix
## with the cleaned data:

smallSpecObs = specObs[['PsubP','genusSpecies']]
smallSpecObsGrouped = smallSpecObs.groupby('PsubP')

spRich=smallSpecObsGrouped.agg(np.size)
## ugh, keeps abbreviating, want to look at all the values
## looks like a max of 86 species at one subparcel
## can we plot this?

plt.ion()
plt.bar(spRich.index, spRich.iloc[:,0])
spRich.iloc[:,0].max() ## 86
spRich.iloc[:,0].min() ## 11
## we'll want to group that by gap/regen groups


## make matrix for SAC
smallSpecObs['genusSpecies']

bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv("subParcelComm.csv")
#subParcelComm.to_pickle('subParcelComm.p')

####### find x,y for all the PsubP values ##########

## read in the csv for the gps data:

spDF = pd.read_csv('GPSpointsCedrosHoja1.csv', parse_dates=True)

## for now, we only want a PsubP column, and N, E coordinates

spDF['PsubP'] = spDF['Parcela/Plot'].astype('str') + "." + spDF['Subparcela  /Subplot'].astype('str') 
spDF = spDF[['PsubP', 'E', 'N']]
spDF.set_index('PsubP', inplace=True)

## does that match? 

#subParcelComm = pd.read_csv("subParcelComm.csv")

subParcelComm = pd.read_pickle("subParcelComm.p")

spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
## indices are not in same order...


spDF.sort_index(inplace=True)
subParcelComm.sort_index(inplace=True)

## check matching order

spDF.index == subParcelComm.index
spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
pd.concat([spDF.PsubP, subParcelComm.PsubP], axis=1)
spDF.set_index('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
len(subParcelComm.PsubP)
len(spDF.PsubP)
## looks okay. 
spDF.to_csv('subParcelSpatial.csv')
spDF.to_pickle('subParcelSpatial.p')

## to R? or can we do our BC distances without vegan, here in python?

import scipy.spatial as sp

spDF.head(2)
subParcelComm.head(2)

## this is really what we need, I think:
physDist = sp.distance.pdist(spDF, metric='euclidean')

sqPhysDist = sp.distance.squareform(physDist)

bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')

physDist.shape
bcDist.shape

## plot these?

plt.scatter(physDist, bcDist)

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

from sklearn.linear_model import LinearRegression

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)


plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 


########################################

### R #####

## repeat above SAC code with new cleaner data
## new data works. SAC redrawn, new chao estimators, etc

## while we're at it, can we do the beta diversity stuff? 
## some kind of bray curtis vs distance graph?

## for this we need the x,y of all the plots...

spDF <- read.csv('GPSpointsCedrosHoja1.csv') 
