## if we're using R, for big community matrices:
smat <- function(mat){return(mat[1:3,1:3])}

## clean gps points

## Ana's GPS points are not working for me. Going to keep some notes on what I think is going on here, 
## and what I do to try to fix.

## problem: when I load the first page of Ana's spread entitled "GPSpointsCedros.xls", 
## the points are far away from where I think they should be. 

## They sort of appear like they are UTM zone 17S, but I think perhaps that (1) E and N are flipped, 
## and (2) that the E column is off by 10^7 meters.

## #2 may be due to the use of a different UTM zone, not sure. But we'll try bringin them into a 
## range that makes sense here by adding 10^7 to all of them 

## #1 we'll just try flipping these axes, E becomes N, vice versa. 


### works! we're in business. 

################################################

## Ana's data is too big for me to see with my 
## spreadsheet software
## Get ana's data out the xls files. Maybe if we can 
## get it into individual sheets, we can manage 
## a little better with our software.
## or even better, get the various sheets into 
## dataframes

sudo pip3 install openpyxl

## py

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## read one of the wkbks in:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']


wb.sheetnames
sheet['A1'].value

## alternatively:

sheet.cell(row=1, column=2).value

## are we missing data?
sheet.max_row
sheet.max_column

openpyxl.utils.get_column_letter(40)

## doesn't look like it. Can we go to a pandas dataframe?

## probably best to delete header, for this exercise

sheet['A1'].value

sheet.delete_rows(1)

sheet['A22'].value

## looks okay. convert to a df?

df = pd.DataFrame(sheet.values)
df.columns = df.iloc[0]
df.drop(0,0, inplace=True)

## yup looks good. 

### try this with the larger excel document 

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb['Cedros bosq Circ plots']

sheet.max_row
sheet.max_column

df2 = pd.DataFrame(sheet.values)

df2.columns = df2.iloc[0]
df2.drop(0,0, inplace=True)

## definitely more reliable than the spreadsheet gui, libreoffice. 
## something really funny is going on with the libreoffice software on this. 
## anyway now  we gotta start really studying what Ana did...

## it's messy, a big pile of spreadsheets. If I had more time I'd 
## maybe try to make a proper database out of all this...

## but I don't
## so, what did they do?

########################################################

## what I can understand about Ana's spreadsheets:

####### BaseDeDatosCedrosEnvaSarah7012012.xlsx ########

#### Sheet 1 - Cedros bosq Circ plots ####

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

sheet = wb['Cedros bosq Circ plots']
cbcpDF = pd.DataFrame(sheet.values)
cbcpDF.columns = cbcpDF.iloc[0]
cbcpDF.drop(0,0, inplace=True)

## this looks like trees, shrubs, woody stuff. Each species is given a count, 

## all species unique? Any repeats?

cbcpDF.ESPECIE.duplicated()

## yup, most. So what does the count indicate?  
## those are not counts. "No de individuo" is a unique identifier for
## each tree 
## good, that will make the SAC easier

## can we isolate this into a single workbook? Then libreoffice might 
## digest it a little better...

#cbcpDF.to_csv('CedrosBosqCircPlots.csv')

## works, libreoffice wasn't loading the full sheet originally. 
## okay, so the circular plots are basically the "large woody plants" data,
## for one of the forest plots? But which type of forest?
## from Ana's report, it seems like there should be one for 
## bosques cerrados and bosques secandarios
## so there should also be a sheet for the gaps and the other type of forest...
## the habitat column all says "BC", so this is probably the 
## closed forest tree data.

cbcpDF.columns

cbcpDF.Hábitat.unique()

cbcpDF.Hábitat.duplicated()

cbcpDF.Hábitat.duplicated()

cbcpAllSpp

## check for number of unique species
## to do this for real, we'll need to deal with all the random white spaces
cbcpAllSpp = cbcpDF['Genero '] + " " + cbcpDF['Especie']
len(cbcpAllSpp.unique())  ## ~139 spp

######## sheet 2, "Cedros reg circ plots " #######

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros reg circ plots "] ## note they included a space in this name
crcpDF = pd.DataFrame(sheet.values)
crcpDF.columns = crcpDF.iloc[0]
crcpDF.drop(0,0, inplace=True)

## whoah, this has 1216 rows... that's a lot of plants...

crcpDF.to_csv("CedrosRegCircPlots.csv") 

crcpDF.Hábitat == "RG"

crcpDF.Hábitat == "BC"

crcpDF.Hábitat.unique()

## the habitat column indicates just two values, "RG" and "RCA"
## what do these mean?

crcpAllSpp = crcpDF['Genero '] + " " + crcpDF['Especie']
len(crcpAllSpp.unique())  ## 197

######## sheet 3, "Cedros Arb juv bosq" ####################

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
sheet = wb["Cedros Arb juv bosq"]

cajbDF = pd.DataFrame(sheet.values)
cajbDF.columns = cajbDF.iloc[0]
cajbDF.drop(0,0, inplace=True)
#cajbDF.to_csv("CedrosArbJuvBosq.csv") 
cajbDF.shape 
## 657 rows, 26 columns, different data
## spreadsheet only picks up ~509 rows
## all of these are from bosque cerrado

cajbDF.Hábitat.unique()

## all "BC"

## so this is the juvenile trees from closed forests, methinks

## it seems like these are botanical collections
## according to Ana's report, collections of juvenile trees 
## were only made in the inner, 5m x 5m parcels. 
## so it seems like this is the 5x5 closed forest plant collections.

cajbAllSpp = cajbDF['Genero '] + " " + cajbDF['Especie']
len(cajbAllSpp.unique())  ## 92

######## sheet 4, "Cedros Arb juv reg " ####################

## again, note the space in the name

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros Arb juv reg "] ## note they included a space in this name
cajrDF = pd.DataFrame(sheet.values)
cajrDF.columns = cajrDF.iloc[0]
cajrDF.drop(0,0, inplace=True)
#cajrDF.to_csv("CedrosArbJuvReg.csv") 

## this looks like the complement to the above (the juvenile trees 
## from the closed forest). It looks like the unique identifiers 
## for parcel are continuous:

cajrDF.columns

## six regular parcels
cajrDF['Parcela/Plot'].unique() ## 1,2,3,4,5,10
cajbDF['Parcela/Plot'].unique() ## 7,6,8,9

cajrDF['Genero ']  ## fucking spaces. I will clean all these up if I do this project

## can we check for number of unique species?
## to do this for real, we'll need to deal with all the random white spaces



cajrAllSpp = cajrDF['Genero '] + " " + cajrDF['Especie']
len(cajrAllSpp.unique()) ## 89


######## sheet 5, "Cedros Arb juv reg " ####################

## this is not a data. But some good stuff here. Some of the habitat types are
## explained:

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

## BC Bosque cerrado
## BS Bosque secundario
## CLB Claro de bosque
## RG regeneración fincas agricultura y ganadería
## RCA Regeneración cañaveral

########## combine observations from 

## can we estimate the total unique tree species collected/observed?

help(pd.concat)

pd.concat

spp = [
cbcpAllSpp,
crcpAllSpp,
cajbAllSpp,
cajrAllSpp
]

sppC = pd.concat(spp)

len(sppC.unique())

type(sppC)
len(sppC)

## 315 species of tree recorded from these plots. 

## we need to start thinking about the proper unit for 
## construct sampling effort curves...

####### other workbook: "CedrosFinalReview.7022012.xlsx" #######

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
#bdfDF.to_csv("CedrosFinalReview.csv") 

bdfDF.columns

## number of unique species here?

bdfSG = bdfDF['Genero '] + ' ' + bdfDF['ESPECIE']

len(bdfSG.unique()) ##337


len(bdfDF.ESPECIE.unique()) ##337

bdfDF.Hábitat.unique() 

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

bdfDF['Final site '].unique()

bdfDF['Parcela'].unique()

bdfDF['Subparcela'].unique()

## pretty similar to above. 

## so again, how do we make a SAC out of this? What are our rows?

## I'm going to guess that the most useful sample unit here is the 30x30
## plot, of which there should be 27, according to the report from 
## Mika. 

## how/where are these labeled in the spreadsheet?

## actually, it looks more like there are 61 sub-parcel/plot

## to get a unique id for these, we need to index both 
## plot and subplot together.

## but what are these sub-plots? the terminology is loose in the 
## explanation, but it looks like a "subplot" corresponds to one
## of the 30x30 events, with both of its smaller, nested plots. 

## I don't understand the placement of the plots. When I color code
## by sub-block, they mostly group together spatially but not 
## entirely. A few random points are far removed in each group.

## doesn't matter for the sac, but it will haunt us later if we don't
## figure that out, methinks.  

## for the moment, let's create a unique ID column and go to vegan

bdfDF.head(2)

PsubP = bdfDF.Parcela.astype(str) + "." + bdfDF.Subparcela.astype(str)

bdfDF['PsubP'] = PsubP

bdfDF.head(2)

## now, we need a simple community matrix, where rows are 
## unique IDs from this PsubP, and columns are the ESPECIE column.

## so maybe let's drop to these, ESPECIE and PsubP:

bdfDF['PsubP']

smallbdf = bdfDF[['PsubP','ESPECIE']]

bdfGroup = smallbdf.groupby('PsubP')
bdfGroup.agg(np.size)
## neat, that's basically the species richness of each subparcel


bdfDummy = pd.get_dummies(smallbdf['ESPECIE'])
## wow, that was easy.

## sanity checks
bdfDummy.iloc[0,] ## yup

bdfDummy.loc[:,'vs. Perebea angustifolia'] ## yup

bdfDummy['Leandra subseriata']

[0:5] ##yup

bdfDummy.iloc[0:3,0:5]

bdfDummy['Turpinia occidentalis (Sw.) G. Don '][2347:2351] 
bdfDummy.iloc[2347:2351,0:5]

## there are some serious whitespace issues with this data. 
## will need some time to clean up species names, etc.
## but for now, let's just get a curve for folks to look at.

## okay, add our PsubP back in there, make it our first column

##bdfDummy['PsubP'] = PsubP
## better:
bdfDummy.insert(0, 'PsubP', PsubP)

## ugh, my head has been out of pandas for a long time...I think we 
## need the grouby function, check this site:
## <https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm>

bdfDummyGroup = bdfDummy.groupby('PsubP')

subParcelComm = bdfDummyGroup.agg(np.sum)

##subParcelComm.to_csv("subParcelComm.csv")

## looks promising. 
## to vegan!



###################################

## R/vegan

R

library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)

## fix rownames 
rownames(subParcelComm) <- subParcelComm$PsubP
subParcelComm <- subParcelComm[,-1]


anaSAC <- specaccum(subParcelComm, method = "exact")

pdf(file="anaSubparcelSAC.pdf")
plot(anaSAC)
dev.off()

specpool(subParcelComm)

##########
    Species     chao  chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     343 516.3946 39.15056 483.6557 21.60728 566.8038 404.4403 10.54889 61
##########

## neat. lots of tree species

## what about some sort of turnover chart?

################

## what do want out of this data set?

## what is ana's hypothesis/goal
## what is mine?

## next steps:

## 1 - clean data 

## and then, not necessarily in this order:

## 2 - ordination - does gap/forest-age explain community?
## 3 - create vegetation zones 
## 4 - show community turnover. This will be perhaps the most important,
## this is pretty much just BC by distance classes. 
## also some sort of mantal's r correlogram
##     because the cloudforest will probably 
## 5 - compare to other places

#####################

## clean data ##

## what spreadsheets do we need? 

## so far, it looks like we can do with just the 
## 2012 data sheet: "Base dat final 6 29 2012"

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)

## basically, we need to hunt for white space. 

bdfDF.columns

## typos here:

## 'Final site ' 
## 'N. Actual '
## 'N. anterior '
## 'Genero '

## from this data, we want to retain:

specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
##rename, without white spaces in the names
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.head(1)

## we got some empty rows. Can we get rid of these?
specObs.tail(3)

aa = specObs.dropna(how='all', axis=0)
aa.shape
## drops 11 rows, is that right? 
## looks right, keep it:

specObs.shape
specObs.tail(15)

specObs.dropna(how='all', axis=0, inplace=True)
specObs.shape
specObs.tail(10)

## how do we search our columns for white space?

## can we apply strip to all the elements of Df?
 
specObs.genero.str.strip()


## that seems to work okay. do this for all string columns

strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()


specObs.head(5)
specObs.head(5)

## looks good. 
## we may need to get rid of accents, etc, in the text,
## but let's wait and see if they cause problems

## let's make a clean species/genus column, and see if our 
## old numbers still match, or if we had a bunch of duplicates
## that typos were making unique:



## what next? we want a physical distance matrix, and a 
## a BC distance matrix. 

specObs['genusSpecies'] = specObs.genero + " " + specObs.especie

specObs.head(2)

specObs['genusSpecies'].unique().shape

## yup, lost like thirty species. 

## this is getting complicated...
## let's get all the specObs creation commands here, at once:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.dropna(how='all', axis=0, inplace=True)
strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()

specObs.especie[specObs.especie.isnull()] = ""
specObs['genusSpecies'] = specObs.familia + " " + specObs.genero + " " + specObs.especie
specObs['genusSpecies'] = specObs['genusSpecies'].str.strip()
specObs['PsubP'] = specObs.parcela.astype('str') + "." + specObs.subparcela.astype('str')
specObs = specObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","habito","elevacion","habitat",]]
specObs['genusSpecies'] = specObs['genusSpecies'].str.replace('sp. ', 'sp.', regex=False)
specObs.genusSpecies.unique()
specObs.to_csv('specObs.csv')
#specObs.to_pickle('specObs.p')

#specObs = pd.read_csv('specObs.csv')

len(specObs['genusSpecies'].unique())

specObs.familia.isnull().any() ## nope
specObs.genero.isnull().any() ## also no empty genero cells. The problem is with the names of species


## there is a species that starts with "vs. "
## wtf?

## okay, let's regenerate our site x species matrix
## with the cleaned data:

smallSpecObs = specObs[['PsubP','genusSpecies']]
smallSpecObsGrouped = smallSpecObs.groupby('PsubP')

spRich=smallSpecObsGrouped.agg(np.size)
## ugh, keeps abbreviating, want to look at all the values
## looks like a max of 86 species at one subparcel
## can we plot this?

plt.ion()
plt.bar(spRich.index, spRich.iloc[:,0])
spRich.iloc[:,0].max() ## 86
spRich.iloc[:,0].min() ## 11
## we'll want to group that by gap/regen groups


## make matrix for SAC
smallSpecObs['genusSpecies']

bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv("subParcelComm.csv")
#subParcelComm.to_pickle('subParcelComm.p')

####### find x,y for all the PsubP values ##########

## read in the csv for the gps data:

spDF = pd.read_csv('GPSpointsCedrosHoja1.csv', parse_dates=True)

## for now, we only want a PsubP column, and N, E coordinates

spDF['PsubP'] = spDF['Parcela/Plot'].astype('str') + "." + spDF['Subparcela  /Subplot'].astype('str') 
spDF = spDF[['PsubP', 'E', 'N']]
spDF.set_index('PsubP', inplace=True)

## does that match? 
#subParcelComm = pd.read_csv("subParcelComm.csv")

subParcelComm = pd.read_pickle("subParcelComm.p")

spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
## indices are not in same order...


spDF.sort_index(inplace=True)
subParcelComm.sort_index(inplace=True)

## check matching order

spDF.index == subParcelComm.index
spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
pd.concat([spDF.PsubP, subParcelComm.PsubP], axis=1)
spDF.set_index('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
len(subParcelComm.PsubP)
len(spDF.PsubP)
## looks okay. 
spDF.to_csv('subParcelSpatial.csv')
spDF.to_pickle('subParcelSpatial.p')

## to R? or can we do our BC distances without vegan, here in python?

import scipy.spatial as sp

spDF.head(2)
subParcelComm.head(2)

## this is really what we need, I think:
physDist = sp.distance.pdist(spDF, metric='euclidean')

sqPhysDist = sp.distance.squareform(physDist)

bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')

physDist.shape
bcDist.shape

## plot these?

plt.scatter(physDist, bcDist)

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

from sklearn.linear_model import LinearRegression

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)


plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## okay, that's cool. 
## we should see if it is more dramatic if we focus 
## on the primary forest plots only



########################################

#### adding to Bitty's list ####

## Bitty maintains a plant list for Los Cedros.
## let's see how many of these are new 
## species for los Cedros

## read in Bitty's spreadsheet:

wb = openpyxl.load_workbook("BittyLosCedrosPlants.xlsx")
wb.sheetnames
sheet = wb["Plants"]
bittyDF = pd.DataFrame(sheet.values)

## it looks like we only need the scientific name and family from this. 
## go to just these:

bittyDF = pd.DataFrame(sheet.values)
bittyDF.columns = bittyDF.iloc[0,:]
bittyDF.drop(0,0, inplace=True) 
bittyDF[['Scientific name', 'Family']].head()
bittyDF=bittyDF[['Scientific name', 'Family']]
bittyDF.dropna(how='all', axis=0, inplace=True)
bittyDF['Scientific name'] = bittyDF['Scientific name'].str.strip()
bittyDF['Family'] = bittyDF['Family'].str.strip()

## check the unique values:

len(bittyDF['Scientific name'].unique())
bittyDF.shape

## how do we compare the Ana data to this?

specObs['genero'].unique()

## this is going to be an imperfect process. But can we whittle the problem 
## down to something manageable?

## split up bitty's scientific name into genus and species:

aa = bittyDF['Scientific name']
bitGen = aa.str.split(pat=' ', expand=True).iloc[:,0]
bitGen.isin(specObs['genero'].unique())

sum(bitGen.isin(specObs['genero'].unique())) ## ~100 genera that are in Bitty's data are in Ana's?

## What are they?

mask = bitGen.isin(specObs['genero'].unique())
len(bitGen[mask])
bitGen[mask]

## and vice versa?
anaGen = pd.Series(specObs['genero'].unique())

## can we get rid of 'sp.' genera?

mask = anaGen.str.contains("sp. *[0-9]+", case=False, regex=True)
anaGen[mask]

## that regex may be useful above for cleaning...

## how to edit this in place...
anaGen = anaGen.str.replace("sp. *[0-9]+", "unidentified", regex=True)

## kick out the unIDs
anaGen = pd.Series(anaGen.unique())

## okay, so are there genera that are represented in Bitty's data?
anaGen.isin(bitGen)

sum(anaGen.isin(bitGen)) ## 64 genera that are in Ana's data are in Bit

## 
~anaGen.isin(bitGen)

sum(~anaGen.isin(bitGen)) ## 74 more genera? can that be true?

## sanity checks
mask = (~anaGen.isin(bitGen)) 
#anaGen[mask]
anaGenNotBit = anaGen[mask]
anaGenNotBit.reset_index(inplace=True, drop=True)

## so we now for certain that all species within these are not on bitty's list.

## now we need to recover the full scientific name and family from Ana's list, 
## and family, and add these to Bitty's data, or at least put it in a form that 
## is easy to add.

## for now, keep it separate to make this easier for her?

specObs[recNotBit] ## NOT in bitty's genera, but yes in ana's

specObs[~recNotBit] ## in bitty's genera and ana's

## how to recover the full records from ana data by genus name:

recNotBit = (~specObs['genero'].isin(bitGen))
newGen = specObs[recNotBit]

## cleanout the genera with just "sp." values:
mask = ~(newGen['genero'].str.contains("sp. *[0-9]*", case=False, regex=True))
newGen[mask].shape
newGen = newGen[mask]

## so this the spreadsheet of new genera, 
## with all of the species within these.

## check 
## this should be ana's but not Bitty's
"Conostegia" in specObs.genero.values
"Conostegia" in bitGen.values
"Conostegia" in newGen.genero.values

## this should be in both
"Dacryodes" in specObs.genero.values
"Dacryodes" in bitGen.values
"Dacryodes" in newGen.genero.values

## seems to work. From newGen, we need the family, genus, species columns:
newGen = newGen[['familia', 'genero', 'especie']]

## how to we subset to just unique species values?

newGenGrouped = newGen.groupby(['genero','especie']).agg(pd.Series.unique)

newGen.head()
newGenGrouped.head()
newGenGrouped.shape

## what does a csv look like if we use this?

newGenGrouped.to_csv('newGeneraForBitty.csv')

#### ok, so how do we get the other plants? #####

## these other species will not be in the new genera, 
## so we need to look in the genera that are already
## covered. 

## This gets messy, probably, but the start is simple:

## these are the records that are of genera that are 
## in both Bitty's plant list and Ana's:

sharedGen = specObs[~recNotBit] 

## subset to family, genus, species
sharedGen = sharedGen[['familia', 'genero', 'especie']]

## collapse it:
sharedGen = sharedGen.groupby(['genero','especie']).agg(pd.Series.unique)

## now how to avoid duplicates with bitty's list?

## let's try joining our genus+species column, and matching to
## Bitty's scientific names:

sharedGen = sharedGen.reset_index()[["familia","genero","especie"]]

sharedGen['GenSpec'] = sharedGen.genero.str.cat(sharedGen.especie, sep=' ')

## search Bitty's df:

sharedGen['GenSpec'].isin(bittyDF['Scientific name'])

sum(sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 44 matches.

sum(~sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 153 spp not listed in Bitty's list

## sanity checks
"Rhodostemonodaphne cyclops" in bittyDF['Scientific name'].values
"Trichilia glabra" in bittyDF['Scientific name'].values

## okay, so how do we subset specObs to these?
newSpp = sharedGen[(~sharedGen['GenSpec'].isin(bittyDF['Scientific name']))]
mask = ~(newSpp['GenSpec'].str.contains("sp. *[0-9]*", case=False, regex=True))
newSpp = newSpp[mask]

## I think this is what bitty needs?
newSpp.to_csv('oldGeneraDifferentSpp4bitty.csv')

## bitty has noticed a particular record of interest. Get her the full story:



############## ridgetop paper data ########################

## there was an additional paper about biodiversity at los Cedros,
## and elsewhere, Wilson and Rhemtulla (2018).

## their data is public, let's look at it:

wb = openpyxl.load_workbook("doi_10.5061_dryad.3nr41__v1/prescence_abscence.xlsx")

wb.sheetnames
sheet = wb["Sheet1"]
ridgeDF = pd.DataFrame(sheet.values)

ridgeDF.rename(columns=ridgeDF.iloc[1,:], inplace=True)

ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.reset_index(inplace=True, drop=True)

columns=ridgeDF.iloc[1,:], inplace=True)

#aa = ridgeDF.columns.values
aa = ridgeDF.iloc[1,:], inplace=True)aa[0] = "family"
aa[1] = "gnusSpecies"

ridgeDF.columns[0]='Family'

## which is LC?

## and never mind, Bitty just informed me she has done all this work.

######################################

## okay, back to the turnover graphs...

## can we back up and redo the turnover graphs, split out 
## by habitat type?

## we need to split up both the specObs and spDF dataframes by 
## habitat. How

spDF = pd.read_pickle('subParcelSpatial.p')
specObs = pd.read_pickle('specObs.p')

## we want a set of smaller data frames from specObs, based on their habitat:
soGrouped = specObs.groupby('habitat')

soGrouped.groups.keys()

oldForestObs = soGrouped.get_group('BC')

oldForestObs.head()

oldForestObs.PsubP.unique() ## 12 subparcels of old forest. Is this enough?

## how can we add spatial info?
ofoN = [ spDF.loc[i].N for i in oldForestObs.PsubP ]
ofoE = [ spDF.loc[i].E for i in oldForestObs.PsubP ]
oldForestObs['N'] = ofoN
oldForestObs['E'] = ofoE

oldForestObs.head()

oldForestObs.tail()

## we need a species matrix for just this set of plots:

subParcelComm = pd.read_pickle("subParcelComm.p")
aa = subParcelComm.index.isin(oldForestObs.PsubP)
oldForestComm = subParcelComm[aa]

## any rows all-zeros?

(oldForestComm == 0).all(axis=0) ## lots of species that are not present in any of these plots
sum((oldForestComm == 0).all(axis=0)) ## 190, out of 343, species not represented in these
## but that's not bad, considering the ratio, this is just 1/5 of all plots, but 45% of tree spp
(oldForestComm == 0).all(axis=1) 
## no rows that have zero in all species. Makes sense, that would mean a 
## plot with no trees, classified as bosque cerrado

## I think it makes sense to remove these all-zero species columns:

(oldForestComm == 0).all(axis=0).values

notObserved=~((oldForestComm == 0).all(axis=0).values)

oldForestComm = oldForestComm.iloc[:,notObserved]

oldForestComm.shape ## 12,153 . Makes sense. 

(oldForestComm == 0).all(axis=1).any() ## no species that are not present at least somewhere

## okay, I think we can do our turnover graph

oldForestDF = spDF.loc[oldForestComm.index.values]

## got to get a spatial matrix just for the oldforest..


physDist = sp.distance.pdist(oldForestDF, metric='euclidean')
bcDist = sp.distance.pdist(oldForestComm, metric='brayCurtis')

physDist.shape
bcDist.shape

plt.ion()

plt.scatter(physDist, bcDist)

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

plt.xlabel('distance between subplots')
plt.ylabel('BC dissimilarity')
plt.title('old Forest')
plt.savefig('oldForestTurn.png')

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## works. can this be generalized to the other habitat types?

###### general turnover pipeline #########

specObs = pd.read_pickle('specObs.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
## old data: spDF = pd.read_pickle('subParcelSpatial.p')
## new data:
spDF = pd.read_csv('finalReport2012Coords.csv')
spDF['PsubP'] = spDF['PsubP'].astype('str')
spDF.set_index('PsubP', inplace=True)
spDF.drop('Parcela', axis=1, inplace=True)
spDF.rename(columns={'FinalRep_E':'E', 'FinalRep_N':'N'}, inplace=True)


## get our original in there first:
fig, axes = plt.subplots(nrows=2, ncols=3, sharey=True)
axes = axes.flatten()
physDist = sp.distance.pdist(spDF, metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

axes[0].scatter(physDist, bcDist)
axes[0].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## now look at these individually:
soGrouped = specObs.groupby('habitat')
habs = list(soGrouped.groups.keys())

for h,i in enumerate(habs): 
    print(i)
    print(h)
    obs_i = soGrouped.get_group(i)
    obs_iE = [ spDF.loc[a].E for a in obs_i.PsubP ]
    obs_iN = [ spDF.loc[a].N for a in obs_i.PsubP ]
    obs_i['N'] = obs_iN
    obs_i['E'] = obs_iE
    subParcelInObs_i = subParcelComm.index.isin(obs_i.PsubP)
    comm_i = subParcelComm[subParcelInObs_i]
    notObserved=~((comm_i == 0).all(axis=0).values)
    comm_i = comm_i.iloc[:,notObserved]
    spDF_i = spDF.loc[comm_i.index.values]
    physDist_i = sp.distance.pdist(spDF_i, metric='euclidean')
    bcDist_i = sp.distance.pdist(comm_i, metric='brayCurtis')
    axes[h+1].scatter(physDist_i, bcDist_i)
    X, Y = physDist_i.reshape(-1,1), bcDist_i.reshape(-1,1)
    axes[h+1].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 
    axes[h+1].set_xlabel('distance between subplots')
    axes[h+1].set_ylabel('BC dissimilarity')
    axes[h+1].set_title(f'{i} habitat turnover')

plt.tight_layout()

plt.savefig('turnoverByHabitat.png')

## well, withing the scale of comparisons that we have,
## this is not that interesting, except to say, damn, 
## these habitats are not good predictors of the 
## massive biodiversity that we are seeing here. 

## is it worth our time to do some mem analysis on this?
## what would be the advantages..?

## something to chew on...

#############################################################

## while we're here, SAC for each, to compare
## alpha diversity?

## what do we need for this?

## we need the community matrix for each habitat type, 
## exported as a csv, so we can head over to R and get into 
## vegan

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## grab the pipeline for the community matrix above:

specObs = pd.read_pickle('specObs.p')
spDF = pd.read_pickle('subParcelSpatial.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
smallDF = specObs[['PsubP','genusSpecies']]
dfDummy = pd.get_dummies(smallDF['genusSpecies'])
bdfDummy.insert(0, 'PsubP', specObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)

## to do one habitat type, we want a column that shows 
## the habitat type for each PsubP

aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)

## now groupby this?
bbGrouped = bb.groupby('habitat')

bbGrouped.groups.keys()


cc = bbGrouped.get_group('BC')

~(cc == 0).all(axis=0) ## or...

(cc != 0).any(axis=0)

observed = (cc != 0).any(axis=0).values
dd = cc.iloc[:,observed]

dd.drop('habitat', axis=1, inplace=True)

dd.to_csv('BC_comm.csv')

## let's generalize that to create community dataframes for each habitat
## type:


aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)
bbGrouped = bb.groupby('habitat')
habs = list(bbGrouped.groups.keys())

for i in habs:
    cc = bbGrouped.get_group(i)
    observed = (cc != 0).any(axis=0).values
    dd = cc.iloc[:,observed]
    dd.drop('habitat', axis=1, inplace=True)
    dd.to_csv(f'{i}_comm.csv')

## in R

library(vegan)
library(RColorBrewer)

subParcelComm <- read.csv('BC_comm.csv', header=TRUE, row.names=1)
SAC <- specaccum(subParcelComm, method = "exact")
#pdf(file="BC_SAC.pdf")



plot(SAC)
#dev.off()
specpool(subParcelComm)

    Species     chao chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     137 250.2274  36.241 207.5833    22.91 253.5682 167.1898 10.28604 12

## so ~half of the estimated species will come from this type of forest.

## okay, can we repeat for the others, and build a single diagram out of 
## this?

#pdf("comboSACs.pdf")
par(mfrow = c(2,3))
files <- list.files()
comm <- grep('_comm.csv', list.files())
comms <- files[comm]
comms <- c("subParcelComm.csv", comms)
comtitles <- sub("_comm.csv", "", comms)
comtitles[1] <- "All habs"
j <- 0
for (i in comms){
    j <- j + 1
    comm.i <- read.csv(i, header=TRUE, row.names=1)
    print(i)
    SAC <- specaccum(comm.i, method = "exact")
    plot(SAC, main=comtitles[j], ylab='No. of Spp')
    capture.output(print(paste('Species estimates for', comtitles[j], sep=" ")), 
                    file="habSRestimates.txt", append = TRUE)
    capture.output(specpool(comm.i), 
                    file="habSRestimates.txt",
                    append = TRUE)
    sacDF <- data.frame(cbind(SAC$richness, SAC$sd), row.names=SAC$sites)
    colnames(sacDF) <- c('richness', 'sd')
    write.csv(sacDF, file=paste(comtitles[j], "SAC.csv", sep="_"))
}
#dev.off()

## so now we can go back to python, plot these together. 
## so we need to export appropriate data. These will be CSVs 
## of the sites, richness, and SD.

######## side note - NMS #########
## while we're in here, can we do an NMS of the habitat types,
## see if they group together?

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

## standard discrete color palette will work?

nms <- metaMDS(sPC, try=40)

## make plotting dataframe
specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat) 
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

head(nmsInfo)

## can we see what this looks like?

pdf(file='primenetNMSbyHabitat.pdf')
plot(nondups$MDS1, nondups$MDS2, 
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
## looks okay. How do we add hulls again?
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)
dev.off()

names(table(nmsInfo$habitat))

names(table(nmsInfo$colrs))

## do some checks

nmsInfo[identify(nondups$MDS1, nondups$MDS2),]


data(dune)
attach(dune.env)
ord <- metaMDS(dune)
plot(ord)
ordihull(ord, Management, col=1:4, lwd=3)
detach(dune.env)



names(table(nmsInfo$habitat))
names(table(nmsInfo$colrs))

nmsInfo$colrs

plot(1, col="#FF7F00", pch=19, cex=5)
##################################

## back to python, plot SAC curves

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
plt.ion()

## find our csvs of interest
SACs = [ i for i in os.listdir() if "SAC.csv" in i ]
sacDF = pd.read_csv('RCA_SAC.csv', index_col=0)

## plot this

fig, ax = plt.subplots(1,1)

def oneSacPlot(DF, clr):
    sacDF = pd.read_csv(DF, index_col=0)
    plt.plot(sacDF['richness'],
                color=clr)
    plt.fill_between(x=sacDF.index,
                     y1=sacDF.richness - sacDF.sd,
                     y2=sacDF.richness + sacDF.sd,
                    alpha=0.4,
                    color=clr,
                    )

colors = ['blue', 'green', 'red', 'yellow', 'orange', 'brown']

for colNu,DF in enumerate(SACs):
    oneSacPlot(DF, colors[colNu])


## the goal now is to plot these 

## make SACs

## make some useful SAC combo plots...then what? 

## indicator species for each type of habitat?

## the general story - deforestation knocks back a landscape into
## a pluripotent state? You may well end up with a very different

## wehere did the bosque secondario come from? Is it natural disturbance?
## blowdown, etc?
## or is it from deforestation? 

## If it is natural, then I think the story here is that deforestation
## changes the trajectory of succession...you may not recover your
## old growth, even with time, if you cut your forest. You will with 
## "natural" disturbance. Alternate stable states. 

## how would we test this hypothesis, with this data?

## not sure. Here the assumption would be that the ecosystem
## has alternate stable states, and that access to seeds
## from nearby plants is what will determine the the succession
## of the second growth. We might predict, then, that 
## regeneration of sites surrounded by forest will take 
## on old forest characteristics, and sites in clearings 
## will take on different species, showing more edge effects 
## and all that. 

## but I don't know if we have the ability to examine that 

## would be nice to get the ages on

## other ideas:
## let's try indicator species analysis, see what this turns up...

##############################################

## but first, we need to get our environmental data in order...

## where to start?

## none of the csvs that Ana has given me have environmental
## data other than elevation and habitat type.
## according to her report, we should also have light levels,
## "condiciones del clima" (=? did they have hobos at each site?)

## etc. very confusing. But let's start by getting a site x env 
## matrix with the data we have on our working dataframe:

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
plt.ion()

[ i for i in os.listdir() if '.p' in i ]

specObs = pd.read_pickle('specObs.p')

envOnly=specObs[['PsubP', 'elevacion', 'habitat']].drop_duplicates()


## odd, we have two elevation values for site 1.1: there is one collection
## from 1425m and the rest (~37 other collections) are from 1413 m. 
## I guess let's assume the singleton is the error?

envOnly=specObs[['PsubP', 'elevacion', 'habitat']].drop_duplicates()
envOnly.drop(1, inplace=True)
envOnly.set_index('PsubP', inplace=True)
#envOnly.to_csv('envOnly.csv')
#envOnly.to_pickle('envOnly.p')

## well, that's an environmental matrix, I guess. Not much there to work with. 
 
## we can check elevation as a predictor of community composition, permanova style...

## we also wanted to try clustering - can we recapture our habitat types using a 
## hierarchical clustering method?

######### hierarchical clusturing ##########

## back to R...

library(vegan)
library(stats)
 
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

## we need a distance matrix, BC
## does vegan give us what we need?

sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')



#sPC.ward.reorder <- reorder.hclust(sPC.ward, sPCBray)
## that function doesn't seem to exist in vegan anymore?

identify.hclust()

## also deprecated? can't find any of these...

## looks cool, but can we color by plot type?

## let's see if k=5 works, as this is the number of 
## habitat types that we have:

k <- 5

plot(sPC.ward,
hang=-1,
#xlab='5 groups',
ylab='height',
sub='',
main='',
#labels=cutree(sPC.ward, k=k), 
)

rect.hclust(sPC.ward, k=k)

## how we get our psubp values on there?

## the PsubP values are here:
sPC.ward$labels

## get our environmental matrix:
envOnly <- read.csv('envOnly.csv')

## and general info spreadsheet
specObs <- read.csv('specObs.csv')

## extract what our habitat types from this?


hab <- vector(length = length(sPC.ward$labels))
for (i in 1:length(sPC.ward$labels)){
    ind <- which(envOnly$PsubP==sPC.ward$labels[i])
    hab[i] <- as.character(envOnly$habitat[ind])
}

hablabels <- data.frame(cbind(sPC.ward$labels, hab))

head(hablabels)

head(sPC)

## some sanity checks
envOnly[envOnly$PsubP == 10.1,]
envOnly[envOnly$PsubP == 9.7,]

## generally seem to line up. So can we use this to label the
## leaves of the tree?

par(mfrow=c(1,1))
#png(file="wardCluster.png", width=900, height=500)
plot(sPC.ward,
hang=-1,
xlab='habitat',
ylab='height',
sub='',
main='',
labels=hab, 
)
aa <- rect.hclust(sPC.ward, k=3)
#dev.off()

rect.hclust(sPC.ward, k=3, lwd=4)

rect.hclust

## well, interesting. The RCA plots are on their own again...with some regen forest
## tagging along.

## can we look at these on a map?

envOnly <- read.csv('envOnly.csv')
## old data: pts <- read.csv('subParcelSpatial.csv')
pts <- read.csv('finalReport2012Coords.csv')[,-2]
pts <- pts[order(pts$PsubP),]

k <- 4
tree <- sPC.ward
jit <- 50 ## jitter value

hab <- vector(length = length(tree$labels))
## get labels

for (i in 1:length(sPC.ward$labels)){
    ind <- which(envOnly$PsubP==sPC.ward$labels[i])
    hab[i] <- as.character(envOnly$habitat[ind])
}


all(envOnly$PsubP == pts$PsubP)

makeClustMap <- function(tree, k, labels=NULL){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('finalReport2012Coords.csv')[,-2]
    pts <- pts[order(pts$PsubP),]
    require(stats)
    ## get habitat type labels for tree
    if (is.null(labels)){lab <- NULL}
    else {
        if (labels=='hab'){
        lab <- vector(length = length(tree$labels))
        for (i in 1:length(tree$labels)){
            ind <- which(envOnly$PsubP==tree$labels[i])
            lab[i] <- as.character(envOnly$habitat[ind])
    }}}
    par(mfrow=c(1,2))
    plot(tree, labels=lab)
    clustGroups <- rect.hclust(tree, k=k, border=1:k)
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    plot(x=pts$FinalRep_E, 
        y=pts$FinalRep_N, 
        col=clsp$gr,
        xlab='',
        ylab='',
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
    )
    legend('bottomright', 
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft', 
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

makeClustMap(sPC.ward, 2)

makeClustMap(sPC.ward, k=2, labels='hab')

makeClustMap(sPC.ward, k=4, labels='hab')

## interesting. forcing this to two groups does recapture this 
## result of RCA all by itself, with some of the regenerating 
## forest

## 5 groups - see if habitat types mostly split out as predicted.

png(file='hclust4groups.png', width=1600, height=800)
makeClustMap(sPC.ward, k=4, labels='hab')
dev.off()

clsp <- makeClustMap(sPC.ward, k=4, labels='hab')[,-2]
sum(clsp$habitat == 'RG' | clsp$habitat == 'RCA')

makeClustMap(sPC.ward, k=5, labels='hab')

legend('bottomright',
    legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
    pch=c(19, 21, 22, 17, 23)
)

legend('bottomleft',
    legend=1:k,
    pch=c(15),
    col=1:k,
    title='ClusterGroup')

## maybe build a function for this:

## also get a map behind this? 
## and there is an entire block of regenerating forest by itself

## can we add this cluster-group membership to our data?

## can we make a map of this, to see if there is a spatial pattern in these groups?

## starting to think that a PCNM analysis would be useful. Would probably show if 
## the eastern RCA plots are all on their own...

## this is going to start to involve some spatial analysis. It was inevitable.

## To start off, how to we get a map of our points?

## and we need to address the biodiversity questions - are we as diverse as Yasuni?
## how many species could we expect from the reserve as a whole? 

## also, wtf do we do with the permanent plot? Can we reconstruct using Danilo + Ana's
## data? I still do not understand what Ana did there...


############## get python packages ###########

## since internet is limited out here, let's see what packages we 
## can get while we got it

sudo pip3 install geojson
sudo pip3 install OGR
sudo pip3 install PyShp
sudo pip3 install dbfpy
sudo pip3 install Shapely
sudo pip3 install Fiona
sudo pip3 install GDAL
sudo pip3 install PIL
sudo pip3 install PNGCanvas
sudo pip3 install GeoPandas
sudo pip3 install PyMySQL
sudo pip3 install PyFPDF ## didn't work
sudo pip3 install Rasterio
sudo pip3 install osmnx ## didn't work


###################### make a map of points  ##############################

## we want to show the southeastern third of LC, with all of the plots in it
## we want to show elevation, streams, and our points, with their cluster
## info and their habitat info

## seems like matplotlib makes more sense to me here

import operator
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from osgeo import gdal, gdal_array, osr

from Pillow import Image ## why can't I import pillow? try fixing this with pip and internet connection

## a good first step is to see if we can read in our geotiffed DEG

## can we just read it in like a normal image?

plt.ion()

ax, fig = plt.subplots(1,1)

#deg="LosCedAster.tif" ## this doesn't. Is it an issue with the tif formatting?
deg="/home/daniel/Documents/LosCed/Ecuador_GIS/losced/LosCedOldMapGeoReffed/LosCed_topo.png" 
## yeah, because the above georeffed png works fine
img=mpimg.imread(deg)
plt.imshow(img)

## hmm, regardless, we probably need approach this is in a way that maintains 
## the integrity of the spatial coordinates. The coordinates are lost on png.
## there must be a way to use matplotlib for spatially explicit data 

## so I think we're jumping on this too early. let's work through a geopandas
## tutorial. I think we're going to need to do this right. 


import pandas as pd 
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio
import rasterio.plot
import copy
import random
plt.ion()

gpd.datasets.available

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) 

world.head()
world.plot()

worldfiltered = world[world.continent == 'Asia']
worldfiltered.plot()
worldfiltered.plot(column = 'gdp_md_est', cmap='Reds')
world.plot(column = 'pop_est')

## that is badass. I love python. 

nybb = gpd.read_file(gpd.datasets.get_path('nybb')) 

cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities')) 

cities.plot()

help(plt.subplots)

fig, ax = plt.subplots(figsize=[4,4])

## this is more like what we need 

## so how can we work with our data? We want a geopanda
## for our sample sites

anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
## our cluster info is here:
cGroup = pd.read_csv("cGroup.csv")
## add this cluster info in
anaPt = anaPt.merge(cGroup, on="PsubP")
## make a vector for coloring the clustering results:
cdik={
1:"k",
2:"r",
3:"g",
4:"b"
}
anaPt['clustCols'] = [ cdik[i] for i in anaPt.gr ]
## where is our habitat data?:
envOnly = pd.read_csv('envOnly.csv')
anaPt = anaPt.merge(envOnly, on="PsubP")
## set the habitat symbol
msym_dik={
'BC':'o',
'BS':'p',
'CLB':'s',
'RCA':'^',
'RG':'D',
}
anaPt['habSyms'] = [ msym_dik[i] for i in anaPt.habitat ]

## that was easy. let's do that for our other layers of interest:
paths = gpd.read_file('GIS/PathsLC.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')

## now start plotting
fig, ax = plt.subplots(1,1)
for i in anaPt.habitat.unique():
    print(i)
    anaPtI = anaPt[anaPt['habitat'] ==  i].reset_index()
    anaPtI.plot(color=anaPtI.clustCols, 
                marker=anaPtI.habSyms[0], 
                ax=ax)

hydro.plot(color='Blue', ax=ax)

paths.plot(color='red', linestyle='--', ax=ax)

raster = rasterio.open("GIS/anaPlotDEG.tif")
rasterio.plot.show(raster, ax=ax)

## jitter?
## don't need this with corrected GPS info
## def jitt(x,σ=10):
##     ''' returns a jittered value using normal dist '''
##     xJit = x + random.gauss(0, σ)
##     return(xJit)
## 
## def jitCoords(gdf, σ=10):
##     ''' jitters a geopanda df. Amount of jitter is set 
##         with the 'σ' parameter'''
##     aa=copy.deepcopy(gdf)
##     xx = aa.geometry.x.apply(jitt, args=(σ,)).astype('int')
##     yy = aa.geometry.y.apply(jitt, args=(σ,)).astype('int')
##     aa.geometry = gpd.points_from_xy(xx,yy)
##     return(aa)



## looks like jit = 40 is a nice amount

## This is so easy. I'm in love. 
## how do we make a lengend?
help(ax.legend)
## this might be helpful:
http://matplotlib.org/users/legend_guide.html
http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists

## how can we trim down the rasters a bit?
## to clip this raster to size, I guess we need to work
## with the rasterio program in the shell?

#rio clip input.tif output.tif --bounds xmin ymin xmax ymax
rio clip LosCedAster.tif test.tif --bounds "745020 10032300 749161 10035795" --overwrite

## get forest cover raster for 2005?
## legend

## well, it's a nicer plot, but I am still unclear why 
## the bosques primarias split into two types. 

## let's run some indicator species analysis, or maybe cooccurrence
## networks if we need more sensitivity. 

## good cooccurrence network diagram would be worth looking at. 

## also permanova models for elevation, slope, aspect, proximity 
## to stream

## and I guess you gotta do a variance partitioning  analysis.
## which would include the dbmems...

## lots of work to do. 


######################## indicator species ######################

## let's see if we can get indicator species for our habitat types, 
## and/or cluster types

install.packages('indicspecies')

library(vegan)
library(indicspecies)

## check out the tutorial:

data(wetland)
groups = c(rep(1, 17), rep(2, 14), rep(3,10))
indval = multipatt(wetland, groups, control = how(nperm=999))
## seems simple

## our community matrix...
comM <- read.csv('subParcelComm.csv')
rownames(comM) <- comM$PsubP

## general plant observation dataframe
specObs <- read.csv('specObs.csv', row.names=1)
## make a PsubP/habitat dataframe:
specObs[,c('PsubP','habitat')]
habs <- unique(specObs[,c('PsubP','habitat')])
rownames(habs) <- NULL

## and our cluster groups:
cldf <- read.csv('cGroup.csv')

## make sure they are the same order
habs <- habs[order(habs$PsubP),]
comM <- comM[order(comM$PsubP),]
cldf <- cldf[order(cldf$PsubP),]
all(habs$PsubP == comM$PsubP)
all(habs$PsubP == rownames(comM))
all(cldf$PsubP == rownames(comM))

## now we can make a vector of our habs dataframe:
habsV <- habs$habitat

clV <- cldf$gr

## clean up extra column of com matrix, careful with this
#comM <- comM[,-1] ## get rid of PsubP

## and try out the indicator species function

habIndSpp <- multipatt(comM, habsV, func = 'r.g', control=how(nperm=9999))

clustIndSpp <- multipatt(comM, clV, func = 'r.g', control=how(nperm=9999))



###################### cooccurrence matrix ######################

## it's been a couple years, how do we do this again?

## we'll use the cooccur package again, they seem to be maintaining
## it, and people seem to be using it. I don't know the specific 
## advantages over the classic correlation approaches, but some 
## folks like this approach and it is computationally cheaper, 
## which is meaningful out here with just my little laptop. 

library(cooccur)

## example:

data(finches)
cooccur.finches <- cooccur(mat=finches,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

summary(cooccur.finches)

plot(cooccur.finches)

## simple enough

## need my community matrix
## think we need to transpose?

comMat <- t(read.csv('subParcelComm.csv', row.names='PsubP'))

## I don't know if abundance matters here. We'll leave it. 

coocc_thresh <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

## lots of warnings - try removing the threshold?
## might also be the abundance data causing problems.

coocc <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=FALSE,
                          spp_names=TRUE)
## takes way longer...
## hmm, now how do we visually this?

plot(coocc)
plot(coocc_thresh) 

## I think we want to add in the habitat types. 
## Then run a second analysis with cluster types,
## and maybe both. 

## what does this look like? species are rows, so we need a row 
## for each habitat type, and cluster type. 

## borrow the above code from the indicator species analysis:

library(cooccur)

## general plant observation dataframe
specObs <- read.csv('specObs.csv')
## make a PsubP/habitat dataframe:
habs <- unique(specObs[,c('PsubP','habitat')])
## and our cluster groups:
cldf <- read.csv('cGroup.csv')
## merge these
aa <- merge(habs, cldf, by='PsubP')
## habitat dummy vars
habDum <- model.matrix(aa$PsubP ~ aa$habitat)[,-1]
rownames(habDum) <- aa$PsubP
colnames(habDum) <- c('BS', 'CLB', 'RCA','RG')
BC <- c(rep(0, nrow(habDum)))
habDum <- data.frame(cbind(BC, habDum))
habDum$BC[rowSums(habDum) == 0] <- 1
## cluster dummy vars
clDum <- model.matrix(aa$PsubP ~ as.factor(aa$gr))[,-1]
rownames(clDum) <- aa$PsubP
colnames(clDum) <- c('cl2', 'cl3','cl4')
cl1 <- c(rep(0, nrow(clDum)))
clDum <- data.frame(cbind(cl1, clDum))
clDum$cl1[rowSums(clDum) == 0] <- 1
## I think we just need to transpose these and stack them on our 
## community matrix
comMat <- t(read.csv('subParcelComm.csv', row.names='PsubP'))
comMat <- rbind(t(habDum),t(clDum),comMat)

## visual checks:
cbind(habDum, aa$habitat)

cbind(clDum, aa$gr)
## works


dim()
dim(t(clDum))

## now rerun the cooccurrence analysis:
coocc_thresh <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

aa <- prob.table(coocc_thresh)
bb <- aa[complete.cases(aa),]
cc <- aa[!complete.cases(aa),]
dd <- bb[,c('sp1_name', 'sp2_name','p_gt', 'sp1_inc', 'sp2_inc', 'obs_cooccur','prob_cooccur','exp_cooccur')]
ee <- dd[dd$p_gt <= 0.10,]


## what do we want? if we are interested in cooccurring species,
## we want a very small chance in the null model of seeing our 
## level of cooccurrence or higher. 

## so I think we want the p_gt statistic, for positive cooccurrences

## subset to p_gt <= 0.05

bb <- aa[aa$p_gt <= .05,]

dim(bb)

## okay, that looks like a bust. It is not as sensitive as our 
## indicator species analysis. I thought I was casting a wider 
## net with this method, but no bueno. The system is too 
## noisy, I guess. We could try correlations instead, but doesn't seem 
## like a good use of time right now.  

## seems like the best next steps are the mantel tests, permanovas, 
## then move onto the PCNMs and varpar analysis. 

###################################################

## mantel tests

install.packages('ecodist')

library('vegan')
library('ecodist')

## make the distance matrix 
## old data: aa <- read.csv('subParcelSpatial.csv', row.names="PsubP")[,-1]

## corrected coordinates:

cc <- read.csv('finalReport2012Coords.csv')

aa <- read.csv('finalReport2012Coords.csv')[,-2]
aa <- aa[order(aa$PsubP),]
physdist <- vegdist(aa, method='euclidean')
## make the community dissimilarity matrix
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
row.names(bb) <- bb$PsubP
bb <- bb[,-1]
all(aa$PsubP == rownames(bb))
braydist <- vegdist(bb, method='bray')

## correlogram
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
global_mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## can we subset this by habitat and by cluster group?
## this shows strong evidence of global autocorrelation

## what if we subset a little...

getDists <- function(comfile, physfile='finalReport2012Coords.csv'){ 
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv('finalReport2012Coords.csv')[,-2]
    spM <- spM[order(spM$PsubP),]
    rownames(spM) <- spM$PsubP
    spM <- spM[,-1]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),] 
    spM <- spM[order(as.numeric((rownames(comM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        }
    else{print("something doesn't fit right")}
    }

comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')

## start df
all_mant_test  <- global_mant_test

## test individually
par(mfrow=c(2,3))
for (i in 1:length(comms)){
    print(comms[i])
    dists.i <- getDists(comms[i])
    cgram.i <- mgram(dists.i[[1]], dists.i[[2]]) ## correlogram object
    plot(cgram.i, main = comms[i] )
    mant_test.i <- mantel(dists.i[[1]] ~ dists.i[[2]], nperm = 10000) ## overall test
    all_mant_test <- rbind(mant_test.i, all_mant_test)
}
plot(cgram, main = "global" )

comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')

rownames(all_mant_test) <- c(rev(comms), "global")



## The natural forest sites
## seem to be behaving similarly, classic positive autocorrelation
## dropping off to zero (with a dip into negative autocorrelation 
## around 1500m?), and the ag sites, especially RCA, are doing 
## weird stuff. In the turnover diagram, RG sites were 
## actually showing steeper turnover, with sites 
## increasing in dissimilarity more rapidly. The mantel 
## seems to show that here, too. 

## what happens if we exclude the weird RCA plots from the mantel, 
## and also when we examine the old forest sites together (BC+BS+CLB)

## exclude RCA plots:

## general plant observation dataframe
specObs <- read.csv('specObs.csv', row.names=1)
## make a PsubP/habitat dataframe:
habs <- unique(specObs[,c('PsubP','habitat')])
## order by site
habs <- habs[order(habs$Psub),]
rownames(habs) <- NULL

## let's look at the old-forest-only sites:

PsubPoldForest <- habs$habitat != 'RCA' & habs$habitat != 'RG'

aa <- read.csv('subParcelSpatial.csv')[-1]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- NULL
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
rownames(bb) <- NULL
all(habs$PsubP == aa$PsubP)
all(habs$PsubP == bb$PsubP)
## subset these:
aa <- aa[PsubPoldForest,]
bb <- bb[PsubPoldForest,]
all(rownames(aa) == rownames(bb))

## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## wow. that cleans up the story a bit:
##    mantelr      pval1      pval2      pval3  llim.2.5% ulim.97.5%
## 0.12648406 0.00810000 0.99200000 0.00970000 0.05079009 0.19028648


## let's include the regen sites:

## to get the sites that are not RCA:
PsubPNotRCA <- habs$habitat != 'RCA'

aa <- read.csv('subParcelSpatial.csv')[-1]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- NULL
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
rownames(bb) <- NULL

all(habs$PsubP == aa$PsubP)
all(habs$PsubP == bb$PsubP)

## subset these:
aa <- aa[PsubPNotRCA,]
bb <- bb[PsubPNotRCA,]
all(rownames(aa) == rownames(bb))

## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test
## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## doesn't do much, the regen sites also really confuse things, 
## they really are a different system. 

## let's run permanovas on the data we have. Still need to 
## create distance-to-stream data, might be useful. 

## but after that, time to dive into PCNM and varpart. Ugh...

## but first, scratch an itch - there is some detective work 
## to be done with the RG points that went outside their 
## cluster

############# fishing for stray RG points  #######################

## we need to rebuild our NMS and cluster results a bit, and see
## why which RG points clustered with the RCA plots, and which 
## clustered with the old forest plots:

## first, figure out the cluster results. Rebuild cluster graphic:

library(vegan)
library(stats)

## redo the tree

k <- 4
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')

png(file='hclust_4groups_PsubP.png', width=1200, height=800)
plot(sPC.ward)
clustGroups <- rect.hclust(tree, k=k, border=1:k)
dev.off()

## from our other plots, we can figure out the 
## the following RGs cluster with the RCAs:
RGwithRCA <- c(1.1, 1.2, 1.3, 1.4, 1.5)
## and the RG that clustered with the old forest:
RGwithOB <- 10.1

## where are these?
## both on the physical map, and in the NMS?

## bring up the NMS again, label with point names:

library(vegan)
library(RColorBrewer)
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
nms <- metaMDS(sPC, try=40)

stressplot(nms)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat)
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)

ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)

aa <- identify(nmsInfo$MDS1, nmsInfo$MDS2)

## too slow can we just label the points on there?
text(nmsInfo$MDS1, nmsInfo$MDS2, nmsInfo$PsubP)

## where is 10.1 on the map?
pts <- read.csv('subParcelSpatial.csv')[-1]


makeMap <- function(jit=50){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('subParcelSpatial.csv')
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    pts <- pts[order(pts$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    eJit=jitter(clsp$E, jit)
    nJit=jitter(clsp$N, jit)
    plot(x=eJit,
        y=nJit,
        col=clsp$gr,
        xlab='',
        ylab='',
        xlim=c(745000,749000),
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
    )
    legend('bottomright',
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft',
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

makeMap(100)
points(pts[pts$PsubP == 10.1,2:3], 
    pch=21, 
    cex=4, 
    col=6, 
    lwd=2, 
    bg="transparent")


## weird. 10.1 more closely resembles an old forest, as it is 
## clustered with the forest groups both in the NMS and the 
## the hclust. 

## but it is situated right next to other regenerating sites 
## that cluster more closely to the nefarious RCA plots 
## and the 

## what do we think decides this?

## Ana mariscal says age - when was it last farmed?
## we need that as a variable, can this be constructed? 

## rebuild the dataframe with habitat, etc:
clsp <- makeClustMap(sPC.ward, k=4, labels='hab')[,-2]
sum(clsp$habitat == 'RG' | clsp$habitat == 'RCA')
## we have 25 subparcels whose history we need to reconstruct. 

############# permanovas ######################

## back up a little to some of the basics

## can we predict differences in our sites by
## the environmental data we have?

library(vegan)

envOnly <- read.csv('envOnly.csv')

envOnly <- envOnly[order(envOnly$PsubP),]

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
sPC <- sPC[order(as.numeric(rownames(sPC))),]


## try habitat and elevation:

aa <- adonis(sPC ~ envOnly$elevacion) ## r2 = 0.08, p < .001
aa <- adonis(sPC ~ envOnly$habitat) ## r2 = 0.19, p < 0.001
aa <- adonis(sPC ~ envOnly$elevacion, strata=envOnly$habitat) ## no errors thrown, but don't think it worked...

## ugh, don't want to learn about the permute package. I think that's
## what we have to do to really get into this strata argument.

aa <- adonis(sPC ~ envOnly$elevacion*envOnly$habitat)  
bb <- adonis(sPC ~ envOnly$habitat*envOnly$elevacion)  

##                                   Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)
## envOnly$elevacion                  1    1.8357 1.83572  5.9572 0.07945  0.001
## envOnly$habitat                    4    4.1072 1.02679  3.3321 0.17776  0.001
## envOnly$elevacion:envOnly$habitat  4    1.4468 0.36170  1.1738 0.06262  0.090
## Residuals                         51   15.7157 0.30815         0.68017
## Total                             60   23.1054                 1.00000
## 
## envOnly$elevacion                 ***
## envOnly$habitat                   ***
## envOnly$elevacion:envOnly$habitat .

## some variance explained. 

################# focus on old Forest ###################

## just checking, is the story clearer if we subset to just old forest? and to just RG?

## just old forest, with no zero columns
oldF <- envOnly$habitat != 'RG' & envOnly$habitat != 'RCA'
## add in cluster membership:
load('cGroup.rda')
all(cGroup$PsubP == envOnly$PsubP)
envOnly$clGr <- cGroup$gr
oldForestEnv <-  envOnly[oldF,]
rownames(oldForestEnv) <- NULL
oldForestComm <- sPC[oldF,]
all(rownames(sPC) == envOnly$PsubP )
nonzs <- colSums(oldForestComm) != 0
oldForestComm <- oldForestComm[,nonzs]
all(oldForestEnv$PsubP == rownames(oldForestComm))

## just for kicks, get the spatial matrix, too:

oldForestSp <- read.csv("subParcelSpatial.csv")
oldForestSp <- oldForestSp[oldForestSp$PsubP %in% oldForestEnv$PsubP,]
all(oldForestSp$PsubP == oldForestEnv$PsubP)
#save(oldForestSp, file='oldForestSp.rda')


#save(oldForestComm, file="oldForestComm.rda")
#save(oldForestEnv, file="oldForestEnv.rda")

bb <- adonis(oldForestComm ~ oldForestEnv$elevacion*oldForestEnv$habitat)  
bb <- adonis(oldForestComm ~ oldForestEnv$habitat*oldForestEnv$elevacion)  


## when you subset to old forest, only elevation matters. 
## makes sense. The sites are interchangeable. 

## given that, what does an ordination of the old forest look like, 
## if we put elevation on there? and do the hierarchical clusters
## separate cleanly?

library(vegan)

OFnms <- metaMDS(oldForestComm, try=40)
stressplot(OFnms)
## as usual, not great, but still generally under 0.3

## make a color ramp for this elevation data
minElev <- min(oldForestEnv$elevacion)
maxElev <- max(oldForestEnv$elevacion)
rangeElev <- maxElev - minElev
colrsRamp <- colorRampPalette(c(rgb(1,0,0,1), rgb(1,0,0,0)), alpha = TRUE)(rangeElev)
ElevColrs <- colrsRamp[oldForestEnv$elevacion - min(oldForestEnv$elevacion)]

# get points
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']

png(file='oldForestNMSelevClust.png', height=800, width=800)
plot(XX, YY, 
    col="black", 
    xlab='NMS1',
    ylab='NMS2',
    pch=21, 
    cex=3, 
    bg=ElevColrs,
    main="Two forest types detected, with elevation gradient",
    )
## can we add cluster group data to the plot?
## group order will be:
names(table(oldForestEnv$clGr))
## so this should work:
ordihull(OFnms, oldForestEnv$clGr, col=c(3,4), lwd=2, lty=2)
## yup, that's intriguing. 
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '1259 m', '1680 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, "transparent", "red"),
        #merge = FALSE,
        cex = 2.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )
dev.off()


aa <- identify(OFnms$points)
OFnms$points[aa,]

### check habitat type on old forest nms ###

colz <- vector()
for (i in oldForestEnv$habitat){
    if (i == 'BC'){colz <- c(colz, '#E41A1C')} 
    if (i == 'BS'){colz <- c(colz, '#377EB8')}
    if (i == 'CLB'){colz <- c(colz, '#4DAF4A')}
}

## just checking
cbind(as.character(oldForestEnv$habitat), colz)

#png(file=, height=800, width=800)
plot(XX, YY, 
    col="black", 
    xlab='NMS1',
    ylab='NMS2',
    pch=21, 
    cex=2, 
    bg=colz,
    main="Old forest sites by habitat",
    )
legend( x='topright',
        legend = c('BC', 'BS', 'CLB'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A'),
        cex = 1.0,
        )

        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
names(table(oldForestEnv$habitat))

## not even going to try to put hulls on that. Pretty much randoms:

fh <- factor(oldForestEnv$habitat)
adonis(oldForestComm ~ fh)

#### ideal forest community ######

## we need some way to quantitatively characterize the old forest basin, 
## and measure if a sight may be moving toward or away from that state.

sPC[,'Cyatheaceae.Alsophila.erinacea', drop=FALSE]

aa <- cbind(envOnly$PsubP,oldF, sPC[,'Cyatheaceae.Alsophila.erinacea'])

###########  dbMEM analysis ################

## been a while, so let's work through the dbMEM materials out there
## looks like ade4 and associated packages are the way to go. 

## where has this package been all my (graduate) life?

library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

data("mafragh")

names(mafragh)

dim(mafragh$flo)

head(mafragh$env)

## check out the spatial data
mxy <- as.matrix(mafragh$xy)

rownames(mxy)
rownames(mxy) <- NULL

s.label(mxy, ppoint.col='darkseagreen4', Sp=mafragh$Spatial.contour)

class(mafragh$Spatial.contour)

## this is great, but what do we want to do with our data?

## for now, we want to define a spatial neighborhood of our points

## in the near future, I think we will need to define watershed polygons
## and look for an effect by microsite

## let's try running our point data through their dbMEM pipeline. 

## we can do this in three cycles - entire data set, forest only, and regen only

## start with entire, I guess. Our sampling scheme is irregular, looks like 
## this:

## pts <- read.csv('subParcelSpatial.csv') ## bad data!!

###### new data here: ########3
pts <- read.csv('finalReport2012Coords.csv')[,-2]
###############################

pts <- pts[order(pts$PsubP),]

env <- read.csv('envOnly.csv')


## are these in order?
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 

pJ <- as.matrix(pts[,2:3])
colnames(ptsMat) <- c('x','y')


### jitter these? Having different data on same points will be problem:
#set.seed=1
#pJ <- jitter(ptsMat, amount=10)
### not working - what if we bring everything closer to axes?

mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)

s.label(pJ)
## looks okay.

## now we need to define what a neighbor is. 
## in the past, we used the minimum spanning tree, recommended
## by borcard and legendre. 

## but these are not the only options

## there is an interactive program to explore these kinds of questions,
## let's see if it works:

listw.explore()

## okay, for the moment let's try the "gabriel" graph type. I don't totally
## understand the effect of the different graph 

## but I want to allow more comparisons than the minimum spanning tree
## might allow, without so many as the triangulating methods that might 
## make a lot of very distant comparisons. This I think equates to more
## eigenvectors, = more components of the model that have to be selected
## against, = model needs more p-value correction

## If you use a harsh spatial weighting, I'm not sure if it makes much 
## difference. I need to sit down with one of these stats folks, I 
## have a lot of questions here. 

## speaking of, for spatial weighting, let's use a simple linear weighting
## for now. 

## there is another parameter here, not well explained in the tutorial,
## = "Standardization Style". From the help file, this seems to be 
## another weighting of the links, but this time based on the 
## neighborhood. As in, if a point has a large neighborhood, it is 
## standardized to compare with the other neighborhoods by the number
## of links. Not sure why this is necessary. One function below (multispa)
## wants this to be "W", so let's go with it. 

## we can rerun all of this with different methods, see how much of a difference
## it makes. 

## this is the code that the interactive program generated based on 
## a "gabriel" neighborhood graph type, a "binary"/"B" standardization,
## and a linear weighting. 

## choose neighborhood type. "2" = gabriel:
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
## get the distances of each link:
distnb <- nbdists(nb, pJ)
## weight them, by their length. Here a linear weighting:
fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ)))
## combine nb and fdist: 
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

## also need to install the "spatialreg" package...

## the above generates the following neighborhood graph:

s.label(pJ, nb=nb, pnb.edge.col='red')

print(listw2mat(lw)[1:10, 1:10], digits = 3)

print(listw2mat(lw), digits = 2)

mem.lw <- mem(lw)

names(attributes(mem.lw))

barplot(attr(mem.lw, "values"))

s.value(pJ, mem.lw, symbol="circle")

s.value(pJ, mem.lw[,c(seq(1,60,5))], symbol="circle")

s.value(pJ, mem.lw[,c(1, 5, 10, 20, 30, 40, 50, 60)], symbol="circle")

## okay, that works, now?

## the goal is to do an RDA of the community matrix by the spatial matrix. 
## the spatial matrix at this point will be a list of eigenvectors, ~60 
## of them. We have to cut this down to size, so time for model selection.

## the model selection process needs a "y" matrix. Since we are going to 
## do an RDA, which is basically two PCAs on either side of the equation, 
## we can go ahead and hellinger transform our community matrix and run a 
## PCA on this:


comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]

all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)

## the ade4 stuff looks powerful, but culty. 

# let's do the standardization with the traditional function
## and see if we can get to the same spot:

library(vegan)

comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)

## okay, select using these PCAs, see if the output makes sense:

mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw)

## okay, as we learned below, change pvalue. The alpha is just a hair past
## the .05 cutoff (.0528!), this is blocking the model selection from
## going forward:

mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw, alpha=0.1)

summary(mem.comM.sel)

mem.comM.sel

## why does jupyter notebook not want to work like our repl does?

wtf <- function(jit=10, alpha=0.05, meth='FWD'){
pJ <- jitter(ptsMat, amount=jit)
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, pJ)
#fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ))) ## linear
fdist <- lapply(distnb, function(x) 1 / x^3) ## concave
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)
mem.lw <- mem(lw)
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)
mem.comM.sel <- mem.select(pca.hell$tab, 
        listw = lw, 
        nperm.global = 10000, 
        alpha=alpha,
        method=meth)
s.label(pJ, nb=nb, pnb.edge.col='red')
#save(pJ, file='jitteredPoints.rda')
#save(lw, file='lw.rda')
#save(mem.comM.sel, file='mem.comM.sel.rda')
return(mem.comM.sel)
}


bb <- wtf(alpha=0.10, jit = 2)
bb$global.test$pvalue
bb$summary$variables

## pvals
0.01729827
0.02159784
0.1460854

## mems
"MEM16" "MEM19" "MEM3"
"MEM16" "MEM6"  "MEM2"  "MEM12" "MEM20"
"MEM16" "MEM12" "MEM7"  "MEM19"
"MEM16" "MEM17" "MEM2"  "MEM20" "MEM12" "MEM9"


## 
cc <- wtf(alpha=0.10, meth='global')
## not useful, just re-reports all mems

## well...

## so try 3 things:
## 1. tone down the jitter distances 
## 2. don't weight the short distances so high, try different function 
## 3. run a lot of trials to get the mems of interest. 

## 1 -- didn't work. Just as wildly different each time.
## 2 -- results in wildly different results, emphasis on mem16, thought changing 
##      to concave weight with high exponent would make it more robust but still 
##      changing radically. 
## 3 -- not doing much for me, after ~20 trials, results are wildly different. Want
##      more elegant solutions, don't want to grind my computer on for 1000's of 
##      trials. 

## the real answer here is to hand curate our neighborhoods, either
## in the pt map itself, or in the neighborhood matrix
## we have to eliminate the false positives and negatives
## that are resulting from the jitter. 

## I think we have to look at each sample site in the coordinate matrix, 
## and actually separate them by some non-random distance, so we can 
## get consistent results that have some basis in reality.

## how to do this...how many duplicates do we have?

unique(ptsMat) ## 46 unique locations

sum(duplicated(ptsMat)) ## 15 duplicates

pts[,2:3]

## any particular pattern to dupped subParcels?

dups <- duplicated(ptsMat)

dupDF <- pts[dups,]

tE <- table(dupDF$E)

tN <- table(dupDF$N)

pts[pts$E == 746847,]

unique(pts[dups,])

## for instance, this set of GPS points is very 
## common:

## 746847, 10034621

pts[pts$E == 746847,]

## 3.1, 3.4, 6.2, 9.2,

## any pattern here on what is duplicated/not?

## not seeing any. Looks like good old fashioned data entry error. 


## fuck.

## Fuck. Fuck. 

## but it is good we looked at this. 

## to make things even more complicated, the GPS points in the specimen-based 
## spreadsheet ("CedrosFinalReview.7022012.xlsx") are slightly different than the GPS info 
## spread sheet ("CedrosGPSPoinst.xls")

## we need to hand verify the coordinates and elevation of each PsubP value. 

## how to do this intelligently? start with lowest numbered
## PsubP, 
## check its coordinates for duplicates
## try to figure out its true coordinates based on specimens sheets

## a function to check dups:

pts <- read.csv('subParcelSpatial.csv')

psubp <- 1.1

checkDups <- function(psubp){
    psubpRow <- pts[pts$PsubP==psubp,]
    perfMatch <- pts[pts$E==psubpRow$E & pts$N == psubpRow$N,]
    halfMatchE <- pts[pts$E == psubpRow$E & pts$N != psubpRow$N,]
    halfMatchN <- pts[pts$E != psubpRow$E & pts$N == psubpRow$N,]
    #print(paste("PsubP = ", psubp, sep=''))
    #print("perfect matches:")
    #print(perfMatch)
    #print("half matches:")
    #print("E")
    #print(halfMatchE)
    #print("N")
    #print(halfMatchN)
    return(rbind(perfMatch, halfMatchE, halfMatchN))
}

aa <- checkDups(1.1)

aa <- checkDups(6.3)

## make a function tells us how many points
## have the same coordinates as the input point

pts <- read.csv('subParcelSpatial.csv')


sink('lookingForDups.txt')
dupsList <- vector('list', length(pts$PsubP))
for (i in 1:length(pts$PsubP)){
    print(paste(i, pts$PsubP[i]))
    aa <- checkDups(pts$PsubP[i])
    print(nrow(aa))
    if (nrow(aa) > 1){
    dupsList[[i]] <- aa
}}
sink()


## meh, sort of worked... now, 
## let's look at these:

1  1.1  2
2  1.2  1
3  1.3  1
4  1.4  2
5  1.5  2
6  2.1  1
7  2.2  1
8  2.3  2
9  2.4  2
10 3.1  4
11 3.2  1
12 3.3  3
13 3.4  4
14 4.1  2
15 4.2  1
16 4.3  2
17 4.4  3
18 5.1  1
19 5.2  2
20 5.3  1
21 5.4  1
22 6.1  1
23 6.2  4
24 6.3  2
25 6.4  1
26 6.5  1
27 6.6  1
28 6.7  1
29 6.8  1
30 6.9  1
31 7.1  2
32 7.2  1
33 7.3  1
34 7.4  1
35 7.5  1
36 7.6  1
37 7.7  1
38 7.8  2
39 7.9  2
40 8.1  3
41 8.2  3
42 8.3  1
43 8.4  3
44 8.5  3
45 8.6  2
46 8.7  1
47 8.8  1
48 8.9  1
49 9.1  1
50 9.2  4
51 9.3  2
52 9.4  2
53 9.5  2
54 9.6  1
55 9.7  1
56 9.8  2
57 9.9  2
58 10.1 1
59 10.2 2
60 10.3 2
61 10.4 1

## 1.1:
dupsList[[1]]
## 1.1 is at the same spot as 6.3
## not sure which is right, but probably 
## 1.1 is correct, because 6.3 is listed as
## sendero oso, and for a higher elevation

## interesting, the second sheet of Ana's GPSPointsCedros.xls
## lists the 1.1 as N748045 E33792. 

## This is different. If we apply the above corrections (flip x and y, 
## add 10^7 m), this gives us E748045 N10033792, which is spot 
## on the entrance trail, which matches their description spot on. 

## hmm. Think I have been given some rotten data...

## which, if either, matches the collections? 


## 1.4
dupsList[[4]]
##   PsubP      E        N
## 4    1.4 747598 10034057
## 59  10.2 747598 10034057

## elevation for 1.4 listed as 1351 on the aster
## elevation for 10.2 listed as 1367 on the aster

## base de datos final puts 1.4 at 747986, 10033888
## base de datos final puts 10.2 at 747413 10033728  

## GPSpointsCedrosHoja2 says 1.4 is at 747986 10033888
## (on the camino chontal)
## GPSpointsCedrosHoja2 has no GPS data for 10.2
## says it is behind the greenhouse. 

## using hoja2/base de datos, this puts this point 
## almost exactly on the camino chontal trail

## a trend is forming. Seems like we used exactly 
## the wrong data. I just used the cleanest data I
## could find, I didn't see there was a difference...
## not my fault! :)

## let's look at a few more before we make the leap. 




5  1.5  2
8  2.3  2
9  2.4  2
10 3.1  4
12 3.3  3
13 3.4  4
14 4.1  2


## 4.3
dupsList[[16]]
##    PsubP      E        N
## 16   4.3 746470 10032679
## 45   8.6 746470 10033324

## base de datos 746459, 10032194  elev= 1618
## which makes a lot more sense. 

## does the base de datos generally disagree 
## with hoja1 of the gps data?
## and generally agree with hoja2?


17 4.4  3
19 5.2  2
23 6.2  4

## 24 6.3  2
dupsList[[24]]
## as above, I think 1.1 is more likely to be 
## correct here, if either are correct. 
## so where does 6.3 actually belong?
## they are all listed as being on the oso/inca 
## trail system, and as old forest sites

## on the first page of the GPS data
## its listed as N0748246 E0034127
## corrected, this means: 
## E0748246 N10034127

## if we check the second page of the GPS data,
## it's listed as N747140 E35018. 
## correct this to E747140 N10035018. 
## and it's spot on, lands close to Oso trail, 
## near the intersection of the inca, oso and new waterfall trails. 

## I wonder if this is a trend, that the second page
## is the most useful.




 

## also a different elevation. WTF. 

31 7.1  2
38 7.8  2
39 7.9  2
40 8.1  3
41 8.2  3
43 8.4  3
44 8.5  3
45 8.6  2
50 9.2  4
51 9.3  2
52 9.4  2
53 9.5  2
56 9.8  2
57 9.9  2

## 10.2
dupsList[[59]]
##    PsubP      E        N
## 4    1.4 747598 10034057
## 59  10.2 747598 10034057

## the only other data I have for this is 
## base dat final, 747413, 10033728
## which sort of makes sense. 

## matches the elevation ok, their data says 1340, 
## mine says 1326. 

## but the name makes no sense, at least not if 
## I am thinking about the same green house,
## which is basically at the main lc lodge. 

60 10.3 2


## so let's make a dataframe comparing the three sources
## of spatial data - the final report, and pages 1 and 2
## of the los Cedros GPS data:

## start with the final review:

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
fpRep = bdfDF[['Parcela', 'Subparcela','N','E' ]]
fpRep.dropna(axis=0, inplace=True, how='all')
PsubP = fpRep.Parcela.astype('str') + "." + fpRep.Subparcela.astype('str')
PsubP = PsubP.astype('float')
fpRep['PsubP'] = PsubP
fpRep.drop(['Subparcela'], axis=1, inplace=True)
#fpRep.drop(['Parcela', 'Subparcela'], axis=1, inplace=True)
fpRep.set_index('PsubP', inplace=True)
fpRep.drop_duplicates(inplace=True)
fpRep.rename_axis('', axis=1, inplace=True)
## flip n/e, give different name so as to not confuse anything
fpRep.rename(index=str, columns={"N":'FinalRep_E', "E":'FinalRep_N'}, inplace=True)
fpRep.FinalRep_N = fpRep.FinalRep_N + 10000000 ## fix northing numbers
fpRep.to_csv('finalReport2012Coords.csv')

## okay, so how does this compare to the gps data 
## we were using?

aa = pd.read_pickle('subParcelSpatial.p')
aa.rename(index=str, columns={'E':'GPSHoja1_E','N':'GPSHoja1_N'}, inplace=True)

bb = pd.concat([fpRep, aa], axis=1, sort=True)

bb.to_csv('coordinateComparison.csv')

## send this off to Ana for comment...

GPSpointsCedrosMapHoja1

## the finalreview pts look way better to me, so let's redo
## all the explicitly spatial tests with these.

#####################################################3




all(pca.hell2$tab == pca.hell$tab) ## yup, the pcas are exactly the same

## copy our text from the notebook
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", listw = lw, alpha=1)

## works here. try with the jupyter lw:
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", listw = lw2, alpha=1)

## hmmm, seems to have something to do with the lw object we're using. 
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", adjR2cum = 0.05, listw = lw, alpha=1)

nb2 <- chooseCN(coordinates(pJ2), type = 2, plot.nb = FALSE)

## this gives us the following:

## $summary
##   variables order         R2      R2Cum   AdjR2Cum pvalue
## 1     MEM28    28 0.02830765 0.02830765 0.01183829  0.009
## 2      MEM2     2 0.02816669 0.05647434 0.02393897  0.005
## 3      MEM1     1 0.02559362 0.08206796 0.03375575  0.008

## plot these:

#png(file='wholeCommMEMS.png', width=1500, height=600)
s.value(pJ, mem.lw[,c(1, 2, 28)], symbol="circle")
#dev.off()


## we can also look at the PCA axes individually with the multispati function
ms.hell <- multispati(pca.hell$li, listw = lw, scannf = F)
g.ms.maps <- s.value(pJ, ms.hell$li, symbol = "circle", ppoint.cex = 0.6)
## well, it picked up the forest contrast. That's encouraging. But not 
## really sure what to do with that. Keep going with the model
## selection method. 

## but generally not working here, when we try to forward select, everything is thrown out. 

#################

## try subsetting to old forest, and repeat:

load(file="oldForestComm.rda")
load(file="oldForestEnv.rda")
load(file="oldForestSp.rda")

mx <- min(oldForestSp[,2])
my <- min(oldForestSp[,3])
oldForestPts <- cbind(oldForestSp[,2] - mx, oldForestSp[,3] - my)

oldForestPts <- jitter(oldForestPts, amount=10)

s.label(oldForestPts, labels=oldForestSp$PsubP)

listw.explore()

library(adespatial);library(sp);library(spdep)

nb <- chooseCN(coordinates(oldForestPts), type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, oldForestPts)
fdist <- lapply(distnb, function(x) 1 - x/max(dist(oldForestPts)))
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

mem.lw <- mem(lw)

s.value(oldForestPts, mem.lw[,c(1, 5, 10, 20, 30)], symbol="circle")
s.value(oldForestPts, mem.lw[,c(1:5)], symbol="circle")

##############################################################3

## side note, I am really confused about the locations of 
## the various sites. At some point in the not-too-distant-future, 
## we need to come to an understanding of this. 

aa <- read.csv('specObs.csv')[,-1]
site5 <- unique(aa[aa$site==5,c('site','PsubP')])
rownames(site5) <- NULL
site6 <- unique(aa[aa$site==6,c('site','PsubP')])
rownames(site6) <- NULL
mem.lw <- mem(lw)

## okay, I think the new points clarify this quite 
## a bit 
##############################################################

library(vegan)
comM.hell <- decostand(oldForestComm, 'hellinger')
pca.hell <- dudi.pca(df = oldForestComm, scale = FALSE, scannf = FALSE, nf = 3)


mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw, )

## huh, we have to relax the alpha to see any interesting results. 
## up the perms, because we have functions on the edge of our cutoffs,
## some precision might help

mem.comM.sel.p20 <- mem.select(pca.hell$tab, 
    method="FWD", 
    listw = lw, 
    alpha=0.2,
    nperm=9999, 
    )


mem.comM.sel$summary$order
mem.comM.sel.p20$summary$order

## these are the same. It does not seem to be relaxing
## either the global or local alphas. Should probably 
## post about this. 

## oh well. Let's go with this. 

## so it seems like this does a global test of all mems first, if that test
## does not meet the alpha setting, even highly significant results 
## for single mems are not examined. Sort like a big anova before checking
## differences with a tukey.

## but that is the problem with damn frequentist methods. There is a strong 
## signal in this second MEM, it maps our pattern of the two forest types 
## really well, and this global alpha throws it out, if we are not careful.

#mm.comM.sel.all <- mem.select(pca.hell$tab, method="FWD", listw = lw, MEM.all=TRUE)

## even this MEM.all=TRUE doesn't tell us that one of the MEMs is highly 
## informative. Kind of an odd function. 

summary(mem.comM.sel)

mem.comM.sel$summary

##    variables order         R2      R2Cum   AdjR2Cum pvalue
## 1       MEM1     1 0.04396803 0.04396803 0.02776410  0.001
## 2       MEM2     2 0.03833651 0.08230453 0.05065986  0.001
## 3       MEM8     8 0.03662942 0.11893395 0.07256205  0.001
## 4       MEM3     3 0.03119336 0.15012730 0.08942211  0.001
## 5      MEM10    10 0.03012891 0.18025621 0.10573405  0.001
## 6       MEM5     5 0.02939932 0.20965553 0.12183948  0.003
## 7      MEM12    12 0.02703676 0.23669229 0.13587807  0.005
## 8       MEM6     6 0.02478950 0.26148180 0.14786361  0.003
## 9       MEM9     9 0.02096149 0.28244329 0.15581563  0.008
## 10     MEM14    14 0.02080523 0.30324852 0.16389822  0.010
## 11     MEM15    15 0.01957388 0.32282240 0.17080294  0.023
## 12     MEM11    11 0.01919429 0.34201669 0.17752087  0.017
## 13      MEM4     4 0.01917227 0.36118897 0.18449655  0.027
## 14     MEM13    13 0.01826007 0.37944903 0.19058570  0.036

## well, up to ~.38 of variance predicted by spatial patterns. 
## we are into some interesting results, my friends.

## lots of MEMs. plot these:

s.value(pJ, mem.lw[,mem.comM.sel$summary$order], symbol="circle")

## very cool. 

## let's summarize that MEM creation/selection pipeline:

## load packages
library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)
## get data:
pts <- read.csv('finalReport2012Coords.csv')[,-2]
pts <- pts[order(pts$PsubP),]
env <- read.csv('envOnly.csv')
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
## are these in order?
all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 
pJ <- as.matrix(pts[,2:3])
colnames(ptsMat) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
## sanity plot check:
#s.label(pJ)
## graphical interface for exploring neighborhoods:
##listw.explore()
## make SWG, with gabriel method:
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
## get the distances of each link:
distnb <- nbdists(nb, pJ)
## weight them, by their length. Here a linear weighting:
fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ)))
## combine nb and fdist: 
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)
## visual check of neighborhoods
s.label(pJ, nb=nb, pnb.edge.col='red')
## generate mems
mem.lw <- mem(lw)
## standardize/"linearize" rows of community matrix, make PCAs
comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)
## okay, select using these PCAs, see if the output makes sense. Use lots of 
## permutations, because we have a MEM on the edge of significance. 
mem.comM.sel <- mem.select(pca.hell$tab, 
    method="FWD", 
    listw = lw, 
    nperm=9999, 
    )

## check results
mem.comM.sel$summary
## plot:
s.value(pJ, mem.lw[,mem.comM.sel$summary$order], symbol="circle")


##### look at big picture again ########

## our todo list is something like:

## see if we can correct the point outside of 
## los cedros, Ana says this is incorrect

## check old forest NMS, groups seem funky to me

## examine differences in communities of seedlings and mature 
## trees - are they a view into the future?

## examine differences among blocks

## finish spatial analysis -  what were we trying to do again?
## get mems as predictor variables. 

## the classical next steps are to conduct a variation partitioning 
## process - changes in our community composition as predicted by:
## slope, aspect, elevation, distance to stream, land use 

## that means I need to generate more environmental data, and clean up 
## the land use question a bit. 

## land use right now is now well defined, and should be broken up into 
## at least two variables: 

## time since last  use 
## type of use (potrero vs agriculture?)

## right now the latter is already broadly defined, RCA vs. BS

## BS is old potrero, mostly, and RCA is intensive agriculture

## so maybe step one is correct small mistakes, the point outside 
## LC and the old forest groups

## step two is generate the other environmental data, 
## step three is complete the broad varpar
## step four check seedlings
## step five, try out the BC distance to Old forest vs. RCA within the BS
## (the pleuripotent hypothesis, alternate stable states) 

###### try to correct point outside los cedros ######

## how can we quickly plot our points and the los cedros 
## polygon?

## back into R. We need to know how to do this anyway, 
## to make all the MEM stuff look nicer.

## load all the libraries:
library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

## here is the example tutorial from ADE folks:

data('mafragh')

mxy <- as.matrix(mafragh$xy)
rownames(mxy) <- NULL

s.label(mxy, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = mafragh$Spatial.contour)

## works, so how do we adapt to this to our data?

## our points are here:
aa <- read.csv('finalReport2012Coords.csv')[,-2]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- aa$PsubP
colnames(aa) <- c('PsubP','E','N')
pts <- aa[,-1]

## how to plot the losCed polygon?

## we have a geojson, there must be an easy way to do this

## this is a horrible way to do this, but:
source('lcPolyGetCoords.R') 

Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")

lcPoly <- SpatialPolygons(list(Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")))
#save(lcPoly, file='lcPoly.rda')

s.label(pts, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = lcPoly)

## looks like 2.4 is the problem. 

pts['2.4',]

## to me it looks like the northing is off by about 1000m, 
## in order to bring it down to the 2.__ subparcels

## how does this look if we correct just this point, by subtracting
## 1000m from the Northing?

pts['2.4',]$N <- 10033474

s.label(pts, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = lcPoly)

## looks better. But looks like all of the 2.__ plots are outside the 
## reserve, still:

s.label(pts, label=NULL, ppoint.col = "darkseagreen4", Sp = lcPoly)

## not sure, but they are close. 

## let's leave them as they are, with that one point corrected. 
## we should probably rerun the spatial stuff with this point, it may 
## have made a difference. 

## that works. 
## can we do this with the MEMs? don't see why not. save that for later.


### debug old forest elevacion NMS ###

## for some reason, our old forest NMS that checks 
## the effects of elevation on plot, the results are reversed
## from my maps. 

load("oldForestComm.rda")
load("oldForestEnv.rda")

## groups look right in the environmentat data. 
## must simply be an ordering issue in the
## hulls

OFnms <- metaMDS(oldForestComm, try=40)
 
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']

## make a color ramp for this elevation data
minElev <- min(oldForestEnv$elevacion)
maxElev <- max(oldForestEnv$elevacion)
rangeElev <- maxElev - minElev
colrsRamp <- colorRampPalette(c(rgb(1,0,0,0), rgb(1,0,0,1)), alpha = TRUE)(rangeElev)
ElevColrs <- colrsRamp[oldForestEnv$elevacion - min(oldForestEnv$elevacion)]


png(file='oldForestNMSelevClust.png', height=800, width=800)
plot(XX, YY,
    col="black",
    pch=21,
    cex=4.5,
    bg=ElevColrs,
    xlab='NMS1',
    ylab='NMS2',
    main="Two forest types detected, with elevation gradient"
    )
ordihull(OFnms, oldForestEnv$clGr, col=c(3,4), lwd=2, lty=2)
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '1259 m', '1680 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, "transparent", "red"),
        #merge = FALSE,
        pt.cex = 3.0,
        cex = 2.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )
dev.off()

names(table(oldForestEnv$clGr))

aa <- identify(OFnms$points)

OFnms$points[aa,]

OFnms$points[identify(OFnms$points),]

## where is our lowest point on this?

oldForestEnv[order(oldForestEnv$elevacion),]

## looks like our lowest is 7.1

OFnms$points['7.1','MDS1']

points(OFnms$points['9.1','MDS1'], 
        OFnms$points['9.1','MDS2'],
        cex=5, 
        pch=21,
        col="blue")

## yup, all lines up. problem debugged. 

## what's next?

## we need environmental data.

### looks like we could have sed this:

install.packages('geojsonio')

## then try
geojson_sp()

######### generate more environmental data ########

## we need slope, aspect, distance to water,
## how can we generate these? 
 
## if we do this in python:

##### to plot polygon boudaries:

lcPoly = gpd.read_file('GIS/lcPoly.geojson')

help(lcPoly.boundary.plot)

lcPoly.boundary.plot(color='purple', linewidth=3)

## I think the other functions will be shapely type 
## operations

##### to get distance to stream
## one example is here:
## https://stackoverflow.com/questions/30740046/calculate-distance-to-nearest-feature-with-geopandas ##
## remember to upvote if it works

import geopandas as gpd

# read geodata for five nyc boroughs
gdf_nyc = gpd.read_file(gpd.datasets.get_path('nybb'))
# read geodata for international cities
gdf_cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))

# convert to a meter projection
gdf_nyc.to_crs(epsg=3857, inplace=True)
gdf_cities.to_crs(epsg=3857, inplace=True)
gdf_nyc.geometry.apply(lambda x: gdf_cities.distance(x).min())

##### to get slope and aspect ######

## try this in python. 

import pandas as pd 
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio
import rasterio.plot
import math
plt.ion()


## the boundaries are here:
lcpoly.bounds

## but we already have a good size DEM for this study, let's 
## try that first. Following <https://stackoverflow.com/questions/47653271/calculating-aspect-slope-in-python3-x-matlab-gradientm-function>

## upvote if works

## slope:

from osgeo import gdal
import numpy as np
import rasterio
from rasterio.plot import show_hist

help(gdal.DEMProcessing)

gdal.DEMProcessing('GIS/anaStudySlope.tif', 'GIS/anaPlotDEG.tif', 'slope')
slopeRast = rasterio.open("GIS/anaStudySlope.tif")
rasterio.plot.show(slopeRast)
## does not work as above...
rasterio.plot.show_hist(slopeRast) ## nada


help(gdal.DEMProcessing)
gdal.DEMProcessing('GIS/anaStudyAspect.tif', 'GIS/anaPlotDEG.tif', "aspect")
aspectRast = rasterio.open("GIS/anaStudyAspect.tif")
rasterio.plot.show(aspectRast)

rasterio.plot.show_hist(slopeRast) ## nada

## fuckit, none of that worked. Did a little pointNclick with 
## qgis, let's see if that worked:

slopeRast = rasterio.open("GIS/lcSlope.tif")
rasterio.plot.show(slopeRast)

aa = slopeRast.read(1)
## this makes a numpy array which could be handy, but we lose
## our spatial indexing?

## "index" might work here:
xx, yy = slopeRast.index(748667, 10033329)
aa[xx,yy]

## not sure if that works perfectly. Let's do a sanity 
## check with the DEM itself:

dem = rasterio.open("GIS/anaPlotDEM.tif")
demR = dem.read(1)

## this should be roughly the highest spot:
xx, yy = dem.index(745605,10038208)
demR[xx, yy]
## no nope...hmmm

## try working with the bigger aster image
lcDEM = rasterio.open("/home/daniel/Documents/LosCed/Ecuador_GIS/losced/losced_topography/LosCedAster.tif")
lcDEMR = lcDEM.read(1)

dir(lcDEM)

lcDEMR.max()

xx, yy = lcDEM.index(745605,10038208)
lcDEMR[xx, yy]
## nope...weird...

## try working backwards

## highest point is:
lcDEMR.max()

np.where(lcDEMR == lcDEMR.max())
lcDEMR[62,284]
## there it is. Where is this on our map?
np.where(lcDEMR == lcDEMR.max())
lcDEM.xy(62,284)
## well, that seems reasonable. 

## try again with the Ana DEM. Where is the
## highest point on the area of the Ana study?

dem = rasterio.open("GIS/anaPlotDEM.tif")
demR = dem.read(1)
np.where(demR == demR.max())

## these two functions are inverse:
dem.xy(7, 0)
## (745035.4095220644, 10035563.857169842)
dem.index(745035,10035564)
## (7, 0)

np.array(dem.index(745035,10035564))

demR(


demR.max()

## yup, lines up. In general, we have UTMs
## for our plots, how do we use these to 
## look up our elevations?

## a function would be good here. given an x and y 

def getZ(xx,yy,robj):
    robjR = robj.read(1)
    rc = robj.index(xx,yy)
    return(robjR[rc])

## try this out

np.where(demR == demR.min())
dem.xy(106,134)

getZ(749165.1614353219, 10032512.771811783, dem)

getZ(746975, 10035200, dem)
## in general, seems to work.

## while we're looking at elevations, how do our DEM elevations line up
## with their GPS elevations?

## our pt data:
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)

## add the elevation as measured from the GPS's, in ana's excel sheets:
envOnly = pd.read_csv('envOnly.csv')
envOnly.set_index('PsubP', inplace=True)
envOnly.sort_index(inplace=True)

(pts.index == envOnly.index).all()

## so can we use the above function to find elevations?

type(pts.geometry)

dir(pts.geometry)


## aspect
aspectRast = rasterio.open("GIS/lcAspect.tif")
rasterio.plot.show(aspectRast)
rasterio.plot.show(aspectRast)

## looks good. Now, how can we get the information for each subparcel?

envOnly['DEM'] = [ getZ(i.x, i.y, dem) for i in pts.geometry ]

## wow, some of these look pretty far off. Plot these:

aa = plt.scatter(envOnly['elevacion'], envOnly['DEM'])
plt.gca().set_xlabel('GPS Elevation')
plt.gca().set_ylabel('DEM Elevation')

## well, at least there is a strong correlation. 

## do this for the slope and aspect data:

envOnly['aspect'] = [ getZ(i.x, i.y, aspectRast) for i in pts.geometry ]

## and maybe we decompose this into two variables - eastern exposure (cos) and northern exposure (sin)?

## to get our geometry right, we need to change compass heading into classical 
## eculidean angles, where the angle is measured counterclockwise, starting from
## "horizontal", which in the case of a compass is straight north. 
## so shift all the angles 90% counterclockwise (subtract 90 degrees), then 
## take additive inverse (make it negative, because angles are measured 
## counterclockwise, unlike the compass heading)

angles = [ -(H - 90) for H in envOnly['aspect'] ]

## then get the sin (northern exposure) and cos (southern exposure):

envOnly['exposeE'] = [ math.cos(math.radians(i)) for i in angles ]
envOnly['exposeN'] = [ math.sin(math.radians(i)) for i in angles ]

## seems to work. 

######### slope #########

slopeRast = rasterio.open("GIS/lcSlope.tif")

rasterio.plot.show(slopeRast)

envOnly['slope'] = [ getZ(i.x, i.y, slopeRast) for i in pts.geometry ]

getZ(xx,yy,robj)

## get some plotting help here at:
http://matplotlib.org/api/axes_api.html?highlight=imshow#matplotlib.axes.Axes.imshow
http://matplotlib.org/api/axes_api.html?highlight=imshow#matplotlib.axes.Axes.contour

rasterio.plot.show(slopeRast)

## write this out as our new environmental matrix

######## todo 

## rerun existing spatial analyses with corrected point
## check permanovas of new environmental data
## do varpart analysis

#########################

## can we redo the turnover diagrams with the 
## revised point (2.4 moved 1km sout)

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
import geopandas as gpd
import rasterio
import rasterio.plot
import copy
import random
import math

specObs = pd.read_pickle('specObs.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
subParcelComm.index = subParcelComm.index.astype('float')
subParcelComm.sort_index(inplace=True)
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)
(pts.index == subParcelComm.index).all()
## get our original in there first:
fig, axes = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(12,8))
axes = axes.flatten()
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
axes[0].scatter(physDist, bcDist)
axes[0].plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
axes[0].set_title('Overall Turnover')
## now look at these individually:
soGrouped = specObs.groupby('habitat')
habs = list(soGrouped.groups.keys())
for h,i in enumerate(habs):
    print(i)
    print(h)
    obs_i = soGrouped.get_group(i)
    obs_iE = [ pts.loc[float(a)].geometry.x for a in obs_i.PsubP ]
    obs_iN = [ pts.loc[float(a)].geometry.y for a in obs_i.PsubP ]
    obs_i['E'] = obs_iE
    obs_i['N'] = obs_iN
    subParcelInObs_i = subParcelComm.index.isin(obs_i.PsubP)
    comm_i = subParcelComm[subParcelInObs_i]
    notObserved=~((comm_i == 0).all(axis=0).values)
    comm_i = comm_i.iloc[:,notObserved]
    pts_i = pts.loc[comm_i.index.values]
    iX, iY = pts_i.geometry.x.to_list(), pts_i.geometry.y.to_list()
    spDF_i = pd.DataFrame(np.array([iX, iY]).transpose(), columns=['X','Y'])
    physDist_i = sp.distance.pdist(spDF_i, metric='euclidean')
    bcDist_i = sp.distance.pdist(comm_i, metric='brayCurtis')
    axes[h+1].scatter(physDist_i, bcDist_i)
    X, Y = physDist_i.reshape(-1,1), bcDist_i.reshape(-1,1)
    axes[h+1].plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
    axes[h+1].set_xlabel('distance between subplots')
    axes[h+1].set_ylabel('BC dissimilarity')
    axes[h+1].set_title(f'{i} habitat turnover')

plt.tight_layout()

## okay, that did make a difference. 
## pain in the ass, a couple hours of my short life,
## but same could be said for all of this. 

## anyway, we should export this new pt data as a csv
## for R:

pd.DataFrame(np.array([pts.index, pts.geometry.x, pts.geometry.y]).transpose(), columns=['PsubP','X','Y']).head()

aa = pd.DataFrame(np.array([pts.index, pts.geometry.x, pts.geometry.y]).transpose(), columns=['PsubP','X','Y'])
aa.set_index('PsubP', inplace=True)
aa.to_csv('pts.csv')

## speaking of, time to head over to R to fix other spots with new point location...

##### debug map with clusters in R ##########

library(stats)
library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
## we need a distance matrix, BC
## does vegan give us what we need?
sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')

makeClustMap <- function(tree, k, labels=NULL){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('pts.csv', row.names='PsubP')
    pts$PsubP <- as.numeric(rownames(pts))
    pts <- pts[,c(3,1,2)]
    rownames(pts) <- NULL
    require(stats)
    ## get habitat type labels for tree
    if (is.null(labels)){lab <- NULL} else {
        if (labels=='hab'){
        lab <- vector(length = length(tree$labels))
        for (i in 1:length(tree$labels)){
            ind <- which(envOnly$PsubP==tree$labels[i])
            lab[i] <- as.character(envOnly$habitat[ind])
    }}}
    par(mfrow=c(1,2))
    plot(tree, labels=lab)
    clustGroups <- rect.hclust(tree, k=k, border=1:k)
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    plot(x=pts$X,
        y=pts$Y,
        col=clsp$gr,
        xlab='',
        ylab='',
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
        )
    legend('bottomright',
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft',
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

k <- 4 ## number of groups 


png(file='hclust4groups.png', width=1600, height=800)
makeClustMap(sPC.ward, k=4, labels='hab')
dev.off()

### fix mantel tests

pts <- read.csv('pts.csv', row.names='PsubP')

pts <- read.csv('pts.csv')

pts$PsubP <- as.numeric(rownames(pts))

pts <- pts[,c(3,1,2)]

aa <- read.csv('pts.csv', row.names=1)

library('vegan')
library('ecodist')

pts <- read.csv('pts.csv', row.names='PsubP')
physdist <- vegdist(pts, method='euclidean')
## make the community dissimilarity matrix
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
row.names(bb) <- bb$PsubP
bb <- bb[,-1]
all(aa$PsubP == rownames(bb))
braydist <- vegdist(bb, method='bray')
## do the global test:
global_mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test
global_mant_test 
## correlogram
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)

getDists <- function(comfile, physfile='pts.csv'){
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv(physfile, row.names=1)
    spM <- spM[order(rownames(spM)),]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),]
    spM <- spM[order(as.numeric((rownames(comM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        }
    else{print("something doesn't fit right")}
    }

## filenames
comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')
## start df
all_mant_test  <- global_mant_test

## test individually
par(mfrow=c(2,3))
for (i in 1:length(comms)){
    print(comms[i])
    dists.i <- getDists(comms[i])
    cgram.i <- mgram(dists.i[[1]], dists.i[[2]]) ## correlogram object
    plot(cgram.i, main = comms[i] )
    mant_test.i <- mantel(dists.i[[1]] ~ dists.i[[2]], nperm = 10000) ## overall test
    all_mant_test <- rbind(mant_test.i, all_mant_test)
}

getDists(comms[i])

getDists <- function(comfile, physfile='pts.csv'){
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv(physfile, row.names=1)
    spM <- spM[order(rownames(spM)),]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),]
    spM <- spM[order(as.numeric((rownames(spM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        } else {print("something doesn't fit right")}
    }

######## update mems 

library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

## get data:
pts <- read.csv('pts.csv')
env <- read.csv('envOnly.csv')
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
## are these in order?
all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 

load('lcPoly.rda')
pJ <- as.matrix(pts[,2:3])
colnames(pJ) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)

## sanity plot check:
s.label(pJ, labels=pts$PsubP, ppoint.col = "darkseagreen4", Sp = lcPoly)

class(coordinates(pJ))

nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
nbdists(nb, coordinates(pJ))

## can we move our lc polygon to match our relocated points on the 
## our coordinates are here:
load('lcPoly.rda')
xx <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolE']
yy <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolN']
## the minimums of our sample sites was here:
pts <- read.csv('pts.csv')
pJ <- as.matrix(pts[,2:3])
colnames(pJ) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
## transform our polygon points with these values?
xx <- xx - mx
yy <- yy - my
## and make a spatial polygon out of this?
lcPolyMoved <- SpatialPolygons(list(Polygons(list(Polygon(cbind(xx,yy))), ID="lcPolyMoved")))

s.label(pJ, labels=pts$PsubP, Sp = lcPolyMoved)

## what about our MEMs?

head(lcPoly@polygons[[1]]@Polygons[[1]]@coords)
class(lcPoly@polygons[[1]]@Polygons[[1]]@coords)

lcPolyOrigin <- SpatialPolygons(list(Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")))

str(lcPoly)

## this works for a background on our mems...

## now what?

## look at new environmental data, individually then with varpart

## so, back to the PERMANOVAs...

####### permanovas, new environmental data ############

##sfl two (three) new variables, slope, eastern and northern exposure

library(vegan)

envOnly <- read.csv('envOnly.csv')
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)


all(envOnly$PsubP == subParcelComm$PsuP)

sPC <- subParcelComm[,-1]

adonis(sPC ~ envOnly$elevacion)

adonis(sPC ~ envOnly$DEM)

## same, makes sense

adonis(sPC ~ envOnly$slope) ## very little, not significant

adonis(sPC ~ envOnly$exposeE) ## very little, definitely not significant

adonis(sPC ~ envOnly$exposeN) ## very little, definitely not significant

adonis(sPC ~ envOnly$toStream) ## statistically significant, but explains very little variance (r2 =.025)



## well, shit. I guess I had to check

## check the old forest only data:

load("oldForestComm.rda")
load("oldForestEnv.rda")

all(rownames(oldForestComm) == rownames(oldForestEnv))

adonis(oldForestComm ~ oldForestEnv$toStream) ## higher, r2 = .06, p=0.005
adonis(oldForestComm ~ oldForestEnv$slope) ## r2 = .04, p = .03 

adonis(oldForestComm ~ oldForestEnv$toStream * oldForestEnv$slope) 
## no significant interaction

adonis(oldForestComm ~ oldForestEnv$exposeE) ## NS, r2 small
adonis(oldForestComm ~ oldForestEnv$exposeN) ## NS, r2 small

## can we do an ordination of the distance to stream?

load('OFnms.rda')
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']
cGroup <- read.csv('cGroup.csv', row.names="PsubP")
oldForestClGr <- cGroup[row.names(oldForestEnv),]

minDtoS <- min(oldForestEnv$toStream)
maxDtoS <- max(oldForestEnv$toStream)
rangeDtoS <- maxDtoS - minDtoS
colrsRamp <- colorRampPalette(c(rgb(0,0,1,0.5), rgb(1,1,0,1)), alpha = TRUE)(rangeDtoS)
DtoSColrs <- colrsRamp[oldForestEnv$toStream - min(oldForestEnv$toStream)]
plot(XX, YY,
    col="black",
    pch=21,
    cex=3,
    bg=DtoSColrs,
    xlab='NMS1',
    ylab='NMS2',
    main="Two forest types detected, distance to stream"
    )
ordihull(OFnms, oldForestClGr, col=c(3,4), lwd=2, lty=2)
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '7 m', '461 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, colrsRamp[1], colrsRamp[maxDtoS - minDtoS]),
        #pt.bg = c(NA, NA, "transparent", "blue"),
        cex = 1.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )

## well, it does look like the type four forests tend to be 
## closer to streams

###### distance to stream #######

## let's do this in python, I think shapely 
## was made for this...

plt.ion()

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
paths = gpd.read_file('GIS/PathsLC.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
raster = rasterio.open("GIS/anaPlotDEM.tif")
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)

fig, ax = plt.subplots(1, 1)
hydro.plot(color='Blue', ax=ax)
lcPoly.boundary.plot(color='purple', linewidth=3,ax=ax)
paths.plot(color='red', linestyle='--', ax=ax)
rasterio.plot.show(raster, ax=ax)

## okay, so we need our distance from each ana point 
## to the closest spot on the river system:

help(pts.geometry[0].distance)

pts.geometry[0].distance(hydro.geometry[0])

pts.geometry[0].distance(hydro.geometry) ## nope. 

pts.geometry.distance(hydro.geometry[0]) 
## yup. seems like the many to one relationship works.

## so for us, we need:

hydro.geometry.distance(pts.geometry[0])

hydro.geometry.distance(pts.geometry[0]).min()

## to get this for all of our pts...

envOnly['toStream'] = [ hydro.geometry.distance(i).min() for i in pts.geometry ]
#envOnly.to_csv('envOnly.csv')

## sanity checks

envOnly.loc[[9.4,9.5]]

envOnly.loc[9.4]

buff=400
fig, ax = plt.subplots(1, 1)
pts.loc[[9.5,9.4]].plot(color='black', ax=ax)
hydro.plot(color='Blue', ax=ax)
ax.set_xlim([pts.loc[9.5].geometry.x -400,pts.loc[9.5].geometry.x + 400])
ax.set_ylim([pts.loc[9.5].geometry.y -400,pts.loc[9.5].geometry.y + 400])

envOnly.loc[2.4]
## seems okay

## some a permanova on it, add to text in above section

## a little bit important (R2 = 0.025), but not much info there. 




## well, we can tromp on to the variation partioning. 

## also should take a look at the blocks that ana made, see 
## if they describe all this well. R2 = 0.025), but not much info there. 

## well, we can tromp on to the variation partioning. 

## also should take a look at the blocks that ana made, see 
## if they predict any variation. 

###### varpart ##########################

## let's see if we have what we need to get a 
## preliminary variation partition done...


## example:
data(mite)
data(mite.env)
data(mite.pcnm)
# Two explanatory data frames -- Hellinger-transform Y
mod <- varpart(mite, mite.env, mite.pcnm, transfo="hel")

## community matrix
comm <- read.csv('subParcelComm.csv', row.names='PsubP')

## we have our environmental data frame:
envOnly <- read.csv('envOnly.csv', row.names='PsubP')

## we want to retain elevacion, habitat, n and e exposure, distance to stream...

env = envOnly[,c('elevacion', 'habitat', 'slope', 'exposeE', 'exposeN')]

## spatial dataframe should be our MEMs:
load('sigMEM.rda')

## we might also want to include our clustering results...

clust <- read.csv('cGroup.csv', row.names='PsubP')

## try 
vp_noClust <- varpart(comm, env, sigMEM, transfo="hel")
plot(vp_noClust, bg=2:5)

vp_Clust <- varpart(comm, env, sigMEM, clust, transfo="hel")
plot(vp_Clust)

## adding in the clustering results doesn't do much. 
## just curious, what do explain alone, via permanova?

adonis(sPC ~ clust$gr) ## explains about ~8% of variance

## most of this variance is also predicted by spatial
## and environmental variables. 

## well, there's a whole lot of noise in this system. 

## I'm used to that. I'm a microbial ecologist of sorts. 

## I wonder if the old forest story is a little bit cleaner?

## what next? repeat all the above for old forests?

library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

load(file='oldForestSp.rda')
load(file="oldForestComm.rda")
load(file="oldForestEnv.rda")

## old forest env data needs to be updated:

oldForestEnv <- envOnly[as.character(oldForestSp$PsubP),]
#save(oldForestEnv, file="oldForestEnv.rda")

## we need MEMs for the forest, also...

rownames(oldForestSp) <- oldForestSp$PsubP
oldForestSp <- oldForestSp[,-1]

oldForestSp

## oh, this analysis is still based on the original, erroneous 
## GPS points

dim(oldForestSp)
dim(unique(oldForestSp))
## yeah, repeats, bad data

## build new spatial data:
pts <- read.csv('pts.csv', row.names='PsubP')

oldForestSp <- pts[row.names(oldForestEnv),]
#save(oldForestSp, file='oldForestSp.rda')

load('ptsmxmy.rda')

oldForestSpRecentered <- cbind(oldForestSp[,1] - ptsmxmy[1], oldForestSp [,2] - ptsmxmy[2])
row.names(oldForestSpRecentered) <- row.names(oldForestSp)
oldForestCoords <- coordinates(oldForestSpRecentered)

## interactive
listw.explore()


## says do this:
nb <- chooseCN(oldForestCoords, type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, oldForestCoords)
fdist <- lapply(distnb, function(x) 1 - x/max(dist(oldForestCoords)))
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

## looks like:
s.label(oldForestCoords, nb=nb, pnb.edge.col='red')

## get old forest mems:
mem.oldForest <- mem(lw)

load('lcPolyMoved.rda')

s.value(oldForestCoords, mem.oldForest, symbol="circle", sp=lcPolyMoved)
## okay, worked

## now find the important ones:

oldForestComm[1:3,1:3]

dim(oldForestComm)

comM.hell <- decostand(oldForestComm, 'hellinger')

## just to be systematic..take all the axes greater than the mean of eigenvalues

ncol(oldForestComm)

pca.hell.all <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = ncol(oldForestComm))

sum(pca.hell.all$eig)

length(pca.hell.all$eig) ## told it to keep them all, why only 35? oh well...

meig <- mean(pca.hell.all$eig)

lines(c(0,50),c(meig,meig), col='red')

pca.hell.all$eig[pca.hell.all$eig > mean(pca.hell.all$eig)]

sum(pca.hell.all$eig > mean(pca.hell.all$eig))
## so maybe the first 12 are useful?

pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = TRUE)

pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 12)

all(pca.hell$tab == pca.hell.all$tab)
## no difference...
length(pca.hell$eig)
## ugh, again the ade4 functions aren't responding to my changes in arguments.
## don't know why that is, I really should write them

## we can extract these ourselves, I guess. But a little out of my league 
## here, because the downstream model selection is based on a new weighted
## dataframe of the sites and species that I don't know how to make.

## so just keep going, with all thirty five axes? seems like this may
## take forever?

## let's try it

mem.comM.sel <- mem.select(pca.hell$tab,
    method="FWD",
    listw = lw,
    nperm=999,
    )

## actually pretty quick.

## look at them:

s.value(oldForestCoords, mem.comM.sel$MEM.select, symbol="circle", Sp=lcPolyMoved)

mem.comM.sel$MEM.select

## huh, none of them are obvious to me. check some environmental vars:

aa <- rda(mem.comM.sel$MEM.select[,1], oldForestEnv$elevacion)

cor(mem.comM.sel$MEM.select[,1], oldForestEnv$elevacion)

head(oldForestEnv)


vp_noClust <- varpart(comm, env, sigMEM, transfo="hel")
plot(vp_noClust, bg=2:5)

vp_Clust <- varpart(comm, env, sigMEM, clust, transfo="hel")
plot(vp_Clust)

## then try to explain what is happening with secondary 
## forests 

## and look at seedling data - how good is this data? 
## can we use it to predict the future?


## anyway, to do:

## clean old forest analysis, it still has the old spatial data
## check new environmental variables on old forest, 
## run varpart for old forest
## maybe check clustering on old forest, see if it splits 
## cleanly in two when looked at in isolation

## check seedling data

############ add cluster results to entire community nms  ################

## before we go to far, let's see how our cluster results bear out
## on the all comm nms, we never did this. 

library(vegan)
library(RColorBrewer)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
envOnly = read.csv('envOnly.csv')
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

nms <- metaMDS(sPC, try=40)

options(repr.plot.width=5, repr.plot.height=5)
stressplot(nms)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
cGroup = read.csv('cGroup.csv')
rownames(nondups) <- NULL
all(nondups$PsubP == cGroup$PsubP)
nmsInfo <- merge(nondups, cGroup)
nmsInfo$MDS1 <- nms$points[,'MDS1']
nmsInfo$MDS2 <- nms$points[,'MDS2']

nmsInfo <- nondups

## plot it 

plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$gr,
    pch=19,
    cex=2.0,
)
text(nmsInfo$MDS1, nmsInfo$MDS2 - 0.015, nmsInfo$PsubP, cex=1.5)
## add legend
legend( x='bottomleft',
        legend = 1:4,
        fill = 1:4,
        cex = 1.0,
        bty='n',
        title='Cluster Group'
        )

## figure out lines only:
plot(1:5,rep(1,5), cex=3, pch=19, col=1:5)
legend( x='bottomright',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        col = c(1:5),
        cex = 1.0,
        lwd = 1,
        )



## add natural cluster hulls:
#ordihull(nms, nmsInfo$gr, col=1:4, lwd=3)

## or with habitat hulls:
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)


cGroup <- read.csv('cGroup.csv')

cGroup <- read.csv('cGroup.csv', row.names="PsubP")


############ check seedling data ################

## can we get our seedling info into a useable format?

## do this in python, methinks. 

python3
import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

wb.sheetnames

sheet = wb['Cedros Arb juv bosq']
dfBosque = pd.DataFrame(sheet.values)
dfBosque.columns = dfBosque.iloc[0]
dfBosque.drop(0,0, inplace=True)
## what do we need from this?
dfBosque = dfBosque[[ 'Norte ', 'Este', 'Elevación', 'Hábitat', 'Sitio/Site', 'Parcela/Plot', 'Sub.Parc.', 'Altura ', 'Familia', 'Genero ', 'Especie']]
## clean up the names a little. As per before, I think the northing and easting are mixed up, reverse here:
dfBosque.rename(index=str, columns=
        {'0':'', 
        'Norte ':'este', 
        'Este': 'norte',
        'Elevación': 'elevacion',
        'Hábitat': 'habitat',
        'Sitio/Site': 'site',
        'Parcela/Plot':"parcela",
        'Sub.Parc.':"subparcela", 
        'Altura ': 'altura',
        'Familia': 'familia',
        'Genero ': 'genero',
        'Especie': 'especie'},
        inplace=True)
## a lot of empty rows now...
dfBosque.dropna(how='all', axis=0, inplace=True)


## repeat the above for the regen plots:

wb.sheetnames

sheet = wb['Cedros Arb juv reg ']
dfRegen = pd.DataFrame(sheet.values)
dfRegen.columns = dfRegen.iloc[0]
dfRegen.drop(0,0, inplace=True)
## what do we need from this?
dfRegen = dfRegen[[ 'Norte ', 'Este', 'Elevación', 'Hábitat', 'Sitio/Site', 'Parcela/Plot', 'Sub.Parc.', 'Altura ', 'Familia', 'Genero ', 'Especie']]
## clean up the names a little. As per before, I think the northing and easting are mixed up, reverse here:
dfRegen.rename(index=str, columns=
        {'0':'', 
        'Norte ':'este', 
        'Este': 'norte',
        'Elevación': 'elevacion',
        'Hábitat': 'habitat',
        'Sitio/Site': 'site',
        'Parcela/Plot':"parcela",
        'Sub.Parc.':"subparcela", 
        'Altura ': 'altura',
        'Familia': 'familia',
        'Genero ': 'genero',
        'Especie': 'especie'},
        inplace=True)
## get rid of empty rows now...
dfRegen.dropna(how='all', axis=0, inplace=True)

(dfBosque.columns.values == dfRegen.columns.values).all()

## now join these two:

juvSpecObs = pd.concat([dfBosque, dfRegen])
juvSpecObs.reset_index(drop=True, inplace=True)

## look okay?
juvSpecObs.head(2)
juvSpecObs.tail(2)


## how do we make a unique taxonomic unit out of each species?
## needs to be compatible with the mature species dataframe...

## looks good. now we need to create unique familyGenusSpecies names 
## get rid of white spaces 

juvSpecObs = pd.concat([dfBosque, dfRegen])
juvSpecObs.reset_index(drop=True, inplace=True)
strCols=["familia", "genero", "especie", "habitat"]
for j in strCols:
    print(j)
    juvSpecObs[j] = juvSpecObs[j].str.strip()


juvSpecObs.especie.isnull().any() ## we have some empty species slots
juvSpecObs.genero.isnull().any() ## no empty genera slots
juvSpecObs.familia.isnull().any() ## no empty family slots
juvSpecObs.especie[juvSpecObs.especie.isnull()] = ""
juvSpecObs.especie.isnull().any() ## that worked... keep following our original data cleaning process
juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis', inplace=True) ## fix some errors where a genus name was left out
juvSpecObs.genero.replace('Saurauria', 'Saurauia', inplace=True)
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia', inplace=True) ## fix some errors where a genus name was left out
juvSpecObs['genusSpecies'] = juvSpecObs.familia + " " + juvSpecObs.genero + " " + juvSpecObs.especie
juvSpecObs['genusSpecies'] = juvSpecObs['genusSpecies'].str.strip()
juvSpecObs['PsubP'] = juvSpecObs["parcela"].astype('str') + "." + juvSpecObs["subparcela"].astype('str')
juvSpecObs['PsubP'] = juvSpecObs.PsubP.astype('float')
juvSpecObs = juvSpecObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","elevacion","habitat",]]
juvSpecObs.genusSpecies.replace("Indeterminada","Indeterminado", regex=True, inplace=True)
juvSpecObs['genusSpecies'] = juvSpecObs['genusSpecies'].str.replace('sp. ', 'sp.', regex=False)
juvSpecObs.sort_values('PsubP', inplace=True)
juvSpecObs.reset_index(drop=True, inplace=True)

## let's make sure our species names with "sp." match the original specObs

#juvSpecObs.to_csv('juvSpecObs.csv', index=False)

juvSpecObs = pd.read_csv("juvSpecObs.csv")

len(juvSpecObs.genusSpecies.unique()) ## 177 unique species

## how many of these are in our mature tree species list?

specObs = pd.read_pickle('specObs.p')

help(specObs.genusSpecies.unique) ## 

specObs.genusSpecies.unique() ## 

np.isin(juvSpecObs.genusSpecies.unique(), specObs.genusSpecies.unique())

len(np.isin(juvSpecObs.genusSpecies.unique(), specObs.genusSpecies.unique())) ## 177

sum(np.isin(juvSpecObs.genusSpecies.unique(), specObs.genusSpecies.unique())) ## 103 species found in mature tree data

## looks like about 70 of juvenile trees are not in the mature species lists
## that is a lot, considering there are only ~300 mature tree species, and
## the area sampled for these juvies is much smaller, I think.
## so the forest is changing, probably

## is this all real, or are there some typos causing some of this?
## how to check this...
 
jvUniq = juvSpecObs.genusSpecies.unique()

## these are the juv species species reportedly not in our mature species
notIn=~(np.isin(jvUniq, specObs.genusSpecies.values))
jvNotMat = jvUniq[notIn]

## just spotcheck a bunch of these, don't know a better way to do this...

## a lot these seem correct, but the indeterminada species I think may well 
## be the same as indeterminado in the mature tree species

## for now, assume they are the same

juvSpecObs.genusSpecies.replace("Indeterminada","Indeterminado", regex=True, inplace=True)

## also we have two "species" that are authorities:

juvSpecObs = pd.read_csv("juvSpecObs.csv")

## add this to above pipeline, also

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis')

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis')[466]
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia')[466]

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis',True)
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia', inplace=True)

## back up, put this in the formation of the table

## okay, so should be pretty compatible now...
## now what?

## we want to see how things are shifting over time. Are sites being pulled 

## maybe repeat our 

## seems like the first stop would be to get a community matrix for the juveniles

juvSpecObs = pd.read_csv('juvSpecObs.csv')

juvSpecObs.head()

aa = juvSpecObs[['PsubP', 'genusSpecies']]
aa.drop_duplicates(inplace=True)
juvDummy = pd.get_dummies(aa['genusSpecies'])
## that's presence/absence, not sure if we want that...
## for subParcelComm we did this:

aa = juvSpecObs[['PsubP', 'genusSpecies']]
aa.set_index('PsubP', inplace=True)
bb = pd.get_dummies(aa['genusSpecies'])
## okay, we need to collapse these 
bb.reset_index(inplace=True)
cc = bb.groupby('PsubP')
juvDummy = cc.agg(np.sum)

juvDummy.iloc[1:20,1:4]

juvDummy.sum(axis=1)

juvDummy.sum(axis=0)

juvDummy.head()

#juvDummy.to_csv('juvComm.csv')


juvComm = juvDummy

## that's nice, but we might want matrix that is compatible with the larger community matrix. 
## How can we add in our extra columns 

## let's compare to our other community matrix:

subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')

subParcelComm.head()

juvDummy.head()

subParcelComm.columns.isin(juvDummy.columns.values)

'Actinidaceae.Saurauia.sp..1' in subParcelComm.columns.values

'Actinidaceae.Saurauia.sp..1' in juvDummy.columns.values

'Actinidaceae Saurauia sp.1' in juvDummy.columns.values

## the column names are different...can we fix this? 
## recreate the community matrix from the mature trees:

aa = specObs[['PsubP', 'genusSpecies']].copy()
aa.PsubP = aa.PsubP.astype(float)
aa.sort_values('PsubP', inplace=True)
aa.PsubP = aa.PsubP.astype(str)
aa.set_index('PsubP', inplace=True)

bb = pd.get_dummies(aa['genusSpecies'])
## okay, we need to collapse these 
bb.reset_index(inplace=True)
cc = bb.groupby('PsubP')
specDummy = cc.agg(np.sum)

specDummy.sum(axis=0).all()
specDummy.sum(axis=1).all()

specDummy.columns.isin(juvDummy.columns.values)

len(specDummy.columns.isin(juvDummy.columns.values))
sum(specDummy.columns.isin(juvDummy.columns.values))

juvDummy.columns.isin(specDummy.columns.values)

len(juvDummy.columns.isin(specDummy.columns.values))
sum(juvDummy.columns.isin(specDummy.columns.values))

## does this community matrix produce the same results
## as our previous?

specDummy.index

#specDummy.to_csv('specDummy.csv')

#specDummy = pd.read_csv('specDummy.csv')

## take a look at an nms:

library(vegan)
library(RColorBrewer)

## our newest version of entire community matrix
specDummy <- read.csv('specDummy.csv', row.names='PsubP')
specDummy <- specDummy[order(as.numeric(row.names(specDummy))),]
nms <- metaMDS(specDummy, try=40)
#stressplot(nms) ## looks pretty similar to our other stresses from older version
## run this through the below graphics pipeline

## compare it with our previous, saved entire community matrix
subParcelComm <- read.csv('subParcelComm.csv', row.names='PsubP')
nms <- metaMDS(subParcelComm, try=40)

## and with a rerun version of the subparcelcomm pipeline
subParcelComm2 <- read.csv('subParcelComm2.csv', row.names='PsubP')
subParcelComm2 <- subParcelComm2[order(as.numeric(row.names(subParcelComm2))),]
nms <- metaMDS(subParcelComm2, try=40)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat)
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups
plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)
text(nmsInfo$MDS1, nmsInfo$MDS2, nmsInfo$PsubP, cex=1.5)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)

## fuck, these two dataframes are giving us slightly different results...
## why?

## the pipeline I have in the notebook goes like this:

specObs = pd.read_csv('specObs.csv')
smallSpecObs = specObs[['PsubP','genusSpecies']]
bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv('subParcelComm.csv') 


## 2) get columns of juveniles and mature trees to line up
## 3) add zero columns where the juveniles lack 

## then start poking around with the juvenile data

## if we need to reorder, this is the code
subParcelComm['PsubP'] = subParcelComm['PsubP'].astype('float')
subParcelComm.sort_values('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
#subParcelComm.to_csv('subParcelComm.csv')

## let's just delete the python object of this to 
## avoid confusion. 

## does R always insert the periods in the name? Run above NMS 
## pipeline again...yes 

## okay, just don't save over from R

## now, to make the mature and juvenile species data frames more compatible...

##  

juvSpecObs = pd.read_csv('juvSpecObs.csv')
subParcelComm = pd.read_csv('subParcelComm.csv')
juvComm = pd.read_csv('juvComm.csv')
specObs = pd.read_csv('specObs.csv')

subParcelComm.columns.isin(juvComm.columns.values)
len(subParcelComm.columns.isin(juvComm.columns.values))
sum(subParcelComm.columns.isin(juvComm.columns.values))

juvComm.columns.isin(subParcelComm.columns.values)
len(juvComm.columns.isin(subParcelComm.columns.values))
sum(juvComm.columns.isin(subParcelComm.columns.values))

## they share 104 species.  
## for a total of 417 species 

juvComm.shape

subParcelComm.shape

## seems like we want three matrices:
## juveniles
## mature trees
## combined

## as far as the combined, what is the point of truly 
## combining the juvenile and mature trees...? 
## maybe just use this for creating the other two. 


## regardless, I think all should have all the same rows (sites)
## and all the same species (columns). 

## how to do this...

## the combined matrix should be the easiest, and maybe we can
## recycle the columns and rows.

## an outer join is the default for concat, what happens 
## when we just use defaults:

aa = pd.concat([subParcelComm, juvComm], sort=False)

## this sorts the columns, integrating the new ones from juvenile data:
bb = pd.concat([subParcelComm, juvComm], sort=True)

## do we need to flatten the rows that have the same PsubP values?
## as per above, don't think so. This combo matrix can just be for 

## let's check this manually, in a text file:

dd = bb.columns.astype('str').copy().to_series()

with open('comboColumns.txt', 'w') as cc:
    for i in dd:
        cc.write(i + "\n")



## looking this over, the "sp." string is a problem. Somethings that are
## presumably the same are being counted as different.

 

## the creation of the two submatrices.

dd = bb.columns.copy().to_series()

dir(dd.str)

help(dd.str.replace)

ee = dd.str.replace('sp. ', 'sp.', regex=False)

len(ee) ## 417
len(ee.unique()) ## 411

## looks like 6 cases of a space after species potentially causing problems. 

## what next - got to clean the original species observation matrices, then rerun
## the whole community matrix pipelines, maybe all analyses that used these
## matrices (basically eveything in the notebook, jesus christ).

## but perhaps the best workflow here is to create the master, combined
## community matrix, that can be subset easily to the juvenile and mature
## trees

## if I do that, no need to individually create or correct the community matrices
## but not sure how to index it so I can do that easily...

## regardless the specobs and juvSpecObs need to be cleaned up. Next time...

## and now it's next time. so, let's try for both specObs and and juvSpecObs,
## then rerun the community matrix creation pipelines. 

## fixed, there are now 104 shared species among the juveniles and adults

##  so to start again, again:

bb = pd.concat([subParcelComm, juvComm], sort=True)

## how can we retain indices?

subParcelComm.set_index('PsubP', inplace=True)
juvComm.set_index('PsubP', inplace=True)

subParcelComm.index
juvComm.index

bb = pd.concat([subParcelComm, juvComm], keys=['adulto','joven'], sort=True)
## how can we get zeroes instead of NA's?
bb.fillna(0, inplace=True)

## get a better look at this...
bb.to_csv('comboMat.csv')

## for me, this looks good. If need to quickly subset by adult/juvenile?

bb.loc['adulto',:]

## you can pass the nested indices as tuples:
bb.loc[('adulto',9.2)]

## ok, that works well.

## now what?

## getting closer to the fun questions. 

## 
