## if we're using R, for big community matrices:
smat <- function(mat){return(mat[1:3,1:3])}

## clean gps points

## Ana's GPS points are not working for me. Going to keep some notes on what I think is going on here, 
## and what I do to try to fix.

## problem: when I load the first page of Ana's spread entitled "GPSpointsCedros.xls", 
## the points are far away from where I think they should be. 

## They sort of appear like they are UTM zone 17S, but I think perhaps that (1) E and N are flipped, 
## and (2) that the E column is off by 10^7 meters.

## #2 may be due to the use of a different UTM zone, not sure. But we'll try bringin them into a 
## range that makes sense here by adding 10^7 to all of them 

## #1 we'll just try flipping these axes, E becomes N, vice versa. 


### works! we're in business. 

################################################

## Ana's data is too big for me to see with my 
## spreadsheet software
## Get ana's data out the xls files. Maybe if we can 
## get it into individual sheets, we can manage 
## a little better with our software.
## or even better, get the various sheets into 
## dataframes

sudo pip3 install openpyxl

## py

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## read one of the wkbks in:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']


wb.sheetnames
sheet['A1'].value

## alternatively:

sheet.cell(row=1, column=2).value

## are we missing data?
sheet.max_row
sheet.max_column

openpyxl.utils.get_column_letter(40)

## doesn't look like it. Can we go to a pandas dataframe?

## probably best to delete header, for this exercise

sheet['A1'].value

sheet.delete_rows(1)

sheet['A22'].value

## looks okay. convert to a df?

df = pd.DataFrame(sheet.values)
df.columns = df.iloc[0]
df.drop(0,0, inplace=True)

## yup looks good. 

### try this with the larger excel document 

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb['Cedros bosq Circ plots']

sheet.max_row
sheet.max_column

df2 = pd.DataFrame(sheet.values)

df2.columns = df2.iloc[0]
df2.drop(0,0, inplace=True)

## definitely more reliable than the spreadsheet gui, libreoffice. 
## something really funny is going on with the libreoffice software on this. 
## anyway now  we gotta start really studying what Ana did...

## it's messy, a big pile of spreadsheets. If I had more time I'd 
## maybe try to make a proper database out of all this...

## but I don't
## so, what did they do?

########################################################

## what I can understand about Ana's spreadsheets:

####### BaseDeDatosCedrosEnvaSarah7012012.xlsx ########

#### Sheet 1 - Cedros bosq Circ plots ####

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

sheet = wb['Cedros bosq Circ plots']
cbcpDF = pd.DataFrame(sheet.values)
cbcpDF.columns = cbcpDF.iloc[0]
cbcpDF.drop(0,0, inplace=True)

## this looks like trees, shrubs, woody stuff. Each species is given a count, 

## all species unique? Any repeats?

cbcpDF.ESPECIE.duplicated()

## yup, most. So what does the count indicate?  
## those are not counts. "No de individuo" is a unique identifier for
## each tree 
## good, that will make the SAC easier

## can we isolate this into a single workbook? Then libreoffice might 
## digest it a little better...

#cbcpDF.to_csv('CedrosBosqCircPlots.csv')

## works, libreoffice wasn't loading the full sheet originally. 
## okay, so the circular plots are basically the "large woody plants" data,
## for one of the forest plots? But which type of forest?
## from Ana's report, it seems like there should be one for 
## bosques cerrados and bosques secandarios
## so there should also be a sheet for the gaps and the other type of forest...
## the habitat column all says "BC", so this is probably the 
## closed forest tree data.

cbcpDF.columns

cbcpDF.Hábitat.unique()

cbcpDF.Hábitat.duplicated()

cbcpDF.Hábitat.duplicated()

cbcpAllSpp

## check for number of unique species
## to do this for real, we'll need to deal with all the random white spaces
cbcpAllSpp = cbcpDF['Genero '] + " " + cbcpDF['Especie']
len(cbcpAllSpp.unique())  ## ~139 spp

######## sheet 2, "Cedros reg circ plots " #######

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros reg circ plots "] ## note they included a space in this name
crcpDF = pd.DataFrame(sheet.values)
crcpDF.columns = crcpDF.iloc[0]
crcpDF.drop(0,0, inplace=True)

## whoah, this has 1216 rows... that's a lot of plants...

crcpDF.to_csv("CedrosRegCircPlots.csv") 

crcpDF.Hábitat == "RG"

crcpDF.Hábitat == "BC"

crcpDF.Hábitat.unique()

## the habitat column indicates just two values, "RG" and "RCA"
## what do these mean?

crcpAllSpp = crcpDF['Genero '] + " " + crcpDF['Especie']
len(crcpAllSpp.unique())  ## 197

######## sheet 3, "Cedros Arb juv bosq" ####################

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
sheet = wb["Cedros Arb juv bosq"]

cajbDF = pd.DataFrame(sheet.values)
cajbDF.columns = cajbDF.iloc[0]
cajbDF.drop(0,0, inplace=True)
#cajbDF.to_csv("CedrosArbJuvBosq.csv") 
cajbDF.shape 
## 657 rows, 26 columns, different data
## spreadsheet only picks up ~509 rows
## all of these are from bosque cerrado

cajbDF.Hábitat.unique()

## all "BC"

## so this is the juvenile trees from closed forests, methinks

## it seems like these are botanical collections
## according to Ana's report, collections of juvenile trees 
## were only made in the inner, 5m x 5m parcels. 
## so it seems like this is the 5x5 closed forest plant collections.

cajbAllSpp = cajbDF['Genero '] + " " + cajbDF['Especie']
len(cajbAllSpp.unique())  ## 92

######## sheet 4, "Cedros Arb juv reg " ####################

## again, note the space in the name

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")
wb.sheetnames
sheet = wb["Cedros Arb juv reg "] ## note they included a space in this name
cajrDF = pd.DataFrame(sheet.values)
cajrDF.columns = cajrDF.iloc[0]
cajrDF.drop(0,0, inplace=True)
#cajrDF.to_csv("CedrosArbJuvReg.csv") 

## this looks like the complement to the above (the juvenile trees 
## from the closed forest). It looks like the unique identifiers 
## for parcel are continuous:

cajrDF.columns

## six regular parcels
cajrDF['Parcela/Plot'].unique() ## 1,2,3,4,5,10
cajbDF['Parcela/Plot'].unique() ## 7,6,8,9

cajrDF['Genero ']  ## fucking spaces. I will clean all these up if I do this project

## can we check for number of unique species?
## to do this for real, we'll need to deal with all the random white spaces



cajrAllSpp = cajrDF['Genero '] + " " + cajrDF['Especie']
len(cajrAllSpp.unique()) ## 89


######## sheet 5, "Cedros Arb juv reg " ####################

## this is not a data. But some good stuff here. Some of the habitat types are
## explained:

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

## BC Bosque cerrado
## BS Bosque secundario
## CLB Claro de bosque
## RG regeneración fincas agricultura y ganadería
## RCA Regeneración cañaveral

########## combine observations from 

## can we estimate the total unique tree species collected/observed?

help(pd.concat)

pd.concat

spp = [
cbcpAllSpp,
crcpAllSpp,
cajbAllSpp,
cajrAllSpp
]

sppC = pd.concat(spp)

len(sppC.unique())

type(sppC)
len(sppC)

## 315 species of tree recorded from these plots. 

## we need to start thinking about the proper unit for 
## construct sampling effort curves...

####### other workbook: "CedrosFinalReview.7022012.xlsx" #######

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
#bdfDF.to_csv("CedrosFinalReview.csv") 

bdfDF.columns

## number of unique species here?

bdfSG = bdfDF['Genero '] + ' ' + bdfDF['ESPECIE']

len(bdfSG.unique()) ##337


len(bdfDF.ESPECIE.unique()) ##337

bdfDF.Hábitat.unique() 

## 'RG', 'RCA', 'CLB', 'BS', 'BC'

bdfDF['Final site '].unique()

bdfDF['Parcela'].unique()

bdfDF['Subparcela'].unique()

## pretty similar to above. 

## so again, how do we make a SAC out of this? What are our rows?

## I'm going to guess that the most useful sample unit here is the 30x30
## plot, of which there should be 27, according to the report from 
## Mika. 

## how/where are these labeled in the spreadsheet?

## actually, it looks more like there are 61 sub-parcel/plot

## to get a unique id for these, we need to index both 
## plot and subplot together.

## but what are these sub-plots? the terminology is loose in the 
## explanation, but it looks like a "subplot" corresponds to one
## of the 30x30 events, with both of its smaller, nested plots. 

## I don't understand the placement of the plots. When I color code
## by sub-block, they mostly group together spatially but not 
## entirely. A few random points are far removed in each group.

## doesn't matter for the sac, but it will haunt us later if we don't
## figure that out, methinks.  

## for the moment, let's create a unique ID column and go to vegan

bdfDF.head(2)

PsubP = bdfDF.Parcela.astype(str) + "." + bdfDF.Subparcela.astype(str)

bdfDF['PsubP'] = PsubP

bdfDF.head(2)

## now, we need a simple community matrix, where rows are 
## unique IDs from this PsubP, and columns are the ESPECIE column.

## so maybe let's drop to these, ESPECIE and PsubP:

bdfDF['PsubP']

smallbdf = bdfDF[['PsubP','ESPECIE']]

bdfGroup = smallbdf.groupby('PsubP')
bdfGroup.agg(np.size)
## neat, that's basically the species richness of each subparcel


bdfDummy = pd.get_dummies(smallbdf['ESPECIE'])
## wow, that was easy.

## sanity checks
bdfDummy.iloc[0,] ## yup

bdfDummy.loc[:,'vs. Perebea angustifolia'] ## yup

bdfDummy['Leandra subseriata']

[0:5] ##yup

bdfDummy.iloc[0:3,0:5]

bdfDummy['Turpinia occidentalis (Sw.) G. Don '][2347:2351] 
bdfDummy.iloc[2347:2351,0:5]

## there are some serious whitespace issues with this data. 
## will need some time to clean up species names, etc.
## but for now, let's just get a curve for folks to look at.

## okay, add our PsubP back in there, make it our first column

##bdfDummy['PsubP'] = PsubP
## better:
bdfDummy.insert(0, 'PsubP', PsubP)

## ugh, my head has been out of pandas for a long time...I think we 
## need the grouby function, check this site:
## <https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm>

bdfDummyGroup = bdfDummy.groupby('PsubP')

subParcelComm = bdfDummyGroup.agg(np.sum)

##subParcelComm.to_csv("subParcelComm.csv")

## looks promising. 
## to vegan!



###################################

## R/vegan

R

library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)

## fix rownames 
rownames(subParcelComm) <- subParcelComm$PsubP
subParcelComm <- subParcelComm[,-1]


anaSAC <- specaccum(subParcelComm, method = "exact")

pdf(file="anaSubparcelSAC.pdf")
plot(anaSAC)
dev.off()

specpool(subParcelComm)

##########
    Species     chao  chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     343 516.3946 39.15056 483.6557 21.60728 566.8038 404.4403 10.54889 61
##########

Species,chao,chao.se,jack1,jack1.se,jack2,boot,boot.se,n
All,343,516.3946,39.15056,483.6557,21.60728,566.8038,404.4403,10.54889,61

## neat. lots of tree species

## what about some sort of turnover chart?

################

## what do want out of this data set?

## what is ana's hypothesis/goal
## what is mine?

## next steps:

## 1 - clean data 

## and then, not necessarily in this order:

## 2 - ordination - does gap/forest-age explain community?
## 3 - create vegetation zones 
## 4 - show community turnover. This will be perhaps the most important,
## this is pretty much just BC by distance classes. 
## also some sort of mantal's r correlogram
##     because the cloudforest will probably 
## 5 - compare to other places

#####################

## clean data ##

## what spreadsheets do we need? 

## so far, it looks like we can do with just the 
## 2012 data sheet: "Base dat final 6 29 2012"

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)

## basically, we need to hunt for white space. 

bdfDF.columns

## typos here:

## 'Final site ' 
## 'N. Actual '
## 'N. anterior '
## 'Genero '

## from this data, we want to retain:

specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
##rename, without white spaces in the names
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.head(1)

## we got some empty rows. Can we get rid of these?
specObs.tail(3)

aa = specObs.dropna(how='all', axis=0)
aa.shape
## drops 11 rows, is that right? 
## looks right, keep it:

specObs.shape
specObs.tail(15)

specObs.dropna(how='all', axis=0, inplace=True)
specObs.shape
specObs.tail(10)

## how do we search our columns for white space?

## can we apply strip to all the elements of Df?
 
specObs.genero.str.strip()


## that seems to work okay. do this for all string columns

strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()


specObs.head(5)
specObs.head(5)

## looks good. 
## we may need to get rid of accents, etc, in the text,
## but let's wait and see if they cause problems

## let's make a clean species/genus column, and see if our 
## old numbers still match, or if we had a bunch of duplicates
## that typos were making unique:



## what next? we want a physical distance matrix, and a 
## a BC distance matrix. 

specObs['genusSpecies'] = specObs.genero + " " + specObs.especie

specObs.head(2)

specObs['genusSpecies'].unique().shape

## yup, lost like thirty species. 

## this is getting complicated...
## let's get all the specObs creation commands here, at once:

## this is getting complicated...
## let's get all the specObs creation commands here, at once:

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
specObs = bdfDF[['Final site ', 'Parcela', 'Subparcela', 'Familia', 'Genero ', 'Especie', 'Hábito', 'ELEVACIÓN', 'Hábitat', ]]
specObs.columns = ['site', 'parcela', 'subparcela', 'familia', 'genero', 'especie', 'habito', 'elevacion', 'habitat']
specObs.dropna(how='all', axis=0, inplace=True)
strCols=["familia", "genero", "especie", "habito", "habitat"]
for j in strCols:
    print(j)
    specObs[j] = specObs[j].str.strip()

specObs.especie[specObs.especie.isnull()] = ""
specObs['genusSpecies'] = specObs.familia + " " + specObs.genero + " " + specObs.especie
specObs['genusSpecies'] = specObs['genusSpecies'].str.strip()
specObs['PsubP'] = specObs.parcela.astype('str') + "." + specObs.subparcela.astype('str')
specObs = specObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","habito","elevacion","habitat",]]
specObs['genusSpecies'] = specObs['genusSpecies'].str.replace('sp. ', 'sp.', regex=False)
specObs.genusSpecies.unique()
specObs.to_csv('specObs.csv')
#specObs.to_pickle('specObs.p')

#specObs = pd.read_csv('specObs.csv')

len(specObs['genusSpecies'].unique())

specObs.familia.isnull().any() ## nope
specObs.genero.isnull().any() ## also no empty genero cells. The problem is with the names of species


## there is a species that starts with "vs. "
## wtf?

## okay, let's regenerate our site x species matrix
## with the cleaned data:

smallSpecObs = specObs[['PsubP','genusSpecies']]
smallSpecObsGrouped = smallSpecObs.groupby('PsubP')

spRich=smallSpecObsGrouped.agg(np.size)
## ugh, keeps abbreviating, want to look at all the values
## looks like a max of 86 species at one subparcel
## can we plot this?

plt.ion()
plt.bar(spRich.index, spRich.iloc[:,0])
spRich.iloc[:,0].max() ## 86
spRich.iloc[:,0].min() ## 11
## we'll want to group that by gap/regen groups


## make matrix for SAC
smallSpecObs['genusSpecies']

bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv("subParcelComm.csv")
#subParcelComm.to_pickle('subParcelComm.p')

####### find x,y for all the PsubP values ##########

## read in the csv for the gps data:

spDF = pd.read_csv('GPSpointsCedrosHoja1.csv', parse_dates=True)

## for now, we only want a PsubP column, and N, E coordinates

spDF['PsubP'] = spDF['Parcela/Plot'].astype('str') + "." + spDF['Subparcela  /Subplot'].astype('str') 
spDF = spDF[['PsubP', 'E', 'N']]
spDF.set_index('PsubP', inplace=True)

## does that match? 
#subParcelComm = pd.read_csv("subParcelComm.csv")

subParcelComm = pd.read_pickle("subParcelComm.p")

spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
## indices are not in same order...


spDF.sort_index(inplace=True)
subParcelComm.sort_index(inplace=True)

## check matching order

spDF.index == subParcelComm.index
spDF.reset_index(inplace=True)
subParcelComm.reset_index(inplace=True)
pd.concat([spDF.PsubP, subParcelComm.PsubP], axis=1)
spDF.set_index('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
len(subParcelComm.PsubP)
len(spDF.PsubP)
## looks okay. 
spDF.to_csv('subParcelSpatial.csv')
spDF.to_pickle('subParcelSpatial.p')

## to R? or can we do our BC distances without vegan, here in python?

import scipy.spatial as sp

spDF.head(2)
subParcelComm.head(2)

## this is really what we need, I think:
physDist = sp.distance.pdist(spDF, metric='euclidean')

sqPhysDist = sp.distance.squareform(physDist)

bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')

physDist.shape
bcDist.shape

## plot these?

plt.scatter(physDist, bcDist)

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

from sklearn.linear_model import LinearRegression

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)


plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## okay, that's cool. 
## we should see if it is more dramatic if we focus 
## on the primary forest plots only



########################################

#### adding to Bitty's list ####

## Bitty maintains a plant list for Los Cedros.
## let's see how many of these are new 
## species for los Cedros

## read in Bitty's spreadsheet:

wb = openpyxl.load_workbook("BittyLosCedrosPlants.xlsx")
wb.sheetnames
sheet = wb["Plants"]
bittyDF = pd.DataFrame(sheet.values)

## it looks like we only need the scientific name and family from this. 
## go to just these:

bittyDF = pd.DataFrame(sheet.values)
bittyDF.columns = bittyDF.iloc[0,:]
bittyDF.drop(0,0, inplace=True) 
bittyDF[['Scientific name', 'Family']].head()
bittyDF=bittyDF[['Scientific name', 'Family']]
bittyDF.dropna(how='all', axis=0, inplace=True)
bittyDF['Scientific name'] = bittyDF['Scientific name'].str.strip()
bittyDF['Family'] = bittyDF['Family'].str.strip()

## check the unique values:

len(bittyDF['Scientific name'].unique())
bittyDF.shape

## how do we compare the Ana data to this?

specObs['genero'].unique()

## this is going to be an imperfect process. But can we whittle the problem 
## down to something manageable?

## split up bitty's scientific name into genus and species:

aa = bittyDF['Scientific name']
bitGen = aa.str.split(pat=' ', expand=True).iloc[:,0]
bitGen.isin(specObs['genero'].unique())

sum(bitGen.isin(specObs['genero'].unique())) ## ~100 genera that are in Bitty's data are in Ana's?

## What are they?

mask = bitGen.isin(specObs['genero'].unique())
len(bitGen[mask])
bitGen[mask]

## and vice versa?
anaGen = pd.Series(specObs['genero'].unique())

## can we get rid of 'sp.' genera?

mask = anaGen.str.contains("sp. *[0-9]+", case=False, regex=True)
anaGen[mask]

## that regex may be useful above for cleaning...

## how to edit this in place...
anaGen = anaGen.str.replace("sp. *[0-9]+", "unidentified", regex=True)

## kick out the unIDs
anaGen = pd.Series(anaGen.unique())

## okay, so are there genera that are represented in Bitty's data?
anaGen.isin(bitGen)

sum(anaGen.isin(bitGen)) ## 64 genera that are in Ana's data are in Bit

## 
~anaGen.isin(bitGen)

sum(~anaGen.isin(bitGen)) ## 74 more genera? can that be true?

## sanity checks
mask = (~anaGen.isin(bitGen)) 
#anaGen[mask]
anaGenNotBit = anaGen[mask]
anaGenNotBit.reset_index(inplace=True, drop=True)

## so we now for certain that all species within these are not on bitty's list.

## now we need to recover the full scientific name and family from Ana's list, 
## and family, and add these to Bitty's data, or at least put it in a form that 
## is easy to add.

## for now, keep it separate to make this easier for her?

specObs[recNotBit] ## NOT in bitty's genera, but yes in ana's

specObs[~recNotBit] ## in bitty's genera and ana's

## how to recover the full records from ana data by genus name:

recNotBit = (~specObs['genero'].isin(bitGen))
newGen = specObs[recNotBit]

## cleanout the genera with just "sp." values:
mask = ~(newGen['genero'].str.contains("sp. *[0-9]*", case=False, regex=True))
newGen[mask].shape
newGen = newGen[mask]

## so this the spreadsheet of new genera, 
## with all of the species within these.

## check 
## this should be ana's but not Bitty's
"Conostegia" in specObs.genero.values
"Conostegia" in bitGen.values
"Conostegia" in newGen.genero.values

## this should be in both
"Dacryodes" in specObs.genero.values
"Dacryodes" in bitGen.values
"Dacryodes" in newGen.genero.values

## seems to work. From newGen, we need the family, genus, species columns:
newGen = newGen[['familia', 'genero', 'especie']]

## how to we subset to just unique species values?

newGenGrouped = newGen.groupby(['genero','especie']).agg(pd.Series.unique)

newGen.head()
newGenGrouped.head()
newGenGrouped.shape

## what does a csv look like if we use this?

newGenGrouped.to_csv('newGeneraForBitty.csv')

#### ok, so how do we get the other plants? #####

## these other species will not be in the new genera, 
## so we need to look in the genera that are already
## covered. 

## This gets messy, probably, but the start is simple:

## these are the records that are of genera that are 
## in both Bitty's plant list and Ana's:

sharedGen = specObs[~recNotBit] 

## subset to family, genus, species
sharedGen = sharedGen[['familia', 'genero', 'especie']]

## collapse it:
sharedGen = sharedGen.groupby(['genero','especie']).agg(pd.Series.unique)

## now how to avoid duplicates with bitty's list?

## let's try joining our genus+species column, and matching to
## Bitty's scientific names:

sharedGen = sharedGen.reset_index()[["familia","genero","especie"]]

sharedGen['GenSpec'] = sharedGen.genero.str.cat(sharedGen.especie, sep=' ')

## search Bitty's df:

sharedGen['GenSpec'].isin(bittyDF['Scientific name'])

sum(sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 44 matches.

sum(~sharedGen['GenSpec'].isin(bittyDF['Scientific name'])) ## initially 153 spp not listed in Bitty's list

## sanity checks
"Rhodostemonodaphne cyclops" in bittyDF['Scientific name'].values
"Trichilia glabra" in bittyDF['Scientific name'].values

## okay, so how do we subset specObs to these?
newSpp = sharedGen[(~sharedGen['GenSpec'].isin(bittyDF['Scientific name']))]
mask = ~(newSpp['GenSpec'].str.contains("sp. *[0-9]*", case=False, regex=True))
newSpp = newSpp[mask]

## I think this is what bitty needs?
newSpp.to_csv('oldGeneraDifferentSpp4bitty.csv')

## bitty has noticed a particular record of interest. Get her the full story:



############## ridgetop paper data ########################

## there was an additional paper about biodiversity at los Cedros,
## and elsewhere, Wilson and Rhemtulla (2018).

## their data is public, let's look at it:

wb = openpyxl.load_workbook("doi_10.5061_dryad.3nr41__v1/prescence_abscence.xlsx")

wb.sheetnames
sheet = wb["Sheet1"]
ridgeDF = pd.DataFrame(sheet.values)

ridgeDF.rename(columns=ridgeDF.iloc[1,:], inplace=True)

ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.drop(ridgeDF.index[0], inplace=True)
ridgeDF.reset_index(inplace=True, drop=True)

columns=ridgeDF.iloc[1,:], inplace=True)

#aa = ridgeDF.columns.values
aa = ridgeDF.iloc[1,:], inplace=True)aa[0] = "family"
aa[1] = "gnusSpecies"

ridgeDF.columns[0]='Family'

## which is LC?

## and never mind, Bitty just informed me she has done all this work.

######################################

## okay, back to the turnover graphs...

## can we back up and redo the turnover graphs, split out 
## by habitat type?

## we need to split up both the specObs and spDF dataframes by 
## habitat. How

spDF = pd.read_pickle('subParcelSpatial.p')
specObs = pd.read_pickle('specObs.p')

## we want a set of smaller data frames from specObs, based on their habitat:
soGrouped = specObs.groupby('habitat')

soGrouped.groups.keys()

oldForestObs = soGrouped.get_group('BC')

oldForestObs.head()

oldForestObs.PsubP.unique() ## 12 subparcels of old forest. Is this enough?

## how can we add spatial info?
ofoN = [ spDF.loc[i].N for i in oldForestObs.PsubP ]
ofoE = [ spDF.loc[i].E for i in oldForestObs.PsubP ]
oldForestObs['N'] = ofoN
oldForestObs['E'] = ofoE

oldForestObs.head()

oldForestObs.tail()

## we need a species matrix for just this set of plots:

subParcelComm = pd.read_pickle("subParcelComm.p")
aa = subParcelComm.index.isin(oldForestObs.PsubP)
oldForestComm = subParcelComm[aa]

## any rows all-zeros?

(oldForestComm == 0).all(axis=0) ## lots of species that are not present in any of these plots
sum((oldForestComm == 0).all(axis=0)) ## 190, out of 343, species not represented in these
## but that's not bad, considering the ratio, this is just 1/5 of all plots, but 45% of tree spp
(oldForestComm == 0).all(axis=1) 
## no rows that have zero in all species. Makes sense, that would mean a 
## plot with no trees, classified as bosque cerrado

## I think it makes sense to remove these all-zero species columns:

(oldForestComm == 0).all(axis=0).values

notObserved=~((oldForestComm == 0).all(axis=0).values)

oldForestComm = oldForestComm.iloc[:,notObserved]

oldForestComm.shape ## 12,153 . Makes sense. 

(oldForestComm == 0).all(axis=1).any() ## no species that are not present at least somewhere

## okay, I think we can do our turnover graph

oldForestDF = spDF.loc[oldForestComm.index.values]

## got to get a spatial matrix just for the oldforest..


physDist = sp.distance.pdist(oldForestDF, metric='euclidean')
bcDist = sp.distance.pdist(oldForestComm, metric='brayCurtis')

physDist.shape
bcDist.shape

plt.ion()

plt.scatter(physDist, bcDist)

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

plt.xlabel('distance between subplots')
plt.ylabel('BC dissimilarity')
plt.title('old Forest')
plt.savefig('oldForestTurn.png')

## do a simple linear regression, though lots of assumptions are
## violated...independence, normality, 

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
plt.plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## works. can this be generalized to the other habitat types?

###### general turnover pipeline #########

specObs = pd.read_pickle('specObs.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
## old data: spDF = pd.read_pickle('subParcelSpatial.p')
## new data:
spDF = pd.read_csv('finalReport2012Coords.csv')
spDF['PsubP'] = spDF['PsubP'].astype('str')
spDF.set_index('PsubP', inplace=True)
spDF.drop('Parcela', axis=1, inplace=True)
spDF.rename(columns={'FinalRep_E':'E', 'FinalRep_N':'N'}, inplace=True)


## get our original in there first:
fig, axes = plt.subplots(nrows=2, ncols=3, sharey=True)
axes = axes.flatten()
physDist = sp.distance.pdist(spDF, metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

axes[0].scatter(physDist, bcDist)
axes[0].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 

## now look at these individually:
soGrouped = specObs.groupby('habitat')
habs = list(soGrouped.groups.keys())

for h,i in enumerate(habs): 
    print(i)
    print(h)
    obs_i = soGrouped.get_group(i)
    obs_iE = [ spDF.loc[a].E for a in obs_i.PsubP ]
    obs_iN = [ spDF.loc[a].N for a in obs_i.PsubP ]
    obs_i['N'] = obs_iN
    obs_i['E'] = obs_iE
    subParcelInObs_i = subParcelComm.index.isin(obs_i.PsubP)
    comm_i = subParcelComm[subParcelInObs_i]
    notObserved=~((comm_i == 0).all(axis=0).values)
    comm_i = comm_i.iloc[:,notObserved]
    spDF_i = spDF.loc[comm_i.index.values]
    physDist_i = sp.distance.pdist(spDF_i, metric='euclidean')
    bcDist_i = sp.distance.pdist(comm_i, metric='brayCurtis')
    axes[h+1].scatter(physDist_i, bcDist_i)
    X, Y = physDist_i.reshape(-1,1), bcDist_i.reshape(-1,1)
    axes[h+1].plot( X, LinearRegression().fit(X, Y).predict(X), c='k') 
    axes[h+1].set_xlabel('distance between subplots')
    axes[h+1].set_ylabel('BC dissimilarity')
    axes[h+1].set_title(f'{i} habitat turnover')

plt.tight_layout()

plt.savefig('turnoverByHabitat.png')

## well, withing the scale of comparisons that we have,
## this is not that interesting, except to say, damn, 
## these habitats are not good predictors of the 
## massive biodiversity that we are seeing here. 

## is it worth our time to do some mem analysis on this?
## what would be the advantages..?

## something to chew on...

#############################################################

## while we're here, SAC for each, to compare
## alpha diversity?

## what do we need for this?

## we need the community matrix for each habitat type, 
## exported as a csv, so we can head over to R and get into 
## vegan

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp

## grab the pipeline for the community matrix above:

specObs = pd.read_pickle('specObs.p')
spDF = pd.read_pickle('subParcelSpatial.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
smallDF = specObs[['PsubP','genusSpecies']]
dfDummy = pd.get_dummies(smallDF['genusSpecies'])
bdfDummy.insert(0, 'PsubP', specObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)

## to do one habitat type, we want a column that shows 
## the habitat type for each PsubP

aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)

## now groupby this?
bbGrouped = bb.groupby('habitat')

bbGrouped.groups.keys()


cc = bbGrouped.get_group('BC')

~(cc == 0).all(axis=0) ## or...

(cc != 0).any(axis=0)

observed = (cc != 0).any(axis=0).values
dd = cc.iloc[:,observed]

dd.drop('habitat', axis=1, inplace=True)

dd.to_csv('BC_comm.csv')

## let's generalize that to create community dataframes for each habitat
## type:


aa = specObs[['PsubP', 'habitat']].drop_duplicates()
aa.set_index('PsubP', inplace=True)
bb = pd.concat([aa, subParcelComm], axis=1)
bbGrouped = bb.groupby('habitat')
habs = list(bbGrouped.groups.keys())

for i in habs:
    cc = bbGrouped.get_group(i)
    observed = (cc != 0).any(axis=0).values
    dd = cc.iloc[:,observed]
    dd.drop('habitat', axis=1, inplace=True)
    dd.to_csv(f'{i}_comm.csv')

## in R

library(vegan)
library(RColorBrewer)

subParcelComm <- read.csv('BC_comm.csv', header=TRUE, row.names=1)
SAC <- specaccum(subParcelComm, method = "exact")
#pdf(file="BC_SAC.pdf")



plot(SAC)
#dev.off()
specpool(subParcelComm)

    Species     chao chao.se    jack1 jack1.se    jack2     boot  boot.se  n
All     137 250.2274  36.241 207.5833    22.91 253.5682 167.1898 10.28604 12

## so ~half of the estimated species will come from this type of forest.

## okay, can we repeat for the others, and build a single diagram out of 
## this?

#pdf("comboSACs.pdf")
par(mfrow = c(2,3))
files <- list.files()
comm <- grep('_comm.csv', list.files())
comms <- files[comm]
comms <- c("subParcelComm.csv", comms)
comtitles <- sub("_comm.csv", "", comms)
comtitles[1] <- "All habs"
j <- 0
for (i in comms){
    j <- j + 1
    comm.i <- read.csv(i, header=TRUE, row.names=1)
    print(i)
    SAC <- specaccum(comm.i, method = "exact")
    plot(SAC, main=comtitles[j], ylab='No. of Spp')
    capture.output(print(paste('Species estimates for', comtitles[j], sep=" ")), 
                    file="habSRestimates.txt", append = TRUE)
    capture.output(specpool(comm.i), 
                    file="habSRestimates.txt",
                    append = TRUE)
    sacDF <- data.frame(cbind(SAC$richness, SAC$sd), row.names=SAC$sites)
    colnames(sacDF) <- c('richness', 'sd')
    write.csv(sacDF, file=paste(comtitles[j], "SAC.csv", sep="_"))
}
#dev.off()

## so now we can go back to python, plot these together. 
## so we need to export appropriate data. These will be CSVs 
## of the sites, richness, and SD.

######## side note - NMS #########
## while we're in here, can we do an NMS of the habitat types,
## see if they group together?

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

## standard discrete color palette will work?

nms <- metaMDS(sPC, try=40)

## make plotting dataframe
specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat) 
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

head(nmsInfo)

## can we see what this looks like?

pdf(file='primenetNMSbyHabitat.pdf')
plot(nondups$MDS1, nondups$MDS2, 
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
## looks okay. How do we add hulls again?
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)
dev.off()

names(table(nmsInfo$habitat))

names(table(nmsInfo$colrs))

## do some checks

nmsInfo[identify(nondups$MDS1, nondups$MDS2),]


data(dune)
attach(dune.env)
ord <- metaMDS(dune)
plot(ord)
ordihull(ord, Management, col=1:4, lwd=3)
detach(dune.env)



names(table(nmsInfo$habitat))
names(table(nmsInfo$colrs))

nmsInfo$colrs

plot(1, col="#FF7F00", pch=19, cex=5)
##################################

## back to python, plot SAC curves

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
plt.ion()

## find our csvs of interest
SACs = [ i for i in os.listdir() if "SAC.csv" in i ]
sacDF = pd.read_csv('RCA_SAC.csv', index_col=0)

## plot this

fig, ax = plt.subplots(1,1)

def oneSacPlot(DF, clr):
    sacDF = pd.read_csv(DF, index_col=0)
    plt.plot(sacDF['richness'],
                color=clr)
    plt.fill_between(x=sacDF.index,
                     y1=sacDF.richness - sacDF.sd,
                     y2=sacDF.richness + sacDF.sd,
                    alpha=0.4,
                    color=clr,
                    )

colors = ['blue', 'green', 'red', 'yellow', 'orange', 'brown']

for colNu,DF in enumerate(SACs):
    oneSacPlot(DF, colors[colNu])


## the goal now is to plot these 

## make SACs

## make some useful SAC combo plots...then what? 

## indicator species for each type of habitat?

## the general story - deforestation knocks back a landscape into
## a pluripotent state? You may well end up with a very different

## wehere did the bosque secondario come from? Is it natural disturbance?
## blowdown, etc?
## or is it from deforestation? 

## If it is natural, then I think the story here is that deforestation
## changes the trajectory of succession...you may not recover your
## old growth, even with time, if you cut your forest. You will with 
## "natural" disturbance. Alternate stable states. 

## how would we test this hypothesis, with this data?

## not sure. Here the assumption would be that the ecosystem
## has alternate stable states, and that access to seeds
## from nearby plants is what will determine the the succession
## of the second growth. We might predict, then, that 
## regeneration of sites surrounded by forest will take 
## on old forest characteristics, and sites in clearings 
## will take on different species, showing more edge effects 
## and all that. 

## but I don't know if we have the ability to examine that 

## would be nice to get the ages on

## other ideas:
## let's try indicator species analysis, see what this turns up...

##############################################

## but first, we need to get our environmental data in order...

## where to start?

## none of the csvs that Ana has given me have environmental
## data other than elevation and habitat type.
## according to her report, we should also have light levels,
## "condiciones del clima" (=? did they have hobos at each site?)

## etc. very confusing. But let's start by getting a site x env 
## matrix with the data we have on our working dataframe:

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
plt.ion()

[ i for i in os.listdir() if '.p' in i ]

specObs = pd.read_pickle('specObs.p')

envOnly=specObs[['PsubP', 'elevacion', 'habitat']].drop_duplicates()


## odd, we have two elevation values for site 1.1: there is one collection
## from 1425m and the rest (~37 other collections) are from 1413 m. 
## I guess let's assume the singleton is the error?

envOnly=specObs[['PsubP', 'elevacion', 'habitat']].drop_duplicates()
envOnly.drop(1, inplace=True)
envOnly.set_index('PsubP', inplace=True)
#envOnly.to_csv('envOnly.csv')
#envOnly.to_pickle('envOnly.p')

## well, that's an environmental matrix, I guess. Not much there to work with. 
 
## we can check elevation as a predictor of community composition, permanova style...

## we also wanted to try clustering - can we recapture our habitat types using a 
## hierarchical clustering method?

######### hierarchical clusturing ##########

## back to R...

library(vegan)
library(stats)
 
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

## we need a distance matrix, BC
## does vegan give us what we need?

sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')



#sPC.ward.reorder <- reorder.hclust(sPC.ward, sPCBray)
## that function doesn't seem to exist in vegan anymore?

identify.hclust()

## also deprecated? can't find any of these...

## looks cool, but can we color by plot type?

## let's see if k=5 works, as this is the number of 
## habitat types that we have:

k <- 5

plot(sPC.ward,
hang=-1,
#xlab='5 groups',
ylab='height',
sub='',
main='',
#labels=cutree(sPC.ward, k=k), 
)

rect.hclust(sPC.ward, k=k)

## how we get our psubp values on there?

## the PsubP values are here:
sPC.ward$labels

## get our environmental matrix:
envOnly <- read.csv('envOnly.csv')

## and general info spreadsheet
specObs <- read.csv('specObs.csv')

## extract what our habitat types from this?


hab <- vector(length = length(sPC.ward$labels))
for (i in 1:length(sPC.ward$labels)){
    ind <- which(envOnly$PsubP==sPC.ward$labels[i])
    hab[i] <- as.character(envOnly$habitat[ind])
}

hablabels <- data.frame(cbind(sPC.ward$labels, hab))

head(hablabels)

head(sPC)

## some sanity checks
envOnly[envOnly$PsubP == 10.1,]
envOnly[envOnly$PsubP == 9.7,]

## generally seem to line up. So can we use this to label the
## leaves of the tree?

par(mfrow=c(1,1))
#png(file="wardCluster.png", width=900, height=500)
plot(sPC.ward,
hang=-1,
xlab='habitat',
ylab='height',
sub='',
main='',
labels=hab, 
)
aa <- rect.hclust(sPC.ward, k=3)
#dev.off()

rect.hclust(sPC.ward, k=3, lwd=4)

rect.hclust

## well, interesting. The RCA plots are on their own again...with some regen forest
## tagging along.

## can we look at these on a map?

envOnly <- read.csv('envOnly.csv')
## old data: pts <- read.csv('subParcelSpatial.csv')
pts <- read.csv('finalReport2012Coords.csv')[,-2]
pts <- pts[order(pts$PsubP),]

k <- 4
tree <- sPC.ward
jit <- 50 ## jitter value

hab <- vector(length = length(tree$labels))
## get labels

for (i in 1:length(sPC.ward$labels)){
    ind <- which(envOnly$PsubP==sPC.ward$labels[i])
    hab[i] <- as.character(envOnly$habitat[ind])
}


all(envOnly$PsubP == pts$PsubP)

makeClustMap <- function(tree, k, labels=NULL){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('finalReport2012Coords.csv')[,-2]
    pts <- pts[order(pts$PsubP),]
    require(stats)
    ## get habitat type labels for tree
    if (is.null(labels)){lab <- NULL}
    else {
        if (labels=='hab'){
        lab <- vector(length = length(tree$labels))
        for (i in 1:length(tree$labels)){
            ind <- which(envOnly$PsubP==tree$labels[i])
            lab[i] <- as.character(envOnly$habitat[ind])
    }}}
    par(mfrow=c(1,2))
    plot(tree, labels=lab)
    clustGroups <- rect.hclust(tree, k=k, border=1:k)
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    plot(x=pts$FinalRep_E, 
        y=pts$FinalRep_N, 
        col=clsp$gr,
        xlab='',
        ylab='',
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
    )
    legend('bottomright', 
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft', 
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

makeClustMap(sPC.ward, 2)

makeClustMap(sPC.ward, k=2, labels='hab')

makeClustMap(sPC.ward, k=4, labels='hab')

## interesting. forcing this to two groups does recapture this 
## result of RCA all by itself, with some of the regenerating 
## forest

## 5 groups - see if habitat types mostly split out as predicted.

png(file='hclust4groups.png', width=1600, height=800)
makeClustMap(sPC.ward, k=4, labels='hab')
dev.off()

clsp <- makeClustMap(sPC.ward, k=4, labels='hab')[,-2]
sum(clsp$habitat == 'RG' | clsp$habitat == 'RCA')

makeClustMap(sPC.ward, k=5, labels='hab')

legend('bottomright',
    legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
    pch=c(19, 21, 22, 17, 23)
)

legend('bottomleft',
    legend=1:k,
    pch=c(15),
    col=1:k,
    title='ClusterGroup')

## maybe build a function for this:

## also get a map behind this? 
## and there is an entire block of regenerating forest by itself

## can we add this cluster-group membership to our data?

## can we make a map of this, to see if there is a spatial pattern in these groups?

## starting to think that a PCNM analysis would be useful. Would probably show if 
## the eastern RCA plots are all on their own...

## this is going to start to involve some spatial analysis. It was inevitable.

## To start off, how to we get a map of our points?

## and we need to address the biodiversity questions - are we as diverse as Yasuni?
## how many species could we expect from the reserve as a whole? 

## also, wtf do we do with the permanent plot? Can we reconstruct using Danilo + Ana's
## data? I still do not understand what Ana did there...


############## get python packages ###########

## since internet is limited out here, let's see what packages we 
## can get while we got it

sudo pip3 install geojson
sudo pip3 install OGR
sudo pip3 install PyShp
sudo pip3 install dbfpy
sudo pip3 install Shapely
sudo pip3 install Fiona
sudo pip3 install GDAL
sudo pip3 install PIL
sudo pip3 install PNGCanvas
sudo pip3 install GeoPandas
sudo pip3 install PyMySQL
sudo pip3 install PyFPDF ## didn't work
sudo pip3 install Rasterio
sudo pip3 install osmnx ## didn't work


###################### make a map of points  ##############################

## we want to show the southeastern third of LC, with all of the plots in it
## we want to show elevation, streams, and our points, with their cluster
## info and their habitat info

## seems like matplotlib makes more sense to me here

import operator
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from osgeo import gdal, gdal_array, osr

from Pillow import Image ## why can't I import pillow? try fixing this with pip and internet connection

## a good first step is to see if we can read in our geotiffed DEG

## can we just read it in like a normal image?

plt.ion()

ax, fig = plt.subplots(1,1)

#deg="LosCedAster.tif" ## this doesn't. Is it an issue with the tif formatting?
deg="/home/daniel/Documents/LosCed/Ecuador_GIS/losced/LosCedOldMapGeoReffed/LosCed_topo.png" 
## yeah, because the above georeffed png works fine
img=mpimg.imread(deg)
plt.imshow(img)

## hmm, regardless, we probably need approach this is in a way that maintains 
## the integrity of the spatial coordinates. The coordinates are lost on png.
## there must be a way to use matplotlib for spatially explicit data 

## so I think we're jumping on this too early. let's work through a geopandas
## tutorial. I think we're going to need to do this right. 


import pandas as pd 
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio
import rasterio.plot
import copy
import random
plt.ion()

gpd.datasets.available

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')) 

world.head()
world.plot()

worldfiltered = world[world.continent == 'Asia']
worldfiltered.plot()
worldfiltered.plot(column = 'gdp_md_est', cmap='Reds')
world.plot(column = 'pop_est')

## that is badass. I love python. 

nybb = gpd.read_file(gpd.datasets.get_path('nybb')) 

cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities')) 

cities.plot()

help(plt.subplots)

fig, ax = plt.subplots(figsize=[4,4])

## this is more like what we need 

## so how can we work with our data? We want a geopanda
## for our sample sites

anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
## our cluster info is here:
cGroup = pd.read_csv("cGroup.csv")
## add this cluster info in
anaPt = anaPt.merge(cGroup, on="PsubP")
## make a vector for coloring the clustering results:
cdik={
1:"k",
2:"r",
3:"g",
4:"b"
}
anaPt['clustCols'] = [ cdik[i] for i in anaPt.gr ]
## where is our habitat data?:
envOnly = pd.read_csv('envOnly.csv')
anaPt = anaPt.merge(envOnly, on="PsubP")
## set the habitat symbol
msym_dik={
'BC':'o',
'BS':'p',
'CLB':'s',
'RCA':'^',
'RG':'D',
}
anaPt['habSyms'] = [ msym_dik[i] for i in anaPt.habitat ]

## that was easy. let's do that for our other layers of interest:
paths = gpd.read_file('GIS/PathsLC.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')

## now start plotting
fig, ax = plt.subplots(1,1)
for i in anaPt.habitat.unique():
    print(i)
    anaPtI = anaPt[anaPt['habitat'] ==  i].reset_index()
    anaPtI.plot(color=anaPtI.clustCols, 
                marker=anaPtI.habSyms[0], 
                ax=ax)

hydro.plot(color='Blue', ax=ax)

paths.plot(color='red', linestyle='--', ax=ax)

raster = rasterio.open("GIS/anaPlotDEG.tif")
rasterio.plot.show(raster, ax=ax)

## jitter?
## don't need this with corrected GPS info
## def jitt(x,σ=10):
##     ''' returns a jittered value using normal dist '''
##     xJit = x + random.gauss(0, σ)
##     return(xJit)
## 
## def jitCoords(gdf, σ=10):
##     ''' jitters a geopanda df. Amount of jitter is set 
##         with the 'σ' parameter'''
##     aa=copy.deepcopy(gdf)
##     xx = aa.geometry.x.apply(jitt, args=(σ,)).astype('int')
##     yy = aa.geometry.y.apply(jitt, args=(σ,)).astype('int')
##     aa.geometry = gpd.points_from_xy(xx,yy)
##     return(aa)



## looks like jit = 40 is a nice amount

## This is so easy. I'm in love. 
## how do we make a lengend?
help(ax.legend)
## this might be helpful:
http://matplotlib.org/users/legend_guide.html
http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists

## how can we trim down the rasters a bit?
## to clip this raster to size, I guess we need to work
## with the rasterio program in the shell?

#rio clip input.tif output.tif --bounds xmin ymin xmax ymax
rio clip LosCedAster.tif test.tif --bounds "745020 10032300 749161 10035795" --overwrite

## get forest cover raster for 2005?
## legend

## well, it's a nicer plot, but I am still unclear why 
## the bosques primarias split into two types. 

## let's run some indicator species analysis, or maybe cooccurrence
## networks if we need more sensitivity. 

## good cooccurrence network diagram would be worth looking at. 

## also permanova models for elevation, slope, aspect, proximity 
## to stream

## and I guess you gotta do a variance partitioning  analysis.
## which would include the dbmems...

## lots of work to do. 


######################## indicator species ######################

## let's see if we can get indicator species for our habitat types, 
## and/or cluster types

install.packages('indicspecies')

library(vegan)
library(indicspecies)

## check out the tutorial:

data(wetland)
groups = c(rep(1, 17), rep(2, 14), rep(3,10))
indval = multipatt(wetland, groups, control = how(nperm=999))
## seems simple

## our community matrix...
comM <- read.csv('subParcelComm.csv')
rownames(comM) <- comM$PsubP

## general plant observation dataframe
specObs <- read.csv('specObs.csv', row.names=1)
## make a PsubP/habitat dataframe:
specObs[,c('PsubP','habitat')]
habs <- unique(specObs[,c('PsubP','habitat')])
rownames(habs) <- NULL

## and our cluster groups:
cldf <- read.csv('cGroup.csv')

## make sure they are the same order
habs <- habs[order(habs$PsubP),]
comM <- comM[order(comM$PsubP),]
cldf <- cldf[order(cldf$PsubP),]
all(habs$PsubP == comM$PsubP)
all(habs$PsubP == rownames(comM))
all(cldf$PsubP == rownames(comM))

## now we can make a vector of our habs dataframe:
habsV <- habs$habitat

clV <- cldf$gr

## clean up extra column of com matrix, careful with this
#comM <- comM[,-1] ## get rid of PsubP

## and try out the indicator species function

habIndSpp <- multipatt(comM, habsV, func = 'r.g', control=how(nperm=9999))

clustIndSpp <- multipatt(comM, clV, func = 'r.g', control=how(nperm=9999))



###################### cooccurrence matrix ######################

## it's been a couple years, how do we do this again?

## we'll use the cooccur package again, they seem to be maintaining
## it, and people seem to be using it. I don't know the specific 
## advantages over the classic correlation approaches, but some 
## folks like this approach and it is computationally cheaper, 
## which is meaningful out here with just my little laptop. 

library(cooccur)

## example:

data(finches)
cooccur.finches <- cooccur(mat=finches,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

summary(cooccur.finches)

plot(cooccur.finches)

## simple enough

## need my community matrix
## think we need to transpose?

comMat <- t(read.csv('subParcelComm.csv', row.names='PsubP'))

## I don't know if abundance matters here. We'll leave it. 

coocc_thresh <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

## lots of warnings - try removing the threshold?
## might also be the abundance data causing problems.

coocc <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=FALSE,
                          spp_names=TRUE)
## takes way longer...
## hmm, now how do we visually this?

plot(coocc)
plot(coocc_thresh) 

## I think we want to add in the habitat types. 
## Then run a second analysis with cluster types,
## and maybe both. 

## what does this look like? species are rows, so we need a row 
## for each habitat type, and cluster type. 

## borrow the above code from the indicator species analysis:

library(cooccur)

## general plant observation dataframe
specObs <- read.csv('specObs.csv')
## make a PsubP/habitat dataframe:
habs <- unique(specObs[,c('PsubP','habitat')])
## and our cluster groups:
cldf <- read.csv('cGroup.csv')
## merge these
aa <- merge(habs, cldf, by='PsubP')
## habitat dummy vars
habDum <- model.matrix(aa$PsubP ~ aa$habitat)[,-1]
rownames(habDum) <- aa$PsubP
colnames(habDum) <- c('BS', 'CLB', 'RCA','RG')
BC <- c(rep(0, nrow(habDum)))
habDum <- data.frame(cbind(BC, habDum))
habDum$BC[rowSums(habDum) == 0] <- 1
## cluster dummy vars
clDum <- model.matrix(aa$PsubP ~ as.factor(aa$gr))[,-1]
rownames(clDum) <- aa$PsubP
colnames(clDum) <- c('cl2', 'cl3','cl4')
cl1 <- c(rep(0, nrow(clDum)))
clDum <- data.frame(cbind(cl1, clDum))
clDum$cl1[rowSums(clDum) == 0] <- 1
## I think we just need to transpose these and stack them on our 
## community matrix
comMat <- t(read.csv('subParcelComm.csv', row.names='PsubP'))
comMat <- rbind(t(habDum),t(clDum),comMat)

## visual checks:
cbind(habDum, aa$habitat)

cbind(clDum, aa$gr)
## works


dim()
dim(t(clDum))

## now rerun the cooccurrence analysis:
coocc_thresh <- cooccur(mat=comMat,
                          type="spp_site",
                          thresh=TRUE,
                          spp_names=TRUE)

aa <- prob.table(coocc_thresh)
bb <- aa[complete.cases(aa),]
cc <- aa[!complete.cases(aa),]
dd <- bb[,c('sp1_name', 'sp2_name','p_gt', 'sp1_inc', 'sp2_inc', 'obs_cooccur','prob_cooccur','exp_cooccur')]
ee <- dd[dd$p_gt <= 0.10,]


## what do we want? if we are interested in cooccurring species,
## we want a very small chance in the null model of seeing our 
## level of cooccurrence or higher. 

## so I think we want the p_gt statistic, for positive cooccurrences

## subset to p_gt <= 0.05

bb <- aa[aa$p_gt <= .05,]

dim(bb)

## okay, that looks like a bust. It is not as sensitive as our 
## indicator species analysis. I thought I was casting a wider 
## net with this method, but no bueno. The system is too 
## noisy, I guess. We could try correlations instead, but doesn't seem 
## like a good use of time right now.  

## seems like the best next steps are the mantel tests, permanovas, 
## then move onto the PCNMs and varpar analysis. 

###################################################

## mantel tests

install.packages('ecodist')

library('vegan')
library('ecodist')

## make the distance matrix 
## old data: aa <- read.csv('subParcelSpatial.csv', row.names="PsubP")[,-1]

## corrected coordinates:

cc <- read.csv('finalReport2012Coords.csv')

aa <- read.csv('finalReport2012Coords.csv')[,-2]
aa <- aa[order(aa$PsubP),]
physdist <- vegdist(aa, method='euclidean')
## make the community dissimilarity matrix
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
row.names(bb) <- bb$PsubP
bb <- bb[,-1]
all(aa$PsubP == rownames(bb))
braydist <- vegdist(bb, method='bray')

## correlogram
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
global_mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## can we subset this by habitat and by cluster group?
## this shows strong evidence of global autocorrelation

## what if we subset a little...

getDists <- function(comfile, physfile='finalReport2012Coords.csv'){ 
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv('finalReport2012Coords.csv')[,-2]
    spM <- spM[order(spM$PsubP),]
    rownames(spM) <- spM$PsubP
    spM <- spM[,-1]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),] 
    spM <- spM[order(as.numeric((rownames(comM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        }
    else{print("something doesn't fit right")}
    }

comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')

## start df
all_mant_test  <- global_mant_test

## test individually
par(mfrow=c(2,3))
for (i in 1:length(comms)){
    print(comms[i])
    dists.i <- getDists(comms[i])
    cgram.i <- mgram(dists.i[[1]], dists.i[[2]]) ## correlogram object
    plot(cgram.i, main = comms[i] )
    mant_test.i <- mantel(dists.i[[1]] ~ dists.i[[2]], nperm = 10000) ## overall test
    all_mant_test <- rbind(mant_test.i, all_mant_test)
}
plot(cgram, main = "global" )

comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')

rownames(all_mant_test) <- c(rev(comms), "global")



## The natural forest sites
## seem to be behaving similarly, classic positive autocorrelation
## dropping off to zero (with a dip into negative autocorrelation 
## around 1500m?), and the ag sites, especially RCA, are doing 
## weird stuff. In the turnover diagram, RG sites were 
## actually showing steeper turnover, with sites 
## increasing in dissimilarity more rapidly. The mantel 
## seems to show that here, too. 

## what happens if we exclude the weird RCA plots from the mantel, 
## and also when we examine the old forest sites together (BC+BS+CLB)

## exclude RCA plots:

## general plant observation dataframe
specObs <- read.csv('specObs.csv', row.names=1)
## make a PsubP/habitat dataframe:
habs <- unique(specObs[,c('PsubP','habitat')])
## order by site
habs <- habs[order(habs$Psub),]
rownames(habs) <- NULL

## let's look at the old-forest-only sites:

PsubPoldForest <- habs$habitat != 'RCA' & habs$habitat != 'RG'

aa <- read.csv('subParcelSpatial.csv')[-1]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- NULL
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
rownames(bb) <- NULL
all(habs$PsubP == aa$PsubP)
all(habs$PsubP == bb$PsubP)
## subset these:
aa <- aa[PsubPoldForest,]
bb <- bb[PsubPoldForest,]
all(rownames(aa) == rownames(bb))

## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## wow. that cleans up the story a bit:
##    mantelr      pval1      pval2      pval3  llim.2.5% ulim.97.5%
## 0.12648406 0.00810000 0.99200000 0.00970000 0.05079009 0.19028648


## let's include the regen sites:

## to get the sites that are not RCA:
PsubPNotRCA <- habs$habitat != 'RCA'

aa <- read.csv('subParcelSpatial.csv')[-1]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- NULL
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
rownames(bb) <- NULL

all(habs$PsubP == aa$PsubP)
all(habs$PsubP == bb$PsubP)

## subset these:
aa <- aa[PsubPNotRCA,]
bb <- bb[PsubPNotRCA,]
all(rownames(aa) == rownames(bb))

## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test
## make dist objects
physdist <- vegdist(aa, method='euclidean')
braydist <- vegdist(bb, method='bray')
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)
mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test

## doesn't do much, the regen sites also really confuse things, 
## they really are a different system. 

## let's run permanovas on the data we have. Still need to 
## create distance-to-stream data, might be useful. 

## but after that, time to dive into PCNM and varpart. Ugh...

## but first, scratch an itch - there is some detective work 
## to be done with the RG points that went outside their 
## cluster

############# fishing for stray RG points  #######################

## we need to rebuild our NMS and cluster results a bit, and see
## why which RG points clustered with the RCA plots, and which 
## clustered with the old forest plots:

## first, figure out the cluster results. Rebuild cluster graphic:

library(vegan)
library(stats)

## redo the tree

k <- 4
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')

png(file='hclust_4groups_PsubP.png', width=1200, height=800)
plot(sPC.ward)
clustGroups <- rect.hclust(tree, k=k, border=1:k)
dev.off()

## from our other plots, we can figure out the 
## the following RGs cluster with the RCAs:
RGwithRCA <- c(1.1, 1.2, 1.3, 1.4, 1.5)
## and the RG that clustered with the old forest:
RGwithOB <- 10.1

## where are these?
## both on the physical map, and in the NMS?

## bring up the NMS again, label with point names:

library(vegan)
library(RColorBrewer)
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
nms <- metaMDS(sPC, try=40)

stressplot(nms)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat)
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)

ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)

aa <- identify(nmsInfo$MDS1, nmsInfo$MDS2)

## too slow can we just label the points on there?
text(nmsInfo$MDS1, nmsInfo$MDS2, nmsInfo$PsubP)

## where is 10.1 on the map?
pts <- read.csv('subParcelSpatial.csv')[-1]


makeMap <- function(jit=50){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('subParcelSpatial.csv')
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    pts <- pts[order(pts$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    eJit=jitter(clsp$E, jit)
    nJit=jitter(clsp$N, jit)
    plot(x=eJit,
        y=nJit,
        col=clsp$gr,
        xlab='',
        ylab='',
        xlim=c(745000,749000),
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
    )
    legend('bottomright',
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft',
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

makeMap(100)
points(pts[pts$PsubP == 10.1,2:3], 
    pch=21, 
    cex=4, 
    col=6, 
    lwd=2, 
    bg="transparent")


## weird. 10.1 more closely resembles an old forest, as it is 
## clustered with the forest groups both in the NMS and the 
## the hclust. 

## but it is situated right next to other regenerating sites 
## that cluster more closely to the nefarious RCA plots 
## and the 

## what do we think decides this?

## Ana mariscal says age - when was it last farmed?
## we need that as a variable, can this be constructed? 

## rebuild the dataframe with habitat, etc:
clsp <- makeClustMap(sPC.ward, k=4, labels='hab')[,-2]
sum(clsp$habitat == 'RG' | clsp$habitat == 'RCA')
## we have 25 subparcels whose history we need to reconstruct. 

############# permanovas ######################

## back up a little to some of the basics

## can we predict differences in our sites by
## the environmental data we have?

library(vegan)

envOnly <- read.csv('envOnly.csv')

envOnly <- envOnly[order(envOnly$PsubP),]

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
sPC <- sPC[order(as.numeric(rownames(sPC))),]


## try habitat and elevation:

aa <- adonis(sPC ~ envOnly$elevacion) ## r2 = 0.08, p < .001
aa <- adonis(sPC ~ envOnly$habitat) ## r2 = 0.19, p < 0.001
aa <- adonis(sPC ~ envOnly$elevacion, strata=envOnly$habitat) ## no errors thrown, but don't think it worked...

## ugh, don't want to learn about the permute package. I think that's
## what we have to do to really get into this strata argument.

aa <- adonis(sPC ~ envOnly$elevacion*envOnly$habitat)  
bb <- adonis(sPC ~ envOnly$habitat*envOnly$elevacion)  

##                                   Df SumsOfSqs MeanSqs F.Model      R2 Pr(>F)
## envOnly$elevacion                  1    1.8357 1.83572  5.9572 0.07945  0.001
## envOnly$habitat                    4    4.1072 1.02679  3.3321 0.17776  0.001
## envOnly$elevacion:envOnly$habitat  4    1.4468 0.36170  1.1738 0.06262  0.090
## Residuals                         51   15.7157 0.30815         0.68017
## Total                             60   23.1054                 1.00000
## 
## envOnly$elevacion                 ***
## envOnly$habitat                   ***
## envOnly$elevacion:envOnly$habitat .

## some variance explained. 

################# focus on old Forest ###################

## just checking, is the story clearer if we subset to just old forest? and to just RG?

## just old forest, with no zero columns
oldF <- envOnly$habitat != 'RG' & envOnly$habitat != 'RCA'
## add in cluster membership:
load('cGroup.rda')
all(cGroup$PsubP == envOnly$PsubP)
envOnly$clGr <- cGroup$gr
oldForestEnv <-  envOnly[oldF,]
rownames(oldForestEnv) <- NULL
oldForestComm <- sPC[oldF,]
all(rownames(sPC) == envOnly$PsubP )
nonzs <- colSums(oldForestComm) != 0
oldForestComm <- oldForestComm[,nonzs]
all(oldForestEnv$PsubP == rownames(oldForestComm))

## just for kicks, get the spatial matrix, too:

oldForestSp <- read.csv("subParcelSpatial.csv")
oldForestSp <- oldForestSp[oldForestSp$PsubP %in% oldForestEnv$PsubP,]
all(oldForestSp$PsubP == oldForestEnv$PsubP)
#save(oldForestSp, file='oldForestSp.rda')


#save(oldForestComm, file="oldForestComm.rda")
#save(oldForestEnv, file="oldForestEnv.rda")

bb <- adonis(oldForestComm ~ oldForestEnv$elevacion*oldForestEnv$habitat)  
bb <- adonis(oldForestComm ~ oldForestEnv$habitat*oldForestEnv$elevacion)  


## when you subset to old forest, only elevation matters. 
## makes sense. The sites are interchangeable. 

## given that, what does an ordination of the old forest look like, 
## if we put elevation on there? and do the hierarchical clusters
## separate cleanly?

library(vegan)

OFnms <- metaMDS(oldForestComm, try=40)
stressplot(OFnms)
## as usual, not great, but still generally under 0.3

## make a color ramp for this elevation data
minElev <- min(oldForestEnv$elevacion)
maxElev <- max(oldForestEnv$elevacion)
rangeElev <- maxElev - minElev
colrsRamp <- colorRampPalette(c(rgb(1,0,0,1), rgb(1,0,0,0)), alpha = TRUE)(rangeElev)
ElevColrs <- colrsRamp[oldForestEnv$elevacion - min(oldForestEnv$elevacion)]

# get points
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']

png(file='oldForestNMSelevClust.png', height=800, width=800)
plot(XX, YY, 
    col="black", 
    xlab='NMS1',
    ylab='NMS2',
    pch=21, 
    cex=3, 
    bg=ElevColrs,
    main="Two forest types detected, with elevation gradient",
    )
## can we add cluster group data to the plot?
## group order will be:
names(table(oldForestEnv$clGr))
## so this should work:
ordihull(OFnms, oldForestEnv$clGr, col=c(3,4), lwd=2, lty=2)
## yup, that's intriguing. 
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '1259 m', '1680 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, "transparent", "red"),
        #merge = FALSE,
        cex = 2.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )
dev.off()


aa <- identify(OFnms$points)
OFnms$points[aa,]

### check habitat type on old forest nms ###

colz <- vector()
for (i in oldForestEnv$habitat){
    if (i == 'BC'){colz <- c(colz, '#E41A1C')} 
    if (i == 'BS'){colz <- c(colz, '#377EB8')}
    if (i == 'CLB'){colz <- c(colz, '#4DAF4A')}
}

## just checking
cbind(as.character(oldForestEnv$habitat), colz)

#png(file=, height=800, width=800)
plot(XX, YY, 
    col="black", 
    xlab='NMS1',
    ylab='NMS2',
    pch=21, 
    cex=2, 
    bg=colz,
    main="Old forest sites by habitat",
    )
legend( x='topright',
        legend = c('BC', 'BS', 'CLB'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A'),
        cex = 1.0,
        )

        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
names(table(oldForestEnv$habitat))

## not even going to try to put hulls on that. Pretty much randoms:

fh <- factor(oldForestEnv$habitat)
adonis(oldForestComm ~ fh)

#### ideal forest community ######

## we need some way to quantitatively characterize the old forest basin, 
## and measure if a sight may be moving toward or away from that state.

sPC[,'Cyatheaceae.Alsophila.erinacea', drop=FALSE]

aa <- cbind(envOnly$PsubP,oldF, sPC[,'Cyatheaceae.Alsophila.erinacea'])

###########  dbMEM analysis ################

## been a while, so let's work through the dbMEM materials out there
## looks like ade4 and associated packages are the way to go. 

## where has this package been all my (graduate) life?

library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

data("mafragh")

names(mafragh)

dim(mafragh$flo)

head(mafragh$env)

## check out the spatial data
mxy <- as.matrix(mafragh$xy)

rownames(mxy)
rownames(mxy) <- NULL

s.label(mxy, ppoint.col='darkseagreen4', Sp=mafragh$Spatial.contour)

class(mafragh$Spatial.contour)

## this is great, but what do we want to do with our data?

## for now, we want to define a spatial neighborhood of our points

## in the near future, I think we will need to define watershed polygons
## and look for an effect by microsite

## let's try running our point data through their dbMEM pipeline. 

## we can do this in three cycles - entire data set, forest only, and regen only

## start with entire, I guess. Our sampling scheme is irregular, looks like 
## this:

## pts <- read.csv('subParcelSpatial.csv') ## bad data!!

###### new data here: ########3
pts <- read.csv('finalReport2012Coords.csv')[,-2]
###############################

pts <- pts[order(pts$PsubP),]

env <- read.csv('envOnly.csv')


## are these in order?
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 

pJ <- as.matrix(pts[,2:3])
colnames(ptsMat) <- c('x','y')


### jitter these? Having different data on same points will be problem:
#set.seed=1
#pJ <- jitter(ptsMat, amount=10)
### not working - what if we bring everything closer to axes?

mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)

s.label(pJ)
## looks okay.

## now we need to define what a neighbor is. 
## in the past, we used the minimum spanning tree, recommended
## by borcard and legendre. 

## but these are not the only options

## there is an interactive program to explore these kinds of questions,
## let's see if it works:

listw.explore()

## okay, for the moment let's try the "gabriel" graph type. I don't totally
## understand the effect of the different graph 

## but I want to allow more comparisons than the minimum spanning tree
## might allow, without so many as the triangulating methods that might 
## make a lot of very distant comparisons. This I think equates to more
## eigenvectors, = more components of the model that have to be selected
## against, = model needs more p-value correction

## If you use a harsh spatial weighting, I'm not sure if it makes much 
## difference. I need to sit down with one of these stats folks, I 
## have a lot of questions here. 

## speaking of, for spatial weighting, let's use a simple linear weighting
## for now. 

## there is another parameter here, not well explained in the tutorial,
## = "Standardization Style". From the help file, this seems to be 
## another weighting of the links, but this time based on the 
## neighborhood. As in, if a point has a large neighborhood, it is 
## standardized to compare with the other neighborhoods by the number
## of links. Not sure why this is necessary. One function below (multispa)
## wants this to be "W", so let's go with it. 

## we can rerun all of this with different methods, see how much of a difference
## it makes. 

## this is the code that the interactive program generated based on 
## a "gabriel" neighborhood graph type, a "binary"/"B" standardization,
## and a linear weighting. 

## choose neighborhood type. "2" = gabriel:
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
## get the distances of each link:
distnb <- nbdists(nb, pJ)
## weight them, by their length. Here a linear weighting:
fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ)))
## combine nb and fdist: 
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

## also need to install the "spatialreg" package...

## the above generates the following neighborhood graph:

s.label(pJ, nb=nb, pnb.edge.col='red')

print(listw2mat(lw)[1:10, 1:10], digits = 3)

print(listw2mat(lw), digits = 2)

mem.lw <- mem(lw)

names(attributes(mem.lw))

barplot(attr(mem.lw, "values"))

s.value(pJ, mem.lw, symbol="circle")

s.value(pJ, mem.lw[,c(seq(1,60,5))], symbol="circle")

s.value(pJ, mem.lw[,c(1, 5, 10, 20, 30, 40, 50, 60)], symbol="circle")

## okay, that works, now?

## the goal is to do an RDA of the community matrix by the spatial matrix. 
## the spatial matrix at this point will be a list of eigenvectors, ~60 
## of them. We have to cut this down to size, so time for model selection.

## the model selection process needs a "y" matrix. Since we are going to 
## do an RDA, which is basically two PCAs on either side of the equation, 
## we can go ahead and hellinger transform our community matrix and run a 
## PCA on this:


comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]

all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)

## the ade4 stuff looks powerful, but culty. 

# let's do the standardization with the traditional function
## and see if we can get to the same spot:

library(vegan)

comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)

## okay, select using these PCAs, see if the output makes sense:

mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw)

## okay, as we learned below, change pvalue. The alpha is just a hair past
## the .05 cutoff (.0528!), this is blocking the model selection from
## going forward:

mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw, alpha=0.1)

summary(mem.comM.sel)

mem.comM.sel

## why does jupyter notebook not want to work like our repl does?

wtf <- function(jit=10, alpha=0.05, meth='FWD'){
pJ <- jitter(ptsMat, amount=jit)
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, pJ)
#fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ))) ## linear
fdist <- lapply(distnb, function(x) 1 / x^3) ## concave
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)
mem.lw <- mem(lw)
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)
mem.comM.sel <- mem.select(pca.hell$tab, 
        listw = lw, 
        nperm.global = 10000, 
        alpha=alpha,
        method=meth)
s.label(pJ, nb=nb, pnb.edge.col='red')
#save(pJ, file='jitteredPoints.rda')
#save(lw, file='lw.rda')
#save(mem.comM.sel, file='mem.comM.sel.rda')
return(mem.comM.sel)
}


bb <- wtf(alpha=0.10, jit = 2)
bb$global.test$pvalue
bb$summary$variables

## pvals
0.01729827
0.02159784
0.1460854

## mems
"MEM16" "MEM19" "MEM3"
"MEM16" "MEM6"  "MEM2"  "MEM12" "MEM20"
"MEM16" "MEM12" "MEM7"  "MEM19"
"MEM16" "MEM17" "MEM2"  "MEM20" "MEM12" "MEM9"


## 
cc <- wtf(alpha=0.10, meth='global')
## not useful, just re-reports all mems

## well...

## so try 3 things:
## 1. tone down the jitter distances 
## 2. don't weight the short distances so high, try different function 
## 3. run a lot of trials to get the mems of interest. 

## 1 -- didn't work. Just as wildly different each time.
## 2 -- results in wildly different results, emphasis on mem16, thought changing 
##      to concave weight with high exponent would make it more robust but still 
##      changing radically. 
## 3 -- not doing much for me, after ~20 trials, results are wildly different. Want
##      more elegant solutions, don't want to grind my computer on for 1000's of 
##      trials. 

## the real answer here is to hand curate our neighborhoods, either
## in the pt map itself, or in the neighborhood matrix
## we have to eliminate the false positives and negatives
## that are resulting from the jitter. 

## I think we have to look at each sample site in the coordinate matrix, 
## and actually separate them by some non-random distance, so we can 
## get consistent results that have some basis in reality.

## how to do this...how many duplicates do we have?

unique(ptsMat) ## 46 unique locations

sum(duplicated(ptsMat)) ## 15 duplicates

pts[,2:3]

## any particular pattern to dupped subParcels?

dups <- duplicated(ptsMat)

dupDF <- pts[dups,]

tE <- table(dupDF$E)

tN <- table(dupDF$N)

pts[pts$E == 746847,]

unique(pts[dups,])

## for instance, this set of GPS points is very 
## common:

## 746847, 10034621

pts[pts$E == 746847,]

## 3.1, 3.4, 6.2, 9.2,

## any pattern here on what is duplicated/not?

## not seeing any. Looks like good old fashioned data entry error. 


## fuck.

## Fuck. Fuck. 

## but it is good we looked at this. 

## to make things even more complicated, the GPS points in the specimen-based 
## spreadsheet ("CedrosFinalReview.7022012.xlsx") are slightly different than the GPS info 
## spread sheet ("CedrosGPSPoinst.xls")

## we need to hand verify the coordinates and elevation of each PsubP value. 

## how to do this intelligently? start with lowest numbered
## PsubP, 
## check its coordinates for duplicates
## try to figure out its true coordinates based on specimens sheets

## a function to check dups:

pts <- read.csv('subParcelSpatial.csv')

psubp <- 1.1

checkDups <- function(psubp){
    psubpRow <- pts[pts$PsubP==psubp,]
    perfMatch <- pts[pts$E==psubpRow$E & pts$N == psubpRow$N,]
    halfMatchE <- pts[pts$E == psubpRow$E & pts$N != psubpRow$N,]
    halfMatchN <- pts[pts$E != psubpRow$E & pts$N == psubpRow$N,]
    #print(paste("PsubP = ", psubp, sep=''))
    #print("perfect matches:")
    #print(perfMatch)
    #print("half matches:")
    #print("E")
    #print(halfMatchE)
    #print("N")
    #print(halfMatchN)
    return(rbind(perfMatch, halfMatchE, halfMatchN))
}

aa <- checkDups(1.1)

aa <- checkDups(6.3)

## make a function tells us how many points
## have the same coordinates as the input point

pts <- read.csv('subParcelSpatial.csv')


sink('lookingForDups.txt')
dupsList <- vector('list', length(pts$PsubP))
for (i in 1:length(pts$PsubP)){
    print(paste(i, pts$PsubP[i]))
    aa <- checkDups(pts$PsubP[i])
    print(nrow(aa))
    if (nrow(aa) > 1){
    dupsList[[i]] <- aa
}}
sink()


## meh, sort of worked... now, 
## let's look at these:

1  1.1  2
2  1.2  1
3  1.3  1
4  1.4  2
5  1.5  2
6  2.1  1
7  2.2  1
8  2.3  2
9  2.4  2
10 3.1  4
11 3.2  1
12 3.3  3
13 3.4  4
14 4.1  2
15 4.2  1
16 4.3  2
17 4.4  3
18 5.1  1
19 5.2  2
20 5.3  1
21 5.4  1
22 6.1  1
23 6.2  4
24 6.3  2
25 6.4  1
26 6.5  1
27 6.6  1
28 6.7  1
29 6.8  1
30 6.9  1
31 7.1  2
32 7.2  1
33 7.3  1
34 7.4  1
35 7.5  1
36 7.6  1
37 7.7  1
38 7.8  2
39 7.9  2
40 8.1  3
41 8.2  3
42 8.3  1
43 8.4  3
44 8.5  3
45 8.6  2
46 8.7  1
47 8.8  1
48 8.9  1
49 9.1  1
50 9.2  4
51 9.3  2
52 9.4  2
53 9.5  2
54 9.6  1
55 9.7  1
56 9.8  2
57 9.9  2
58 10.1 1
59 10.2 2
60 10.3 2
61 10.4 1

## 1.1:
dupsList[[1]]
## 1.1 is at the same spot as 6.3
## not sure which is right, but probably 
## 1.1 is correct, because 6.3 is listed as
## sendero oso, and for a higher elevation

## interesting, the second sheet of Ana's GPSPointsCedros.xls
## lists the 1.1 as N748045 E33792. 

## This is different. If we apply the above corrections (flip x and y, 
## add 10^7 m), this gives us E748045 N10033792, which is spot 
## on the entrance trail, which matches their description spot on. 

## hmm. Think I have been given some rotten data...

## which, if either, matches the collections? 


## 1.4
dupsList[[4]]
##   PsubP      E        N
## 4    1.4 747598 10034057
## 59  10.2 747598 10034057

## elevation for 1.4 listed as 1351 on the aster
## elevation for 10.2 listed as 1367 on the aster

## base de datos final puts 1.4 at 747986, 10033888
## base de datos final puts 10.2 at 747413 10033728  

## GPSpointsCedrosHoja2 says 1.4 is at 747986 10033888
## (on the camino chontal)
## GPSpointsCedrosHoja2 has no GPS data for 10.2
## says it is behind the greenhouse. 

## using hoja2/base de datos, this puts this point 
## almost exactly on the camino chontal trail

## a trend is forming. Seems like we used exactly 
## the wrong data. I just used the cleanest data I
## could find, I didn't see there was a difference...
## not my fault! :)

## let's look at a few more before we make the leap. 




5  1.5  2
8  2.3  2
9  2.4  2
10 3.1  4
12 3.3  3
13 3.4  4
14 4.1  2


## 4.3
dupsList[[16]]
##    PsubP      E        N
## 16   4.3 746470 10032679
## 45   8.6 746470 10033324

## base de datos 746459, 10032194  elev= 1618
## which makes a lot more sense. 

## does the base de datos generally disagree 
## with hoja1 of the gps data?
## and generally agree with hoja2?


17 4.4  3
19 5.2  2
23 6.2  4

## 24 6.3  2
dupsList[[24]]
## as above, I think 1.1 is more likely to be 
## correct here, if either are correct. 
## so where does 6.3 actually belong?
## they are all listed as being on the oso/inca 
## trail system, and as old forest sites

## on the first page of the GPS data
## its listed as N0748246 E0034127
## corrected, this means: 
## E0748246 N10034127

## if we check the second page of the GPS data,
## it's listed as N747140 E35018. 
## correct this to E747140 N10035018. 
## and it's spot on, lands close to Oso trail, 
## near the intersection of the inca, oso and new waterfall trails. 

## I wonder if this is a trend, that the second page
## is the most useful.




 

## also a different elevation. WTF. 

31 7.1  2
38 7.8  2
39 7.9  2
40 8.1  3
41 8.2  3
43 8.4  3
44 8.5  3
45 8.6  2
50 9.2  4
51 9.3  2
52 9.4  2
53 9.5  2
56 9.8  2
57 9.9  2

## 10.2
dupsList[[59]]
##    PsubP      E        N
## 4    1.4 747598 10034057
## 59  10.2 747598 10034057

## the only other data I have for this is 
## base dat final, 747413, 10033728
## which sort of makes sense. 

## matches the elevation ok, their data says 1340, 
## mine says 1326. 

## but the name makes no sense, at least not if 
## I am thinking about the same green house,
## which is basically at the main lc lodge. 

60 10.3 2


## so let's make a dataframe comparing the three sources
## of spatial data - the final report, and pages 1 and 2
## of the los Cedros GPS data:

## start with the final review:

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

wb = openpyxl.load_workbook("CedrosFinalReview.7022012.xlsx")
sheet = wb['Base dat final 6 29 2012']
bdfDF = pd.DataFrame(sheet.values)
bdfDF.columns = bdfDF.iloc[0]
bdfDF.drop(0,0, inplace=True)
fpRep = bdfDF[['Parcela', 'Subparcela','N','E' ]]
fpRep.dropna(axis=0, inplace=True, how='all')
PsubP = fpRep.Parcela.astype('str') + "." + fpRep.Subparcela.astype('str')
PsubP = PsubP.astype('float')
fpRep['PsubP'] = PsubP
fpRep.drop(['Subparcela'], axis=1, inplace=True)
#fpRep.drop(['Parcela', 'Subparcela'], axis=1, inplace=True)
fpRep.set_index('PsubP', inplace=True)
fpRep.drop_duplicates(inplace=True)
fpRep.rename_axis('', axis=1, inplace=True)
## flip n/e, give different name so as to not confuse anything
fpRep.rename(index=str, columns={"N":'FinalRep_E', "E":'FinalRep_N'}, inplace=True)
fpRep.FinalRep_N = fpRep.FinalRep_N + 10000000 ## fix northing numbers
fpRep.to_csv('finalReport2012Coords.csv')

## okay, so how does this compare to the gps data 
## we were using?

aa = pd.read_pickle('subParcelSpatial.p')
aa.rename(index=str, columns={'E':'GPSHoja1_E','N':'GPSHoja1_N'}, inplace=True)

bb = pd.concat([fpRep, aa], axis=1, sort=True)

bb.to_csv('coordinateComparison.csv')

## send this off to Ana for comment...

GPSpointsCedrosMapHoja1

## the finalreview pts look way better to me, so let's redo
## all the explicitly spatial tests with these.

#####################################################3




all(pca.hell2$tab == pca.hell$tab) ## yup, the pcas are exactly the same

## copy our text from the notebook
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", listw = lw, alpha=1)

## works here. try with the jupyter lw:
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", listw = lw2, alpha=1)

## hmmm, seems to have something to do with the lw object we're using. 
mem.comM.sel <- mem.select(pca.hell2$tab, method="FWD", adjR2cum = 0.05, listw = lw, alpha=1)

nb2 <- chooseCN(coordinates(pJ2), type = 2, plot.nb = FALSE)

## this gives us the following:

## $summary
##   variables order         R2      R2Cum   AdjR2Cum pvalue
## 1     MEM28    28 0.02830765 0.02830765 0.01183829  0.009
## 2      MEM2     2 0.02816669 0.05647434 0.02393897  0.005
## 3      MEM1     1 0.02559362 0.08206796 0.03375575  0.008

## plot these:

#png(file='wholeCommMEMS.png', width=1500, height=600)
s.value(pJ, mem.lw[,c(1, 2, 28)], symbol="circle")
#dev.off()


## we can also look at the PCA axes individually with the multispati function
ms.hell <- multispati(pca.hell$li, listw = lw, scannf = F)
g.ms.maps <- s.value(pJ, ms.hell$li, symbol = "circle", ppoint.cex = 0.6)
## well, it picked up the forest contrast. That's encouraging. But not 
## really sure what to do with that. Keep going with the model
## selection method. 

## but generally not working here, when we try to forward select, everything is thrown out. 

#################

## try subsetting to old forest, and repeat:

load(file="oldForestComm.rda")
load(file="oldForestEnv.rda")
load(file="oldForestSp.rda")

mx <- min(oldForestSp[,2])
my <- min(oldForestSp[,3])
oldForestPts <- cbind(oldForestSp[,2] - mx, oldForestSp[,3] - my)

oldForestPts <- jitter(oldForestPts, amount=10)

s.label(oldForestPts, labels=oldForestSp$PsubP)

listw.explore()

library(adespatial);library(sp);library(spdep)

nb <- chooseCN(coordinates(oldForestPts), type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, oldForestPts)
fdist <- lapply(distnb, function(x) 1 - x/max(dist(oldForestPts)))
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

mem.lw <- mem(lw)

s.value(oldForestPts, mem.lw[,c(1, 5, 10, 20, 30)], symbol="circle")
s.value(oldForestPts, mem.lw[,c(1:5)], symbol="circle")

##############################################################3

## side note, I am really confused about the locations of 
## the various sites. At some point in the not-too-distant-future, 
## we need to come to an understanding of this. 

aa <- read.csv('specObs.csv')[,-1]
site5 <- unique(aa[aa$site==5,c('site','PsubP')])
rownames(site5) <- NULL
site6 <- unique(aa[aa$site==6,c('site','PsubP')])
rownames(site6) <- NULL
mem.lw <- mem(lw)

## okay, I think the new points clarify this quite 
## a bit 
##############################################################

library(vegan)
comM.hell <- decostand(oldForestComm, 'hellinger')
pca.hell <- dudi.pca(df = oldForestComm, scale = FALSE, scannf = FALSE, nf = 3)


mem.comM.sel <- mem.select(pca.hell$tab, method="FWD", listw = lw, )

## huh, we have to relax the alpha to see any interesting results. 
## up the perms, because we have functions on the edge of our cutoffs,
## some precision might help

mem.comM.sel.p20 <- mem.select(pca.hell$tab, 
    method="FWD", 
    listw = lw, 
    alpha=0.2,
    nperm=9999, 
    )


mem.comM.sel$summary$order
mem.comM.sel.p20$summary$order

## these are the same. It does not seem to be relaxing
## either the global or local alphas. Should probably 
## post about this. 

## oh well. Let's go with this. 

## so it seems like this does a global test of all mems first, if that test
## does not meet the alpha setting, even highly significant results 
## for single mems are not examined. Sort like a big anova before checking
## differences with a tukey.

## but that is the problem with damn frequentist methods. There is a strong 
## signal in this second MEM, it maps our pattern of the two forest types 
## really well, and this global alpha throws it out, if we are not careful.

#mm.comM.sel.all <- mem.select(pca.hell$tab, method="FWD", listw = lw, MEM.all=TRUE)

## even this MEM.all=TRUE doesn't tell us that one of the MEMs is highly 
## informative. Kind of an odd function. 

summary(mem.comM.sel)

mem.comM.sel$summary

##    variables order         R2      R2Cum   AdjR2Cum pvalue
## 1       MEM1     1 0.04396803 0.04396803 0.02776410  0.001
## 2       MEM2     2 0.03833651 0.08230453 0.05065986  0.001
## 3       MEM8     8 0.03662942 0.11893395 0.07256205  0.001
## 4       MEM3     3 0.03119336 0.15012730 0.08942211  0.001
## 5      MEM10    10 0.03012891 0.18025621 0.10573405  0.001
## 6       MEM5     5 0.02939932 0.20965553 0.12183948  0.003
## 7      MEM12    12 0.02703676 0.23669229 0.13587807  0.005
## 8       MEM6     6 0.02478950 0.26148180 0.14786361  0.003
## 9       MEM9     9 0.02096149 0.28244329 0.15581563  0.008
## 10     MEM14    14 0.02080523 0.30324852 0.16389822  0.010
## 11     MEM15    15 0.01957388 0.32282240 0.17080294  0.023
## 12     MEM11    11 0.01919429 0.34201669 0.17752087  0.017
## 13      MEM4     4 0.01917227 0.36118897 0.18449655  0.027
## 14     MEM13    13 0.01826007 0.37944903 0.19058570  0.036

## well, up to ~.38 of variance predicted by spatial patterns. 
## we are into some interesting results, my friends.

## lots of MEMs. plot these:

s.value(pJ, mem.lw[,mem.comM.sel$summary$order], symbol="circle")

## very cool. 

## let's summarize that MEM creation/selection pipeline:

## load packages
library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)
## get data:
pts <- read.csv('finalReport2012Coords.csv')[,-2]
pts <- pts[order(pts$PsubP),]
env <- read.csv('envOnly.csv')
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
## are these in order?
all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 
pJ <- as.matrix(pts[,2:3])
colnames(ptsMat) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
## sanity plot check:
#s.label(pJ)
## graphical interface for exploring neighborhoods:
##listw.explore()
## make SWG, with gabriel method:
nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
## get the distances of each link:
distnb <- nbdists(nb, pJ)
## weight them, by their length. Here a linear weighting:
fdist <- lapply(distnb, function(x) 1 - x/max(dist(pJ)))
## combine nb and fdist: 
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)
## visual check of neighborhoods
s.label(pJ, nb=nb, pnb.edge.col='red')
## generate mems
mem.lw <- mem(lw)
## standardize/"linearize" rows of community matrix, make PCAs
comM.hell <- decostand(comM, 'hellinger')
pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 4)
## okay, select using these PCAs, see if the output makes sense. Use lots of 
## permutations, because we have a MEM on the edge of significance. 
mem.comM.sel <- mem.select(pca.hell$tab, 
    method="FWD", 
    listw = lw, 
    nperm=9999, 
    )

## check results
mem.comM.sel$summary
## plot:
s.value(pJ, mem.lw[,mem.comM.sel$summary$order], symbol="circle")


##### look at big picture again ########

## our todo list is something like:

## see if we can correct the point outside of 
## los cedros, Ana says this is incorrect

## check old forest NMS, groups seem funky to me

## examine differences in communities of seedlings and mature 
## trees - are they a view into the future?

## examine differences among blocks

## finish spatial analysis -  what were we trying to do again?
## get mems as predictor variables. 

## the classical next steps are to conduct a variation partitioning 
## process - changes in our community composition as predicted by:
## slope, aspect, elevation, distance to stream, land use 

## that means I need to generate more environmental data, and clean up 
## the land use question a bit. 

## land use right now is now well defined, and should be broken up into 
## at least two variables: 

## time since last  use 
## type of use (potrero vs agriculture?)

## right now the latter is already broadly defined, RCA vs. BS

## BS is old potrero, mostly, and RCA is intensive agriculture

## so maybe step one is correct small mistakes, the point outside 
## LC and the old forest groups

## step two is generate the other environmental data, 
## step three is complete the broad varpar
## step four check seedlings
## step five, try out the BC distance to Old forest vs. RCA within the BS
## (the pleuripotent hypothesis, alternate stable states) 

###### try to correct point outside los cedros ######

## how can we quickly plot our points and the los cedros 
## polygon?

## back into R. We need to know how to do this anyway, 
## to make all the MEM stuff look nicer.

## load all the libraries:
library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

## here is the example tutorial from ADE folks:

data('mafragh')

mxy <- as.matrix(mafragh$xy)
rownames(mxy) <- NULL

s.label(mxy, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = mafragh$Spatial.contour)

## works, so how do we adapt to this to our data?

## our points are here:
aa <- read.csv('finalReport2012Coords.csv')[,-2]
aa <- aa[order(aa$PsubP),]
rownames(aa) <- aa$PsubP
colnames(aa) <- c('PsubP','E','N')
pts <- aa[,-1]

## how to plot the losCed polygon?

## we have a geojson, there must be an easy way to do this

## this is a horrible way to do this, but:
source('lcPolyGetCoords.R') 

Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")

lcPoly <- SpatialPolygons(list(Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")))
#save(lcPoly, file='lcPoly.rda')

s.label(pts, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = lcPoly)

## looks like 2.4 is the problem. 

pts['2.4',]

## to me it looks like the northing is off by about 1000m, 
## in order to bring it down to the 2.__ subparcels

## how does this look if we correct just this point, by subtracting
## 1000m from the Northing?

pts['2.4',]$N <- 10033474

s.label(pts, ppoint.pch = 15, ppoint.col = "darkseagreen4", Sp = lcPoly)

## looks better. But looks like all of the 2.__ plots are outside the 
## reserve, still:

s.label(pts, label=NULL, ppoint.col = "darkseagreen4", Sp = lcPoly)

## not sure, but they are close. 

## let's leave them as they are, with that one point corrected. 
## we should probably rerun the spatial stuff with this point, it may 
## have made a difference. 

## that works. 
## can we do this with the MEMs? don't see why not. save that for later.


### debug old forest elevacion NMS ###

## for some reason, our old forest NMS that checks 
## the effects of elevation on plot, the results are reversed
## from my maps. 

load("oldForestComm.rda")
load("oldForestEnv.rda")

## groups look right in the environmentat data. 
## must simply be an ordering issue in the
## hulls

OFnms <- metaMDS(oldForestComm, try=40)
 
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']

## make a color ramp for this elevation data
minElev <- min(oldForestEnv$elevacion)
maxElev <- max(oldForestEnv$elevacion)
rangeElev <- maxElev - minElev
colrsRamp <- colorRampPalette(c(rgb(1,0,0,0), rgb(1,0,0,1)), alpha = TRUE)(rangeElev)
ElevColrs <- colrsRamp[oldForestEnv$elevacion - min(oldForestEnv$elevacion)]


png(file='oldForestNMSelevClust.png', height=800, width=800)
plot(XX, YY,
    col="black",
    pch=21,
    cex=4.5,
    bg=ElevColrs,
    xlab='NMS1',
    ylab='NMS2',
    main="Two forest types detected, with elevation gradient"
    )
ordihull(OFnms, oldForestEnv$clGr, col=c(3,4), lwd=2, lty=2)
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '1259 m', '1680 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, "transparent", "red"),
        #merge = FALSE,
        pt.cex = 3.0,
        cex = 2.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )
dev.off()

names(table(oldForestEnv$clGr))

aa <- identify(OFnms$points)

OFnms$points[aa,]

OFnms$points[identify(OFnms$points),]

## where is our lowest point on this?

oldForestEnv[order(oldForestEnv$elevacion),]

## looks like our lowest is 7.1

OFnms$points['7.1','MDS1']

points(OFnms$points['9.1','MDS1'], 
        OFnms$points['9.1','MDS2'],
        cex=5, 
        pch=21,
        col="blue")

## yup, all lines up. problem debugged. 

## what's next?

## we need environmental data.

### looks like we could have sed this:

install.packages('geojsonio')

## then try
geojson_sp()

######### generate more environmental data ########

## we need slope, aspect, distance to water,
## how can we generate these? 
 
## if we do this in python:

##### to plot polygon boudaries:

lcPoly = gpd.read_file('GIS/lcPoly.geojson')

help(lcPoly.boundary.plot)

lcPoly.boundary.plot(color='purple', linewidth=3)

## I think the other functions will be shapely type 
## operations

##### to get distance to stream
## one example is here:
## https://stackoverflow.com/questions/30740046/calculate-distance-to-nearest-feature-with-geopandas ##
## remember to upvote if it works

import geopandas as gpd

# read geodata for five nyc boroughs
gdf_nyc = gpd.read_file(gpd.datasets.get_path('nybb'))
# read geodata for international cities
gdf_cities = gpd.read_file(gpd.datasets.get_path('naturalearth_cities'))

# convert to a meter projection
gdf_nyc.to_crs(epsg=3857, inplace=True)
gdf_cities.to_crs(epsg=3857, inplace=True)
gdf_nyc.geometry.apply(lambda x: gdf_cities.distance(x).min())

##### to get slope and aspect ######

## try this in python. 

import pandas as pd 
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
import rasterio
import rasterio.plot
import math
plt.ion()


## the boundaries are here:
lcpoly.bounds

## but we already have a good size DEM for this study, let's 
## try that first. Following <https://stackoverflow.com/questions/47653271/calculating-aspect-slope-in-python3-x-matlab-gradientm-function>

## upvote if works

## slope:

from osgeo import gdal
import numpy as np
import rasterio
from rasterio.plot import show_hist

help(gdal.DEMProcessing)

gdal.DEMProcessing('GIS/anaStudySlope.tif', 'GIS/anaPlotDEG.tif', 'slope')
slopeRast = rasterio.open("GIS/anaStudySlope.tif")
rasterio.plot.show(slopeRast)
## does not work as above...
rasterio.plot.show_hist(slopeRast) ## nada


help(gdal.DEMProcessing)
gdal.DEMProcessing('GIS/anaStudyAspect.tif', 'GIS/anaPlotDEG.tif', "aspect")
aspectRast = rasterio.open("GIS/anaStudyAspect.tif")
rasterio.plot.show(aspectRast)

rasterio.plot.show_hist(slopeRast) ## nada

## fuckit, none of that worked. Did a little pointNclick with 
## qgis, let's see if that worked:

slopeRast = rasterio.open("GIS/lcSlope.tif")
rasterio.plot.show(slopeRast)

aa = slopeRast.read(1)
## this makes a numpy array which could be handy, but we lose
## our spatial indexing?

## "index" might work here:
xx, yy = slopeRast.index(748667, 10033329)
aa[xx,yy]

## not sure if that works perfectly. Let's do a sanity 
## check with the DEM itself:

dem = rasterio.open("GIS/anaPlotDEM.tif")
demR = dem.read(1)

## this should be roughly the highest spot:
xx, yy = dem.index(745605,10038208)
demR[xx, yy]
## no nope...hmmm

## try working with the bigger aster image
lcDEM = rasterio.open("/home/daniel/Documents/LosCed/Ecuador_GIS/losced/losced_topography/LosCedAster.tif")
lcDEMR = lcDEM.read(1)

dir(lcDEM)

lcDEMR.max()

xx, yy = lcDEM.index(745605,10038208)
lcDEMR[xx, yy]
## nope...weird...

## try working backwards

## highest point is:
lcDEMR.max()

np.where(lcDEMR == lcDEMR.max())
lcDEMR[62,284]
## there it is. Where is this on our map?
np.where(lcDEMR == lcDEMR.max())
lcDEM.xy(62,284)
## well, that seems reasonable. 

## try again with the Ana DEM. Where is the
## highest point on the area of the Ana study?

dem = rasterio.open("GIS/anaPlotDEM.tif")
demR = dem.read(1)
np.where(demR == demR.max())

## these two functions are inverse:
dem.xy(7, 0)
## (745035.4095220644, 10035563.857169842)
dem.index(745035,10035564)
## (7, 0)

np.array(dem.index(745035,10035564))

demR(


demR.max()

## yup, lines up. In general, we have UTMs
## for our plots, how do we use these to 
## look up our elevations?

## a function would be good here. given an x and y 

def getZ(xx,yy,robj):
    robjR = robj.read(1)
    rc = robj.index(xx,yy)
    return(robjR[rc])

## try this out

np.where(demR == demR.min())
dem.xy(106,134)

getZ(749165.1614353219, 10032512.771811783, dem)

getZ(746975, 10035200, dem)
## in general, seems to work.

## while we're looking at elevations, how do our DEM elevations line up
## with their GPS elevations?

## our pt data:
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)

## add the elevation as measured from the GPS's, in ana's excel sheets:
envOnly = pd.read_csv('envOnly.csv')
envOnly.set_index('PsubP', inplace=True)
envOnly.sort_index(inplace=True)

(pts.index == envOnly.index).all()

## so can we use the above function to find elevations?

type(pts.geometry)

dir(pts.geometry)


## aspect
aspectRast = rasterio.open("GIS/lcAspect.tif")
rasterio.plot.show(aspectRast)
rasterio.plot.show(aspectRast)

## looks good. Now, how can we get the information for each subparcel?

envOnly['DEM'] = [ getZ(i.x, i.y, dem) for i in pts.geometry ]

## wow, some of these look pretty far off. Plot these:

aa = plt.scatter(envOnly['elevacion'], envOnly['DEM'])
plt.gca().set_xlabel('GPS Elevation')
plt.gca().set_ylabel('DEM Elevation')

## well, at least there is a strong correlation. 

## do this for the slope and aspect data:

envOnly['aspect'] = [ getZ(i.x, i.y, aspectRast) for i in pts.geometry ]

## and maybe we decompose this into two variables - eastern exposure (cos) and northern exposure (sin)?

## to get our geometry right, we need to change compass heading into classical 
## eculidean angles, where the angle is measured counterclockwise, starting from
## "horizontal", which in the case of a compass is straight north. 
## so shift all the angles 90% counterclockwise (subtract 90 degrees), then 
## take additive inverse (make it negative, because angles are measured 
## counterclockwise, unlike the compass heading)

angles = [ -(H - 90) for H in envOnly['aspect'] ]

## then get the sin (northern exposure) and cos (southern exposure):

envOnly['exposeE'] = [ math.cos(math.radians(i)) for i in angles ]
envOnly['exposeN'] = [ math.sin(math.radians(i)) for i in angles ]

## seems to work. 

######### slope #########

slopeRast = rasterio.open("GIS/lcSlope.tif")

rasterio.plot.show(slopeRast)

envOnly['slope'] = [ getZ(i.x, i.y, slopeRast) for i in pts.geometry ]

getZ(xx,yy,robj)

## get some plotting help here at:
http://matplotlib.org/api/axes_api.html?highlight=imshow#matplotlib.axes.Axes.imshow
http://matplotlib.org/api/axes_api.html?highlight=imshow#matplotlib.axes.Axes.contour

rasterio.plot.show(slopeRast)

## write this out as our new environmental matrix

######## todo 

## rerun existing spatial analyses with corrected point
## check permanovas of new environmental data
## do varpart analysis

#########################

## can we redo the turnover diagrams with the 
## revised point (2.4 moved 1km sout)

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
from sklearn.linear_model import LinearRegression
import scipy.spatial as sp
import geopandas as gpd
import rasterio
import rasterio.plot
import copy
import random
import math

specObs = pd.read_pickle('specObs.p')
subParcelComm = pd.read_pickle("subParcelComm.p")
subParcelComm.index = subParcelComm.index.astype('float')
subParcelComm.sort_index(inplace=True)
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)
(pts.index == subParcelComm.index).all()
## get our original in there first:
fig, axes = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(12,8))
axes = axes.flatten()
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
axes[0].scatter(physDist, bcDist)
axes[0].plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
axes[0].set_title('Overall Turnover')
## now look at these individually:
soGrouped = specObs.groupby('habitat')
habs = list(soGrouped.groups.keys())
for h,i in enumerate(habs):
    print(i)
    print(h)
    obs_i = soGrouped.get_group(i)
    obs_iE = [ pts.loc[float(a)].geometry.x for a in obs_i.PsubP ]
    obs_iN = [ pts.loc[float(a)].geometry.y for a in obs_i.PsubP ]
    obs_i['E'] = obs_iE
    obs_i['N'] = obs_iN
    subParcelInObs_i = subParcelComm.index.isin(obs_i.PsubP)
    comm_i = subParcelComm[subParcelInObs_i]
    notObserved=~((comm_i == 0).all(axis=0).values)
    comm_i = comm_i.iloc[:,notObserved]
    pts_i = pts.loc[comm_i.index.values]
    iX, iY = pts_i.geometry.x.to_list(), pts_i.geometry.y.to_list()
    spDF_i = pd.DataFrame(np.array([iX, iY]).transpose(), columns=['X','Y'])
    physDist_i = sp.distance.pdist(spDF_i, metric='euclidean')
    bcDist_i = sp.distance.pdist(comm_i, metric='brayCurtis')
    axes[h+1].scatter(physDist_i, bcDist_i)
    X, Y = physDist_i.reshape(-1,1), bcDist_i.reshape(-1,1)
    axes[h+1].plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
    axes[h+1].set_xlabel('distance between subplots')
    axes[h+1].set_ylabel('BC dissimilarity')
    axes[h+1].set_title(f'{i} habitat turnover')

plt.tight_layout()

## okay, that did make a difference. 
## pain in the ass, a couple hours of my short life,
## but same could be said for all of this. 

## anyway, we should export this new pt data as a csv
## for R:

pd.DataFrame(np.array([pts.index, pts.geometry.x, pts.geometry.y]).transpose(), columns=['PsubP','X','Y']).head()

aa = pd.DataFrame(np.array([pts.index, pts.geometry.x, pts.geometry.y]).transpose(), columns=['PsubP','X','Y'])
aa.set_index('PsubP', inplace=True)
aa.to_csv('pts.csv')

## speaking of, time to head over to R to fix other spots with new point location...

##### debug map with clusters in R ##########

library(stats)
library(vegan)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]
## we need a distance matrix, BC
## does vegan give us what we need?
sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')

makeClustMap <- function(tree, k, labels=NULL){
    envOnly <- read.csv('envOnly.csv')
    pts <- read.csv('pts.csv', row.names='PsubP')
    pts$PsubP <- as.numeric(rownames(pts))
    pts <- pts[,c(3,1,2)]
    rownames(pts) <- NULL
    require(stats)
    ## get habitat type labels for tree
    if (is.null(labels)){lab <- NULL} else {
        if (labels=='hab'){
        lab <- vector(length = length(tree$labels))
        for (i in 1:length(tree$labels)){
            ind <- which(envOnly$PsubP==tree$labels[i])
            lab[i] <- as.character(envOnly$habitat[ind])
    }}}
    par(mfrow=c(1,2))
    plot(tree, labels=lab)
    clustGroups <- rect.hclust(tree, k=k, border=1:k)
    cmem <- vector(length=k)
    PsubP <- vector()
    gr <- vector()
    for (i in 1:k){
        cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
        PsubP <- c(PsubP, cmem.i)
        gr <- c(gr, rep(i, length(cmem.i)))
        }
    cGroup <- data.frame(cbind(PsubP, gr))
    cGroup <- cGroup[order(cGroup$PsubP),]
    clsp <- base::merge(pts, cGroup)
    clsp <- base::merge(clsp, envOnly)
    shapes <- vector(length=nrow(clsp))
    shapes[clsp$habitat == 'BC'] <- 19
    shapes[clsp$habitat == 'BS'] <- 21
    shapes[clsp$habitat == 'CLB'] <- 22
    shapes[clsp$habitat == 'RCA'] <- 17
    shapes[clsp$habitat == 'RG'] <- 23
    plot(x=pts$X,
        y=pts$Y,
        col=clsp$gr,
        xlab='',
        ylab='',
        asp=1,
        cex=2,
        lwd=2,
        pch=shapes,
        )
    legend('bottomright',
        legend=c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        pch=c(19, 21, 22, 17, 23)
    )
    legend('bottomleft',
        legend=1:k,
        pch=c(15),
        col=1:k,
        title='ClusterGroup')
    return(clsp)
    }

k <- 4 ## number of groups 


png(file='hclust4groups.png', width=1600, height=800)
makeClustMap(sPC.ward, k=4, labels='hab')
dev.off()

### fix mantel tests

pts <- read.csv('pts.csv', row.names='PsubP')

pts <- read.csv('pts.csv')

pts$PsubP <- as.numeric(rownames(pts))

pts <- pts[,c(3,1,2)]

aa <- read.csv('pts.csv', row.names=1)

library('vegan')
library('ecodist')

pts <- read.csv('pts.csv', row.names='PsubP')
physdist <- vegdist(pts, method='euclidean')
## make the community dissimilarity matrix
bb <- read.csv('subParcelComm.csv')
bb <- bb[order(bb$PsubP),]
row.names(bb) <- bb$PsubP
bb <- bb[,-1]
all(aa$PsubP == rownames(bb))
braydist <- vegdist(bb, method='bray')
## do the global test:
global_mant_test <- mantel(braydist ~ physdist, nperm = 10000) ## overall test
global_mant_test 
## correlogram
cgram <- mgram(braydist, physdist) ## correlogram object
plot(cgram)

getDists <- function(comfile, physfile='pts.csv'){
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv(physfile, row.names=1)
    spM <- spM[order(rownames(spM)),]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),]
    spM <- spM[order(as.numeric((rownames(comM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        }
    else{print("something doesn't fit right")}
    }

## filenames
comms <- c('BC_comm.csv', 'BS_comm.csv', 'CLB_comm.csv', 'RCA_comm.csv', 'RG_comm.csv')
## start df
all_mant_test  <- global_mant_test

## test individually
par(mfrow=c(2,3))
for (i in 1:length(comms)){
    print(comms[i])
    dists.i <- getDists(comms[i])
    cgram.i <- mgram(dists.i[[1]], dists.i[[2]]) ## correlogram object
    plot(cgram.i, main = comms[i] )
    mant_test.i <- mantel(dists.i[[1]] ~ dists.i[[2]], nperm = 10000) ## overall test
    all_mant_test <- rbind(mant_test.i, all_mant_test)
}

getDists(comms[i])

getDists <- function(comfile, physfile='pts.csv'){
    comM <- read.csv(comfile, row.names=1)
    comM <- comM[order(as.numeric((rownames(comM)))),]
    spM <- read.csv(physfile, row.names=1)
    spM <- spM[order(rownames(spM)),]
    physdist <- vegdist(spM, method='euclidean')
    spM <- spM[rownames(spM)%in%rownames(comM),]
    spM <- spM[order(as.numeric((rownames(spM)))),]
    if(all(rownames(spM) == rownames(comM))){
        physdist <- vegdist(spM, method='euclidean')
        braydist <- vegdist(comM, method='bray')
        return(list(braydist, physdist))
        } else {print("something doesn't fit right")}
    }

######## update mems 

library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

## get data:
pts <- read.csv('pts.csv')
env <- read.csv('envOnly.csv')
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
comM <- comM[order(as.numeric(rownames(comM))),]
## are these in order?
all(rownames(comM) == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
all(env == env[order(env$PsubP),])
all(pts == pts[order(pts$PsubP),])
all(pts$PsubP == env$PsubP)
## looks good. 

load('lcPoly.rda')
pJ <- as.matrix(pts[,2:3])
colnames(pJ) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)

## sanity plot check:
s.label(pJ, labels=pts$PsubP, ppoint.col = "darkseagreen4", Sp = lcPoly)

class(coordinates(pJ))

nb <- chooseCN(coordinates(pJ), type = 2, plot.nb = FALSE)
nbdists(nb, coordinates(pJ))

## can we move our lc polygon to match our relocated points on the 
## our coordinates are here:
load('lcPoly.rda')
xx <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolE']
yy <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolN']
## the minimums of our sample sites was here:
pts <- read.csv('pts.csv')
pJ <- as.matrix(pts[,2:3])
colnames(pJ) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
## transform our polygon points with these values?
xx <- xx - mx
yy <- yy - my
## and make a spatial polygon out of this?
lcPolyMoved <- SpatialPolygons(list(Polygons(list(Polygon(cbind(xx,yy))), ID="lcPolyMoved")))

s.label(pJ, labels=pts$PsubP, Sp = lcPolyMoved)

## what about our MEMs?

head(lcPoly@polygons[[1]]@Polygons[[1]]@coords)
class(lcPoly@polygons[[1]]@Polygons[[1]]@coords)

lcPolyOrigin <- SpatialPolygons(list(Polygons(list(Polygon(LCpolyCoords)), ID="losCedros")))

str(lcPoly)

## this works for a background on our mems...

## now what?

## look at new environmental data, individually then with varpart

## so, back to the PERMANOVAs...

####### permanovas, new environmental data ############

##sfl two (three) new variables, slope, eastern and northern exposure

library(vegan)

envOnly <- read.csv('envOnly.csv')
subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)


all(envOnly$PsubP == subParcelComm$PsuP)

sPC <- subParcelComm[,-1]

adonis(sPC ~ envOnly$elevacion)

adonis(sPC ~ envOnly$DEM)

## same, makes sense

adonis(sPC ~ envOnly$slope) ## very little, not significant

adonis(sPC ~ envOnly$exposeE) ## very little, definitely not significant

adonis(sPC ~ envOnly$exposeN) ## very little, definitely not significant

adonis(sPC ~ envOnly$toStream) ## statistically significant, but explains very little variance (r2 =.025)



## well, shit. I guess I had to check

## check the old forest only data:

load("oldForestComm.rda")
load("oldForestEnv.rda")

all(rownames(oldForestComm) == rownames(oldForestEnv))

adonis(oldForestComm ~ oldForestEnv$toStream) ## higher, r2 = .06, p=0.005
adonis(oldForestComm ~ oldForestEnv$slope) ## r2 = .04, p = .03 

adonis(oldForestComm ~ oldForestEnv$toStream * oldForestEnv$slope) 
## no significant interaction

adonis(oldForestComm ~ oldForestEnv$exposeE) ## NS, r2 small
adonis(oldForestComm ~ oldForestEnv$exposeN) ## NS, r2 small

## can we do an ordination of the distance to stream?

load('OFnms.rda')
XX <- OFnms$points[,'MDS1']
YY <- OFnms$points[,'MDS2']
cGroup <- read.csv('cGroup.csv', row.names="PsubP")
oldForestClGr <- cGroup[row.names(oldForestEnv),]

minDtoS <- min(oldForestEnv$toStream)
maxDtoS <- max(oldForestEnv$toStream)
rangeDtoS <- maxDtoS - minDtoS
colrsRamp <- colorRampPalette(c(rgb(0,0,1,0.5), rgb(1,1,0,1)), alpha = TRUE)(rangeDtoS)
DtoSColrs <- colrsRamp[oldForestEnv$toStream - min(oldForestEnv$toStream)]
plot(XX, YY,
    col="black",
    pch=21,
    cex=3,
    bg=DtoSColrs,
    xlab='NMS1',
    ylab='NMS2',
    main="Two forest types detected, distance to stream"
    )
ordihull(OFnms, oldForestClGr, col=c(3,4), lwd=2, lty=2)
legend( x='topright',
        legend = c('clustGroup 3', 'clustGroup 4', '7 m', '461 m'),
        col = c(3,4, 1, 1),
        bty = 'n',
        pt.bg = c(NA, NA, colrsRamp[1], colrsRamp[maxDtoS - minDtoS]),
        #pt.bg = c(NA, NA, "transparent", "blue"),
        cex = 1.0,
        lty = c(2,2, NA, NA),
        lwd = 2.0,
        pch = c(NA, NA, 21, 21),
        )

## well, it does look like the type four forests tend to be 
## closer to streams

###### distance to stream #######

## let's do this in python, I think shapely 
## was made for this...

plt.ion()

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
paths = gpd.read_file('GIS/PathsLC.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
raster = rasterio.open("GIS/anaPlotDEM.tif")
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)

fig, ax = plt.subplots(1, 1)
hydro.plot(color='Blue', ax=ax)
lcPoly.boundary.plot(color='purple', linewidth=3,ax=ax)
paths.plot(color='red', linestyle='--', ax=ax)
rasterio.plot.show(raster, ax=ax)

## okay, so we need our distance from each ana point 
## to the closest spot on the river system:

help(pts.geometry[0].distance)

pts.geometry[0].distance(hydro.geometry[0])

pts.geometry[0].distance(hydro.geometry) ## nope. 

pts.geometry.distance(hydro.geometry[0]) 
## yup. seems like the many to one relationship works.

## so for us, we need:

hydro.geometry.distance(pts.geometry[0])

hydro.geometry.distance(pts.geometry[0]).min()

## to get this for all of our pts...

envOnly['toStream'] = [ hydro.geometry.distance(i).min() for i in pts.geometry ]
#envOnly.to_csv('envOnly.csv')

## sanity checks

envOnly.loc[[9.4,9.5]]

envOnly.loc[9.4]

buff=400
fig, ax = plt.subplots(1, 1)
pts.loc[[9.5,9.4]].plot(color='black', ax=ax)
hydro.plot(color='Blue', ax=ax)
ax.set_xlim([pts.loc[9.5].geometry.x -400,pts.loc[9.5].geometry.x + 400])
ax.set_ylim([pts.loc[9.5].geometry.y -400,pts.loc[9.5].geometry.y + 400])

envOnly.loc[2.4]
## seems okay

## some a permanova on it, add to text in above section

## a little bit important (R2 = 0.025), but not much info there. 




## well, we can tromp on to the variation partioning. 

## also should take a look at the blocks that ana made, see 
## if they describe all this well. R2 = 0.025), but not much info there. 

## well, we can tromp on to the variation partioning. 

## also should take a look at the blocks that ana made, see 
## if they predict any variation. 

###### varpart ##########################

## let's see if we have what we need to get a 
## preliminary variation partition done...


## example:
data(mite)
data(mite.env)
data(mite.pcnm)
# Two explanatory data frames -- Hellinger-transform Y
mod <- varpart(mite, mite.env, mite.pcnm, transfo="hel")

## community matrix
comm <- read.csv('subParcelComm.csv', row.names='PsubP')

## we have our environmental data frame:
envOnly <- read.csv('envOnly.csv', row.names='PsubP')

## we want to retain elevacion, habitat, n and e exposure, distance to stream...

env = envOnly[,c('elevacion', 'habitat', 'slope', 'exposeE', 'exposeN')]

## spatial dataframe should be our MEMs:
load('sigMEM.rda')

## we might also want to include our clustering results...

clust <- read.csv('cGroup.csv', row.names='PsubP')

## try 
vp_noClust <- varpart(comm, env, sigMEM, transfo="hel")
plot(vp_noClust, bg=2:5)

vp_Clust <- varpart(comm, env, sigMEM, clust, transfo="hel")
plot(vp_Clust)

## adding in the clustering results doesn't do much. 
## just curious, what do explain alone, via permanova?

adonis(sPC ~ clust$gr) ## explains about ~8% of variance

## most of this variance is also predicted by spatial
## and environmental variables. 

## well, there's a whole lot of noise in this system. 

## I'm used to that. I'm a microbial ecologist of sorts. 

## I wonder if the old forest story is a little bit cleaner?

## what next? repeat all the above for old forests?

library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

load(file='oldForestSp.rda')
load(file="oldForestComm.rda")
load(file="oldForestEnv.rda")

## old forest env data needs to be updated:

oldForestEnv <- envOnly[as.character(oldForestSp$PsubP),]
#save(oldForestEnv, file="oldForestEnv.rda")

## we need MEMs for the forest, also...

rownames(oldForestSp) <- oldForestSp$PsubP
oldForestSp <- oldForestSp[,-1]

oldForestSp

## oh, this analysis is still based on the original, erroneous 
## GPS points

dim(oldForestSp)
dim(unique(oldForestSp))
## yeah, repeats, bad data

## build new spatial data:
pts <- read.csv('pts.csv', row.names='PsubP')

oldForestSp <- pts[row.names(oldForestEnv),]
#save(oldForestSp, file='oldForestSp.rda')

load('ptsmxmy.rda')

oldForestSpRecentered <- cbind(oldForestSp[,1] - ptsmxmy[1], oldForestSp [,2] - ptsmxmy[2])
row.names(oldForestSpRecentered) <- row.names(oldForestSp)
oldForestCoords <- coordinates(oldForestSpRecentered)

## interactive
listw.explore()


## says do this:
nb <- chooseCN(oldForestCoords, type = 2, plot.nb = FALSE)
distnb <- nbdists(nb, oldForestCoords)
fdist <- lapply(distnb, function(x) 1 - x/max(dist(oldForestCoords)))
lw <- nb2listw(nb, style = 'W', glist = fdist, zero.policy = TRUE)

## looks like:
s.label(oldForestCoords, nb=nb, pnb.edge.col='red')

## get old forest mems:
mem.oldForest <- mem(lw)

load('lcPolyMoved.rda')

s.value(oldForestCoords, mem.oldForest, symbol="circle", sp=lcPolyMoved)
## okay, worked

## now find the important ones:

oldForestComm[1:3,1:3]

dim(oldForestComm)

comM.hell <- decostand(oldForestComm, 'hellinger')

## just to be systematic..take all the axes greater than the mean of eigenvalues

ncol(oldForestComm)

pca.hell.all <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = ncol(oldForestComm))

sum(pca.hell.all$eig)

length(pca.hell.all$eig) ## told it to keep them all, why only 35? oh well...

meig <- mean(pca.hell.all$eig)

lines(c(0,50),c(meig,meig), col='red')

pca.hell.all$eig[pca.hell.all$eig > mean(pca.hell.all$eig)]

sum(pca.hell.all$eig > mean(pca.hell.all$eig))
## so maybe the first 12 are useful?

pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = TRUE)

pca.hell <- dudi.pca(df = comM.hell, scale = FALSE, scannf = FALSE, nf = 12)

all(pca.hell$tab == pca.hell.all$tab)
## no difference...
length(pca.hell$eig)
## ugh, again the ade4 functions aren't responding to my changes in arguments.
## don't know why that is, I really should write them

## we can extract these ourselves, I guess. But a little out of my league 
## here, because the downstream model selection is based on a new weighted
## dataframe of the sites and species that I don't know how to make.

## so just keep going, with all thirty five axes? seems like this may
## take forever?

## let's try it

mem.comM.sel <- mem.select(pca.hell$tab,
    method="FWD",
    listw = lw,
    nperm=999,
    )

## actually pretty quick.

## look at them:

s.value(oldForestCoords, mem.comM.sel$MEM.select, symbol="circle", Sp=lcPolyMoved)

mem.comM.sel$MEM.select

## huh, none of them are obvious to me. check some environmental vars:

aa <- rda(mem.comM.sel$MEM.select[,1], oldForestEnv$elevacion)

cor(mem.comM.sel$MEM.select[,1], oldForestEnv$elevacion)

head(oldForestEnv)


vp_noClust <- varpart(comm, env, sigMEM, transfo="hel")
plot(vp_noClust, bg=2:5)

vp_Clust <- varpart(comm, env, sigMEM, clust, transfo="hel")
plot(vp_Clust)

## then try to explain what is happening with secondary 
## forests 

## and look at seedling data - how good is this data? 
## can we use it to predict the future?


## anyway, to do:

## clean old forest analysis, it still has the old spatial data
## check new environmental variables on old forest, 
## run varpart for old forest
## maybe check clustering on old forest, see if it splits 
## cleanly in two when looked at in isolation

## check seedling data

############ add cluster results to entire community nms  ################

## before we go to far, let's see how our cluster results bear out
## on the all comm nms, we never did this. 

library(vegan)
library(RColorBrewer)

subParcelComm <- read.csv('subParcelComm.csv', header=TRUE)
envOnly = read.csv('envOnly.csv')
sPC <- subParcelComm[,-1]
rownames(sPC) <- subParcelComm[,1]

nms <- metaMDS(sPC, try=40)

options(repr.plot.width=5, repr.plot.height=5)
stressplot(nms)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
cGroup = read.csv('cGroup.csv')
rownames(nondups) <- NULL
all(nondups$PsubP == cGroup$PsubP)
nmsInfo <- merge(nondups, cGroup)
nmsInfo$MDS1 <- nms$points[,'MDS1']
nmsInfo$MDS2 <- nms$points[,'MDS2']

nmsInfo <- nondups

## plot it 

plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$gr,
    pch=19,
    cex=2.0,
)
text(nmsInfo$MDS1, nmsInfo$MDS2 - 0.015, nmsInfo$PsubP, cex=1.5)
## add legend
legend( x='bottomleft',
        legend = 1:4,
        fill = 1:4,
        cex = 1.0,
        bty='n',
        title='Cluster Group'
        )

## figure out lines only:
plot(1:5,rep(1,5), cex=3, pch=19, col=1:5)
legend( x='bottomright',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        col = c(1:5),
        cex = 1.0,
        lwd = 1,
        )



## add natural cluster hulls:
#ordihull(nms, nmsInfo$gr, col=1:4, lwd=3)

## or with habitat hulls:
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)


cGroup <- read.csv('cGroup.csv')

cGroup <- read.csv('cGroup.csv', row.names="PsubP")


############ check seedling data ################

## can we get our seedling info into a useable format?

## do this in python, methinks. 

python3

import os
import openpyxl
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle

wb = openpyxl.load_workbook("BaseDeDatosCedrosEnvaSarah7012012.xlsx")

wb.sheetnames

sheet = wb['Cedros Arb juv bosq']
dfBosque = pd.DataFrame(sheet.values)
dfBosque.columns = dfBosque.iloc[0]
dfBosque.drop(0,0, inplace=True)
## what do we need from this?
dfBosque = dfBosque[[ 'Norte ', 'Este', 'Elevación', 'Hábitat', 'Sitio/Site', 'Parcela/Plot', 'Sub.Parc.', 'Altura ', 'Familia', 'Genero ', 'Especie']]
## clean up the names a little. As per before, I think the northing and easting are mixed up, reverse here:
dfBosque.rename(index=str, columns=
        {'0':'', 
        'Norte ':'este', 
        'Este': 'norte',
        'Elevación': 'elevacion',
        'Hábitat': 'habitat',
        'Sitio/Site': 'site',
        'Parcela/Plot':"parcela",
        'Sub.Parc.':"subparcela", 
        'Altura ': 'altura',
        'Familia': 'familia',
        'Genero ': 'genero',
        'Especie': 'especie'},
        inplace=True)
## a lot of empty rows now...
dfBosque.dropna(how='all', axis=0, inplace=True)

## repeat the above for the regen plots:

wb.sheetnames

sheet = wb['Cedros Arb juv reg ']
dfRegen = pd.DataFrame(sheet.values)
dfRegen.columns = dfRegen.iloc[0]
dfRegen.drop(0,0, inplace=True)
## what do we need from this?
dfRegen = dfRegen[[ 'Norte ', 'Este', 'Elevación', 'Hábitat', 'Sitio/Site', 'Parcela/Plot', 'Sub.Parc.', 'Altura ', 'Familia', 'Genero ', 'Especie']]
## clean up the names a little. As per before, I think the northing and easting are mixed up, reverse here:
dfRegen.rename(index=str, columns=
        {'0':'', 
        'Norte ':'este', 
        'Este': 'norte',
        'Elevación': 'elevacion',
        'Hábitat': 'habitat',
        'Sitio/Site': 'site',
        'Parcela/Plot':"parcela",
        'Sub.Parc.':"subparcela", 
        'Altura ': 'altura',
        'Familia': 'familia',
        'Genero ': 'genero',
        'Especie': 'especie'},
        inplace=True)
## get rid of empty rows now...
dfRegen.dropna(how='all', axis=0, inplace=True)

(dfBosque.columns.values == dfRegen.columns.values).all()

## now join these two:

juvSpecObs = pd.concat([dfBosque, dfRegen])
juvSpecObs.reset_index(drop=True, inplace=True)

## look okay?
juvSpecObs.head(2)
juvSpecObs.tail(2)


## how do we make a unique taxonomic unit out of each species?
## needs to be compatible with the mature species dataframe...

## looks good. now we need to create unique familyGenusSpecies names 
## get rid of white spaces 

juvSpecObs = pd.concat([dfBosque, dfRegen])
juvSpecObs.reset_index(drop=True, inplace=True)
strCols=["familia", "genero", "especie", "habitat"]
for j in strCols:
    print(j)
    juvSpecObs[j] = juvSpecObs[j].str.strip()


juvSpecObs.especie.isnull().any() ## we have some empty species slots
juvSpecObs.genero.isnull().any() ## no empty genera slots
juvSpecObs.familia.isnull().any() ## no empty family slots
juvSpecObs.especie[juvSpecObs.especie.isnull()] = ""
juvSpecObs.especie.isnull().any() ## that worked... keep following our original data cleaning process
## fix some typos and other errors:
juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis', inplace=True) 
juvSpecObs.genero.replace('Saurauria', 'Saurauia', inplace=True)
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia', inplace=True) 
juvSpecObs.especie.replace('salviodes', 'salvioides', inplace=True) 
juvSpecObs.genero.replace('Shoenobiblus', 'Schoenobiblus', inplace=True) 
juvSpecObs.especie.replace('cf. imperale', 'cf. imperiale', inplace=True) 
juvSpecObs.genero.replace('Beilschmedia', 'Beilschmiedia', inplace=True) 
juvSpecObs['genusSpecies'] = juvSpecObs.familia + " " + juvSpecObs.genero + " " + juvSpecObs.especie
juvSpecObs['genusSpecies'] = juvSpecObs['genusSpecies'].str.strip()
juvSpecObs['PsubP'] = juvSpecObs["parcela"].astype('str') + "." + juvSpecObs["subparcela"].astype('str')
juvSpecObs['PsubP'] = juvSpecObs.PsubP.astype('float')
juvSpecObs = juvSpecObs[["site","parcela","subparcela","PsubP","familia","genero","especie","genusSpecies","elevacion","habitat",]]
juvSpecObs.genusSpecies.replace("Indeterminada","Indeterminado", regex=True, inplace=True)
juvSpecObs['genusSpecies'] = juvSpecObs['genusSpecies'].str.replace('sp. ', 'sp.', regex=False)
## for our uses, the following are dubious data, to be removed:
probablyNotTrees = pd.Series([
    "Euphorbiaceae sp.", 
    "Rubiaceae Notopleura anomothyrsa", 
    "Urticaceae sp.", 
    "Sapindaceae Paullinia capreolata", 
    "Sapindaceae Paullinia cf. nobilis", 
    "Asteraceae Neurolaena cf. lobata", 
    "Boraginaceae sp.", 
    "Rubiaceae Psychotria cf. stipularis", 
    "Solanaceae sp.", 
    "Indeterminado sp.22", 
    "Indeterminado sp.21", 
    "Arecaceae Aiphanes erinaceae", 
    "Melastomataceae sp.4", 
    "Arecaceae sp.", 
    "Fab-faboideae sp.1", 
    "Indeterminado sp.20", 
    "Malvaceae sp.", 
    "Moraceae sp.2", 
    "Sapindaceae Paullinia cf. capreolata", 
    "Menispermaceae Abuta aff. chocoensis", 
    "Araliaceae Schefflera lasiogyne", 
    "Indeterminado sp.26", 
    "Rubiaceae Palicourea aff. sodiroi", 
    "Rubiaceae Palicourea sodiroi", 
    "Indeterminado sp.27", 
    "Dennstaedtiaceae Saccoloma inaequale", 
    "Gesneriaceae sp."
])
aa = ~juvSpecObs['genusSpecies'].isin(probablyNotTrees)
juvSpecObs = juvSpecObs[aa]
juvSpecObs.sort_values('PsubP', inplace=True)
juvSpecObs.reset_index(drop=True, inplace=True)
#juvSpecObs.to_csv('juvSpecObs.csv', index=False)


juvSpecObs = pd.read_csv("juvSpecObs.csv")

len(juvSpecObs.genusSpecies.unique()) ## 176 unique species

## how many of these are in our mature tree species list?

specObs = pd.read_csv('specObs.csv')


len(np.isin(juvSpecObs.genusSpecies.unique(), specObs.genusSpecies.unique())) ## 149

sum(np.isin(juvSpecObs.genusSpecies.unique(), specObs.genusSpecies.unique())) ## 111 species found in mature tree data

## looks like about 70 of juvenile trees are not in the mature species lists
## that is a lot, considering there are only ~300 mature tree species, and
## the area sampled for these juvies is much smaller, I think.
## so the forest is changing, probably

## is this all real, or are there some typos causing some of this?
## how to check this...
 
jvUniq = juvSpecObs.genusSpecies.unique()

## these are the juv species species reportedly not in our mature species

notIn=~(np.isin(jvUniq, specObs.genusSpecies.values))

jvNotMat = jvUniq[notIn]

## just spotcheck a bunch of these, don't know a better way to do this...

## a lot these seem correct, but the indeterminada species I think may well 
## be the same as indeterminado in the mature tree species

## for now, assume they are the same

juvSpecObs.genusSpecies.replace("Indeterminada","Indeterminado", regex=True, inplace=True)

## also we have two "species" that are authorities:

juvSpecObs = pd.read_csv("juvSpecObs.csv")

## add this to above pipeline, also

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis')

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis')[466]
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia')[466]

juvSpecObs.genero.replace('theobromifolia', 'Caryodaphnopsis',True)
juvSpecObs.especie.replace('(A.H. Gentry) van der Werff & H.G. Richt.', 'theobromifolia', inplace=True)

## back up, put this in the formation of the table

## okay, so should be pretty compatible now...
## now what?

## we want to see how things are shifting over time. Are sites being pulled 

## maybe repeat our 

## seems like the first stop would be to get a community matrix for the juveniles

juvSpecObs = pd.read_csv('juvSpecObs.csv')

juvSpecObs.head()

aa = juvSpecObs[['PsubP', 'genusSpecies']]
aa.drop_duplicates(inplace=True)
juvDummy = pd.get_dummies(aa['genusSpecies'])
## that's presence/absence, not sure if we want that...
## for subParcelComm we did this:

aa = juvSpecObs[['PsubP', 'genusSpecies']]
aa.set_index('PsubP', inplace=True)
bb = pd.get_dummies(aa['genusSpecies'])

## okay, we need to collapse these 
bb.reset_index(inplace=True)
cc = bb.groupby('PsubP')
juvDummy = cc.agg(np.sum)

juvDummy.iloc[1:20,1:4]

juvDummy.sum(axis=1)

juvDummy.sum(axis=0)

juvDummy.head()

juvDummy.to_csv('juvComm.csv')


juvComm = juvDummy

## that's nice, but we might want matrix that is compatible with the larger community matrix. 
## How can we add in our extra columns 

## let's compare to our other community matrix:

subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')

subParcelComm.head()

juvDummy.head()

subParcelComm.columns.isin(juvDummy.columns.values)

'Actinidaceae.Saurauia.sp..1' in subParcelComm.columns.values

'Actinidaceae.Saurauia.sp..1' in juvDummy.columns.values

'Actinidaceae Saurauia sp.1' in juvDummy.columns.values

## the column names are different...can we fix this? 
## recreate the community matrix from the mature trees:

aa = specObs[['PsubP', 'genusSpecies']].copy()
aa.PsubP = aa.PsubP.astype(float)
aa.sort_values('PsubP', inplace=True)
aa.PsubP = aa.PsubP.astype(str)
aa.set_index('PsubP', inplace=True)

bb = pd.get_dummies(aa['genusSpecies'])
## okay, we need to collapse these 
bb.reset_index(inplace=True)
cc = bb.groupby('PsubP')
specDummy = cc.agg(np.sum)

specDummy.sum(axis=0).all()
specDummy.sum(axis=1).all()

specDummy.columns.isin(juvDummy.columns.values)

len(specDummy.columns.isin(juvDummy.columns.values))
sum(specDummy.columns.isin(juvDummy.columns.values))

juvDummy.columns.isin(specDummy.columns.values)

len(juvDummy.columns.isin(specDummy.columns.values))
sum(juvDummy.columns.isin(specDummy.columns.values))

## does this community matrix produce the same results
## as our previous?

specDummy.index

#specDummy.to_csv('specDummy.csv')

#specDummy = pd.read_csv('specDummy.csv')

## take a look at an nms:

library(vegan)
library(RColorBrewer)

## our newest version of entire community matrix
specDummy <- read.csv('specDummy.csv', row.names='PsubP')
specDummy <- specDummy[order(as.numeric(row.names(specDummy))),]
nms <- metaMDS(specDummy, try=40)
#stressplot(nms) ## looks pretty similar to our other stresses from older version
## run this through the below graphics pipeline

## compare it with our previous, saved entire community matrix
subParcelComm <- read.csv('subParcelComm.csv', row.names='PsubP')
nms <- metaMDS(subParcelComm, try=40)

## and with a rerun version of the subparcelcomm pipeline
subParcelComm2 <- read.csv('subParcelComm2.csv', row.names='PsubP')
subParcelComm2 <- subParcelComm2[order(as.numeric(row.names(subParcelComm2))),]
nms <- metaMDS(subParcelComm2, try=40)

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat)
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups
plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
)
text(nmsInfo$MDS1, nmsInfo$MDS2, nmsInfo$PsubP, cex=1.5)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)

## fuck, these two dataframes are giving us slightly different results...
## why?

## the pipeline I have in the notebook goes like this:

specObs = pd.read_csv('specObs.csv')
smallSpecObs = specObs[['PsubP','genusSpecies']]
bdfDummy = pd.get_dummies(smallSpecObs['genusSpecies'])
bdfDummy.insert(0, 'PsubP', smallSpecObs.PsubP)
bdfDummyGroup = bdfDummy.groupby('PsubP')
subParcelComm = bdfDummyGroup.agg(np.sum)
#subParcelComm.to_csv('subParcelComm.csv') 


## 2) get columns of juveniles and mature trees to line up
## 3) add zero columns where the juveniles lack 

## then start poking around with the juvenile data

## if we need to reorder, this is the code
subParcelComm['PsubP'] = subParcelComm['PsubP'].astype('float')
subParcelComm.sort_values('PsubP', inplace=True)
subParcelComm.set_index('PsubP', inplace=True)
#subParcelComm.to_csv('subParcelComm.csv')

## let's just delete the python object of this to 
## avoid confusion. 

## does R always insert the periods in the name? Run above NMS 
## pipeline again...yes 

## okay, just don't save over from R

## now, to make the mature and juvenile species data frames more compatible...

##  

juvSpecObs = pd.read_csv('juvSpecObs.csv')
subParcelComm = pd.read_csv('subParcelComm.csv')
juvComm = pd.read_csv('juvComm.csv')
specObs = pd.read_csv('specObs.csv')

subParcelComm.columns.isin(juvComm.columns.values)
len(subParcelComm.columns.isin(juvComm.columns.values))
sum(subParcelComm.columns.isin(juvComm.columns.values))

juvComm.columns.isin(subParcelComm.columns.values)
len(juvComm.columns.isin(subParcelComm.columns.values))
sum(juvComm.columns.isin(subParcelComm.columns.values))

## they share 104 species.  
## for a total of 417 species 

juvComm.shape

subParcelComm.shape

## seems like we want three matrices:
## juveniles
## mature trees
## combined

## as far as the combined, what is the point of truly 
## combining the juvenile and mature trees...? 
## maybe just use this for creating the other two. 


## regardless, I think all should have all the same rows (sites)
## and all the same species (columns). 

## how to do this...

## the combined matrix should be the easiest, and maybe we can
## recycle the columns and rows.

## an outer join is the default for concat, what happens 
## when we just use defaults:

aa = pd.concat([subParcelComm, juvComm], sort=False)

## this sorts the columns, integrating the new ones from juvenile data:
bb = pd.concat([subParcelComm, juvComm], sort=True)

## do we need to flatten the rows that have the same PsubP values?
## as per above, don't think so. This combo matrix can just be for 

## let's check this manually, in a text file:

dd = bb.columns.astype('str').copy().to_series()

with open('comboColumns.txt', 'w') as cc:
    for i in dd:
        cc.write(i + "\n")



## looking this over, the "sp." string is a problem. Somethings that are
## presumably the same are being counted as different.

 

## the creation of the two submatrices.

dd = bb.columns.copy().to_series()

dir(dd.str)

help(dd.str.replace)

ee = dd.str.replace('sp. ', 'sp.', regex=False)

len(ee) ## 417
len(ee.unique()) ## 411

## looks like 6 cases of a space after species potentially causing problems. 

## what next - got to clean the original species observation matrices, then rerun
## the whole community matrix pipelines, maybe all analyses that used these
## matrices (basically eveything in the notebook, jesus christ).

## but perhaps the best workflow here is to create the master, combined
## community matrix, that can be subset easily to the juvenile and mature
## trees

## if I do that, no need to individually create or correct the community matrices
## but not sure how to index it so I can do that easily...

## regardless the specobs and juvSpecObs need to be cleaned up. Next time...

## and now it's next time. so, let's try for both specObs and and juvSpecObs,
## then rerun the community matrix creation pipelines. 

## fixed, there are now 104 shared species among the juveniles and adults

##  so to start again, again:

bb = pd.concat([subParcelComm, juvComm], sort=True)

## how can we retain indices?

subParcelComm.set_index('PsubP', inplace=True)
juvComm.set_index('PsubP', inplace=True)

subParcelComm.index
juvComm.index



combCommMat = pd.concat([subParcelComm, juvComm], keys=['adulto','joven'], sort=True)
## how can we get zeroes instead of NA's?
combCommMat.fillna(0, inplace=True)
## get a better look at this...
combCommMat.to_csv('comboMat.csv')

## for me, this looks good. If need to quickly subset by adult/juvenile?

combCommMat.loc['adulto',:]

## you can pass the nested indices as tuples:
combCommMat.loc[('adulto',9.2)]

## ok, that works well.

## now what?

## getting closer to the fun questions. 

## I think we may need to rerun all the same pipelines on the seedling 
## data

## the ultimate result should be some sort graphic showing how 
## far the seedling communities have "wandered", and in what direction.

## are they heading into communities that we have already defined 
## with our cluster analysis of the adults? Or is the forest 
## creating something new with it's seedling community?

## To do this properly, we need a coordinate system on which to place 
## the communities. 

## each seedling site should be shown in respect to its mother data. 

## a good first first look would be a regular old NMS, color coded 
## by mother/daughter pairs, with hulls of the old cluster boundaries. 

## can we rig up an NMS real quick like?

R 

library(vegan)
library(RColorBrewer)
comboCommMat <- read.csv('comboMat.csv')

comboCommMat[,'X']

comboCommMat[1:4,1:4]

comboCommMat[,1:4]

tail(comboCommMat)[,1:4]

sum(comboCommMat$X == 'adulto')
sum(comboCommMat$X == 'joven')

adultosAll <- comboCommMat[comboCommMat$X == 'adulto',]

dim(adultosAll)

jovenes <- comboCommMat[comboCommMat$X == 'joven',]

dim(jovenes)

## can we trim off the rows that don't have juvenile data?

adultosInjovenes <- comboCommMat$PsubP %in% jovenes$PsubP

dim(comboCommMat[adultosInjovenes,])

adultosDejovenes <- comboCommMat[adultosInjovenes,]

adultosDejovenes$PsubP

aa <- adultosDejovenes[,3:ncol(adultosDejovenes)]
comboNMS <- metaMDS(aa)
## are any of these species matrix columns empty?

all(colSums(aa) > 0)

any(colSums(aa) == 0)

sum(colSums(aa) == 0) ## 52? wow...that is ~1/8 of our columns...

## if we get rid of these, does it change our NMS?

bb <- aa[,colSums(aa) > 0]
comboNMS <- metaMDS(bb)


## okay, now how do we want to color this? 
## we need to color by the old clusters and habitat types,
## but also need to match up the seedlings with their 
## parent groups 

## to get cluster and habitat type:

envOnly <- read.csv('envOnly.csv')
cGroup <- read.csv('cGroup.csv')

head(cGroup)

all(envOnly$PsubP == cGroup$PsubP)

adultosDejovenes[1:4,1:4]

adultosDejovenes


adultosDejovenes[1:3,1:3]

cGroup[cGroup$PsubP == x,'gr']

## now, how to create new vectors of cluster and habitat type
comboGr <- sapply(adultosDejovenes$PsubP, function(x){ cGroup[cGroup$PsubP == x,'gr']})
comboHab <- sapply(adultosDejovenes$PsubP, function(x){ envOnly[envOnly$PsubP == x,'habitat']})

## those will be hull groups. We'll constrain hulls to the parents only
## What about parent/child designations? I guess we can use a different shape, and text, 
## this should become obvious. 

MDS1 <- comboNMS$points[,'MDS1']
MDS2 <- comboNMS$points[,'MDS2']
plot(MDS1, MDS2,
col=comboGr)
ordihull(comboNMS, comboGr, col=1:4, lwd=3)
legend( x='bottomright',
        legend = 1:4,
        fill = 1:4,
        cex = 1.0,
        title='Cluster',
        )

## how can we constrain to our old hulls? basically, we want polygons 
## that are defined only by the parent points, that the child points 
## can escape.

## we can recover this information by only putting hulls around the adults:

## for hulls:
comboGrAdOnly <- comboGr
for (i in 1:length(comboGrAdOnly)){
    if (adultosDejovenes[i,'X'] == 'joven'){
    comboGrAdOnly[i] <- 0}
}

## for pch:
## we want solid colors, triangle for 
## juvenile plots, circles for adults 

symbs=vector(length=length(comboGr))
for (i in 1:length(comboGr)){
    if (adultosDejovenes[i,'X'] == 'adulto'){
        symbs[i] <- 19} else {
            if (adultosDejovenes[i,'X'] == 'joven'){
                symbs[i] <- 17}
}}

#png(file='jovenesNMS.png', width=1000, height=800)
plot(MDS1, MDS2,
col=comboGr,
pch=symbs,
cex=2.0,
)
ordihull(comboNMS, comboGrAdOnly, col=c('NA',1,2,3,4), lwd=3)
#dev.off()

#png(file='jovenesLeavingNestClusterNMS.png', width=1000, height=1000)
## zoom a little, see how that looks:
MDS1 <- comboNMS$points[,'MDS1']
MDS2 <- comboNMS$points[,'MDS2']
plot(MDS1, MDS2,
col=comboGr,
pch=symbs,
cex=2.0,
xlim=c(-0.3, 0.3),
ylim=c(-0.3, 0.3),
)
ordihull(comboNMS, comboGrAdOnly, col=c('NA',1,2,3,4), lwd=3)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)
#dev.off()
## need a nice legend:
legend( x='bottomright',
        legend = c(1:4),
        fill = c(1:4),
        col = c(1:4),
        cex = 1.0,
        lty = 1,
        lwd = 2,
        title='Cluster',
        )
legend( x='bottomleft',
        pch = c(2,1),
        legend = c('juvenile','adult'),
        cex = 1.0,
        pt.cex = c(1.0,1.5),
        title='Age class',
        )

## things to check:
## wtf is going on with that point way out east?
## how do these points cluster

comboNMS$points

aa <- identify(comboNMS$points)

head(comboNMS$points)

identify(comboNMS$points)

comboNMS$points[identify(comboNMS$points),]

## habitat seedlings:
## we want all points to have the colors of their or their 
## parent points' habitat
## we want the hulls around the adult (parents) points,
## and not necessarily their child points, which should 
## be free to "escape" their parental points. 
## symbols should be one of two types, juvenile or adults

comboHabAdOnly <- comboHab
levels(comboHabAdOnly) <- c(levels(comboHabAdOnly), 'juv')
for (i in 1:length(comboHabAdOnly)){
    if (adultosDejovenes[i,'X'] == 'joven'){
    comboHabAdOnly[i] <- 'juv'}
}

## hull colors
hullColrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00', NA)
names(hullColrsMap) <- names(table(comboHabAdOnly))
## regular colors, all points need this juv or adult:
colrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00')
names(colrsMap) <- names(table(comboHab))
## does this work?
colrs <- colrsMap[comboHab]

plot(MDS1, MDS2,
    col=colrs,
    pch=symbs,
    cex=2.0,
    xlim=c(-0.3, 0.3),
    ylim=c(-0.3, 0.3),
)
ordihull(comboNMS, comboHabAdOnly, col=hullColrsMap, lwd=3)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)
legend( x='bottomright',
        legend = levels(droplevels(comboHab)),
        fill = colrsMap[levels(droplevels(comboHab))],
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
legend( x='bottomleft',
        pch = c(2,1),
        legend = c('juvenile','adult'),
        cex = 1.0,
        pt.cex = c(1.0,1.5),
        title='Age class',
        )

## looks like some groups are unrepresented?
droplevels(comboHab)
## looks like we have no juvenile data from clearings or bosques secondario

names(table(comboHabAdOnly))


########## hierarchical clustering of juveniles #############

## repeat the hierarchical clustering
## to start, we look for grouping of the juveniles 
## only. 

library(stats)
library(vegan)

## where is our community matrix of juveniles?

juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')

## checks
rowSums(juvComm)
any(colSums(juvComm) == 0)
all(colSums(juvComm) > 0)


juvCommBray <- vegdist(juvComm)
juvComm.ward <- hclust(juvCommBray, method='ward.D2')

#save(juvComm.ward, file='juvCommHierClust.rda')

plot(juvComm.ward)
k=3
juvGrp <- rect.hclust(juvComm.ward, border=c('purple','yellow','blue'), k=k)

juvGrp <- rect.hclust(juvComm.ward,  k=k)

juvGrp

vector(length = length(unlist(juvGrp)))

## nms of juveniles only:
juvNMS <- metaMDS(juvComm)
#save(juvNMS, file='juvNMS.rda')

## get the cluster group membership:
par(mfrow=c(1,2))
k=3
plot(juvComm.ward)
juvGrp <- rect.hclust(juvComm.ward, 
    border=(1:k+4),  
    #border=c(5,6,7),  
    k=k)
jvgr <- NULL
for (i in 1:k){
groupNu <- rep(i, length(juvGrp[[i]]))
jvgr <- c(jvgr, groupNu)
}
names(jvgr) <- names(unlist(juvGrp))
## sort these to match the order of our NMS
jvgr <- jvgr[rownames(juvNMS$points)]
plot(juvNMS$points, 
    cex=1.5,
    bg=jvgr+4,
    pch=24,
)
text(juvNMS$points[,'MDS1'], 
    juvNMS$points[,'MDS2'] - 0.02, 
    rownames(juvNMS$points), 
    cex=1)

## save, with a better name:
juvSpecClustMap <- jvgr
#save(juvSpecClustMap, file='juvSpecClustMap.rda')

## k=2 seems like the most natural split here.

## works okay... we should probably export that tree
## as a svg and work on it a little...later

## we need a single graph that shows the evolution of these 
## groups. It should have the hulls of both the old and new 
## clusters, plus the colors of the old, 

## use our combined NMS of juv and adults:

library(vegan)

## we need a vector for hulls, indicating which cluster the 
## subplot belongs to (74 objects)

## regenerate our community matrix of sites with adults and juveniles:
par(mfrow=c(1,1))
comboCommMat <- read.csv('comboMat.csv')
jovenes <- comboCommMat[comboCommMat$X == 'joven',]
adultosInjovenes <- comboCommMat$PsubP %in% jovenes$PsubP
adultosDejovenes <- comboCommMat[adultosInjovenes,]
comboInfo <- adultosDejovenes[,1:2]
colnames(comboInfo) <- c("ageClass","PsubP")
## our adult cluster results were here:
cGroup <- read.csv('cGroup.csv')
adultGroupMaps <- cGroup$gr
names(adultGroupMaps) <- cGroup$PsubP
## our juvenile cluster results were here:
load('juvSpecClustMap.rda')
PsubP <- as.character(comboInfo[comboInfo$ageClass == 'adulto','PsubP'])
## these should be our cluster groups.
adultGroupMaps[PsubP]
## juveniles, bump them forward a bit
juvSpecClustMap[PsubP] + max(adultGroupMaps[PsubP])
## this can be a column in our dataframe 
comboInfo$Clust <- c(adultGroupMaps[PsubP], juvSpecClustMap[PsubP] + max(adultGroupMaps[PsubP]))
## make our symbols 
symbs <- vector(length = nrow(comboInfo))
for (i in 1:length(symbs)){
    if (comboInfo[i,'ageClass'] == 'adulto'){
        symbs[i] <- 21} else {
            if (comboInfo[i,'ageClass'] == 'joven'){
                symbs[i] <- 24}
}}
comboInfo$symbs <- symbs
## we also want to retain the original parent cluster
## info for the juveniles:
comboInfo$parentClust <- adultGroupMaps[as.character(comboInfo$PsubP)]

plot(1:3, rep(1,3), cex=4, pch=24, col=c(1,2,3), bg=c(4,5,6))

## so can we use this to color our NMS graphic?
load('comboNMS.rda')
MDS1 <- comboNMS$points[,'MDS1'] 
MDS2 <- comboNMS$points[,'MDS2'] 
plot(MDS1, MDS2,
    bg=comboInfo$Clust,
    pch=comboInfo$symbs,
    col=comboInfo$parentClust,
    lwd=4.0,
    cex=2.0,
)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)
#ordihull(comboNMS, comboInfo$Clust, col=comboInfo$Clust, lwd=3)
ordihull(comboNMS, comboInfo$Clust, col=names(table(comboInfo$Clust)), lwd=3)
legend( x='bottomright',
        legend = 1:7,
        pch = c(rep(21,4), rep(24,3)),
        pt.bg = 1:7,
        ncol = 2,
        cex = 2.0,
        title='Cluster',
        )

## that is nice. Can we get rid of the outlier and take another look?
## the outlier is the juvenile point of 1.1. Should we also remove the parent point?
## kind of makes sense, we because we removed the rest of the mature tree data
## that weren't parent points with juvenile data...


## our NMS was made from this community matrix of adult and juvenile

comboCommMat <- read.csv('comboMat.csv')
jovenes <- comboCommMat[comboCommMat$X == 'joven',]
adultosInjovenes <- comboCommMat$PsubP %in% jovenes$PsubP
adultosDejovenes <- comboCommMat[adultosInjovenes,]
#save(adultosDejovenes, file='adultosDejovenes.csv')
## get rid of outlier point from community matrix
aa <- adultosDejovenes[adultosDejovenes$PsubP != '1.1',]
## get rid of outlier point from nmsInfo 
aaInfo <- comboInfo[comboInfo$PsubP != 1.1,]
all(aa$PsubP == aaInfo$PsubP)
## get rid of label columns
bb <- aa[,c(3:ncol(aa))]
## check row names
all(bb$PsubP == aaInfo$PsubP)
## redo NMS without outlier:
comboNMS_noOut <- metaMDS(bb)
## interesting, that increased the stress:
## plot this as above 

MDS1 <- comboNMS_noOut$points[,'MDS1'] 
MDS2 <- comboNMS_noOut$points[,'MDS2'] 
plot(MDS1, MDS2,
    bg=aaInfo$Clust,
    pch=aaInfo$symbs,
    col=aaInfo$parentClust,
    lwd=4.0,
    cex=2.0,
)
text(MDS1, MDS2 - 0.005, aaInfo$PsubP, cex=1)
ordihull(comboNMS_noOut$points, aaInfo$Clust, col=names(table(aaInfo$Clust)), lwd=3, lty = c(rep(1,4), rep(2,3)))
legend( x='bottomright',
    legend = 1:7,
    pch = c(rep(21,4), rep(24,3)),
    pt.bg = 1:7,
    ncol = 2,
    cex = 2.0,
    title='Cluster',
    bty='n',
    )
## and a legend for the lines?
legend( x='bottomleft',
    legend = c('mature cluster', 'juvenile cluster'),
    lty = c(1,2),
    bty='n',
    )


## I think that works for now, to talk to Ana and Bitty about this
## we'll probably revisit to improve the aesthetics...

## and repeat this for the land uses ('habitat')?

## meh. But one thing that might be important to look at is if our combined matrix of adult and child 
## points recapitulates these results. The results I think I am seeing is that original four adult 
## clusters should be inside of our new cluster 7. As in, the RCA points and their allies in the regeneration
## plots should cluster with the four adult groups, 

## our combined child/parent matrix, with outlier (1.1) still included

adultosDejovenes[1:4,1:4]

## curious, is the cluster affected by the zero columns?

aa <- adultosDejovenes[,3:ncol(adultosDejovenes)]
## make a unique identifier for rownames
comboInfo$agePsubP <- paste(comboInfo$ageClass, comboInfo$PsubP, sep='.')
comboInfo$agePsubP <- sub('adulto', 'A', comboInfo$agePsubP)
comboInfo$agePsubP <- sub('joven', 'J', comboInfo$agePsubP)
rownames(aa) <- comboInfo$agePsubP
## do the hierarchical clustering with this
k=7
aaBray <- vegdist(aa)
aa.ward <- hclust(aaBray, method='ward.D2')
allComboClust <- rect.hclust(aa.ward, k=k) 
plot(aa.ward)
## get the cluster group membership:
allComboGrp <- rect.hclust(aa.ward, 
    border=(1:k),  
    k=k)
cgr <- NULL
for (i in 1:k){
groupNu <- rep(i, length(allComboGrp[[i]]))
cgr <- c(cgr, groupNu)
}
names(cgr) <- names(unlist(allComboGrp))
## sort these to match the order of our NMS
comboInfo$cgr <- cgr[rownames(aa)]
## clean up a little
comboInfo <- comboInfo[,c("ageClass","PsubP","agePsubP","Clust","symbs","parentClust","cgr")]

head(comboInfo)

## now, what are we trying to do with this? 

## let's see how our NMS graphics compare with this new clustering system, 
## which was done on all the child/adult points at once, instead of 
## piecing together the adult and child ones separately. 

## how to do this... use our old NMS:

load('comboNMS.rda')
MDS1 <- comboNMS$points[,'MDS1'] 
MDS2 <- comboNMS$points[,'MDS2'] 
par(mfrow=c(1,2))
plot(MDS1, MDS2,
    pch=comboInfo$symbs,
    col=comboInfo$Clust,
    lwd=4.0,
    cex=2.0,
)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)
ordihull(comboNMS, comboInfo$Clust, col=names(table(comboInfo$Clust)), lwd=3)
plot(MDS1, MDS2,
    pch=comboInfo$symbs,
    col=comboInfo$cgr,
    lwd=4.0,
    cex=2.0,
)
ordihull(comboNMS, comboInfo$cgr, col=names(table(comboInfo$cgr)), lwd=3)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)

## huh. The results are extremely similar. Even the order is 
## almost the same, the parent forest clusters got switched in 
## order, and a few points swapped cluster groups, but generally
## the hierarchical clustering algorithm was consistent. That's 
## encouraging. So I think we can just move forward with the 
## graphics as they are, and move on to the next step of 
## arguing for alternative stable states using the clusters 
## as we have them

## what about the case for highly endemic communities - can we 
## work on the MEMs a bit... perhaps classify all the plots 
## based on their membership within watersheds of different 
## size classes, and match MEMs to these?

## classification of forest types? As in, can we build a 
## logit linear model based on environmental characteristics
## that predicts the two forest types? I think 

## if we get that far, we probably should call it. This is going to 
## take too long otherwise. 

################ axes of succession ######################

## okay, now we want to create one or two axes along which 
## we can rank these communities, in terms of their 
## similarity to the alternate stable states we think we 
## are seeing. 

## we think we are seeing a trajectory that the forest plots 
## are following (vaguely west on the overall nms graphic)
## and a different trajectory followed 
## and the argument is that the regeneration plots are 
## being divided by these two different poles, some are going 
## one way and some are going the other. 

## the simplest graphic I can think of would be two axis, 
## one with BC distance to the combined indicator species of the 
## parent forest types, and one with the indicator types

## I feel like we have to use the parent indicator species,
## even though we know that the child communities are very
## different, because it is too circular to use indicator 
## species from our juvenile plots

## anyway, we want to assemble a cast of species to 
## compare all points against... it seems like the best here
## would to get all indicator species associated the 
## clusters 3&4 in any way as one axis, and clusters 1&2 for 
## the other.

## axis one: similarity to all indicated species of forests
## axis two: similarity to all indicated species of disturbed sites

## is BC right for this? 

## not sure. Let's get to it, see how it behaves...

## go back over indicator species process:

##############
library(indicspecies)
library(stats)
library(vegan)
## our community matrix...
comM <- read.csv('subParcelComm.csv', row.names="PsubP")
envOnly <- read.csv('envOnly.csv', row.names="PsubP")
cGroup <- read.csv('cGroup.csv', row.names="PsubP")

## order okay?
all(rownames(envOnly) == rownames(comM))
all(rownames(cGroup) == rownames(comM))

## run it. We have some pvalues right on the edge of our cutoff, so let's use
## a lot of permutations for some added precision here:
clustIndSpp <- multipatt(comM, cGroup$gr, func = 'r.g', control=how(nperm=19999))

## how does it look?:

summary(clustIndSpp)

str(clustIndSpp)

head(clustIndSpp$sign)

## how can we lift our species of interest out of this? For instance, to get 
## all the "native" forest species of interest? This would any indicators for 
## type 3 or 4, or 3+4:

## our actual results are here:
indSign <- clustIndSpp$sign

## retain only statistically significant at 0.05
sigSign <- indSign[indSign$p.value <= .05,]

## Annonaceae.Guatteria.megalophylla should be 2+3, if I'm reading this right
## Moraceae.Ficus.cuatrecasana also.
## so if we want 3, 4, and 3+4, we should just keep the rows that have a non-zero
## value for 3 or 4:

forestSign <- sigSign[sigSign$s.3 == 1 | sigSign$s.4 == 1,]
## but we need to eleminate any that are indicators of clusters 1 or 2:
forestSign <- forestSign[forestSign$s.1 == 0 & forestSign$s.2 == 0,]
## and these should be our "native forest community":
natforc <- rownames(forestSign)

## we can repeat this for the other "pole", something like disturbed forest community:

distForestSign <- sigSign[sigSign$s.1 == 1 | sigSign$s.2 == 1,]
## but we need to eleminate any that are indicators of clusters 1 or 2:
distForestSign <- distForestSign[distForestSign$s.3 == 0 & distForestSign$s.4 == 0,]
## and these should be our "native forest community":
distforc <- rownames(distForestSign)

## is it inappropriate to include the regen indicators, if our hypothesis is that 
## the regen sites are plastic, or pluripotent? 

## leave them in for the moment. Think about this more. 

## what would be the plan here...

## what community matrix are we working with here...the combined or the juvenile only?
load('adultosDejovenes.csv')

## all of these names make sense?
distforc %in% colnames(adultosDejovenes)
natforc %in% colnames(adultosDejovenes)

## subset these matrices using these species:

colnames(adultosDejovenes) %in% distforc

comboNatForcMat <- adultosDejovenes[, colnames(adultosDejovenes) %in% natforc]
rownames(comboNatForcMat) <- comboInfo$agePsubP 
## I think it makes the most sense to convert to presence/absence here, because I
## have no idea how to come up with some sort of proper abundances for the "ideal"
## community 
comboNatForcMat[comboNatForcMat > 0] <- 1

## before we add our "ideal community", check:
all(colSums(comboNatForcMat) > 0) ## nope. 
colSums(comboNatForcMat) > 0 
## Lauraceae.Ocotea.stenoneura is not represented in any of these subparcels
## not sure if that is important. Carry on. 
## check rownames
cbind(adultosDejovenes$PsubP, rownames(comboNatForcMat))
## looks good

## so, to add our ideal community:
comboNatForcMat <- rbind(comboNatForcMat, rep(1,nrow(comboNatForcMat)))
rownames(comboNatForcMat)[75] <- "idealNatForest"

## now get distances from this. I think euclidean isn't inappropriate here, 
## because we're going to consider lots of shared zeroes a sign of similarity:

aa <- as.matrix(vegdist(comboNatForcMat, method='euclidean'))
## and we just need the bottomrow, right?

distFromNatForest <- aa['idealNatForest',]

max(aa['idealNatForest',])

sort(aa['idealNatForest',])

## the most "foresty" plot is A.7.3:
rowSums(comboNatForcMat['A.7.3',])

rowSums(comboNatForcMat['idealNatForest',])

## and our other pole, "disturbed forest"
comboDistForcMat <- adultosDejovenes[, colnames(adultosDejovenes) %in% distforc]
rownames(comboDistForcMat) <- comboInfo$agePsubP 
comboDistForcMat[comboDistForcMat > 0] <- 1
comboDistForcMat <- rbind(comboDistForcMat, rep(1,nrow(comboDistForcMat)))
rownames(comboDistForcMat)[75] <- "idealDistForest"
aa <- as.matrix(vegdist(comboDistForcMat, method='euclidean'))
distFromDistForest <- aa['idealDistForest',]
cbind(distFromDistForest, distFromNatForest)

## how do we subset to just our juveniles?

comboNatForcMat

grep('A', rownames(comboNatForcMat))

grep('J', rownames(comboNatForcMat))

juvNatForcMat <- comboNatForcMat[grep('J', rownames(comboNatForcMat)),]

########## put these together, for quick exploration ##############
metr <- "bray"
#metr <- "euclidean"
## "natural" forest ##
comboNatForcMat <- adultosDejovenes[, colnames(adultosDejovenes) %in% natforc]
rownames(comboNatForcMat) <- comboInfo$agePsubP 
comboNatForcMat[comboNatForcMat > 0] <- 1
juvNatForcMat <- comboNatForcMat[grep('J', rownames(comboNatForcMat)),]
juvNatForcMat <- rbind(juvNatForcMat, rep(1,nrow(juvNatForcMat)))
rownames(juvNatForcMat)[38] <- "idealNatForest"
aa <- as.matrix(vegdist(juvNatForcMat, method=metr))
distFromNatForest <- aa['idealNatForest',]
## "disturbed" forest ##
comboDistForcMat <- adultosDejovenes[, colnames(adultosDejovenes) %in% distforc]
rownames(comboDistForcMat) <- comboInfo$agePsubP 
comboDistForcMat[comboDistForcMat > 0] <- 1
juvDistForcMat <- comboDistForcMat[grep('J', rownames(comboDistForcMat)),]
juvDistForcMat <- rbind(juvDistForcMat, rep(1,nrow(juvDistForcMat)))
rownames(juvDistForcMat)[38] <- "idealDistForest"
aa <- as.matrix(vegdist(juvDistForcMat, method=metr))
distFromDistForest <- aa['idealDistForest',]
#######################
distsForests <- cbind(distFromNatForest, distFromDistForest)
#plot(distFromDistForest, distFromDistForest)
plot(jitter(distsForests), jitter(distsForests))
################
 
## yeah, not useful. What we are looking for is another way to present the
## the above NMS. Staring into those clouds, I think I can see that the 
## forest is moving in two directions, something like a "natural" forest 
## trajectory, with both undisturbed forest types 3 & 4 moving forward 
## into a new community type, or maybe two closely related community types.
## And then the more disturbed forest is hard to characterize. In the NMS
## the communities of the more disturbed forest types move outward from
## from the communities, but radially, each subparcel in a different 
## direction. 

## seems like we need to jettison the idea that the trajectories will 
## in some way predictably resemble the old communities. 

## I see two ways forward - 

## 1) try transforming and getting new axis the juvenile. Use the loading
## to decide what species are driving changes

## 2) repeat the above pipeline using indicator species from the seedlings. I think 
## we lack the stat power to differentiate between the two "natural" forest 
## seedling communities, so we can do a k=2, either with ward algorithm or 
## or with kmeans. Then see if we can create a clean graphic showing progress
## toward one or the other community?

## so do #2 first. This will involve trying to split the seedlings only 
## into two groups: 

library(stats)
library(vegan)

## where is our community matrix of juveniles?
juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')

any(colSums(juvComm) < 1)


juvKmeanClust <- kmeans(juvComm, 2)
juvKmeanClust <- kmeans(juvComm, 3)

juvKmeanClust$cluster

## check an NMS with these? 
juvCommNMS <- metaMDS(juvComm)

all(names(juvKmeanClust$cluster) == rownames(juvCommNMS$points))

##
plot(juvCommNMS$points,
    pch=21,
    bg=juvKmeanClust$cluster,
)

## checks
rowSums(juvComm)
any(colSums(juvComm) == 0)
all(colSums(juvComm) > 0)

## k means didn't work, not sure why... 
juvBray <- vegdist(juvComm)

k=3
juv.ward <- hclust(juvBray, method='ward.D2')
#plot(juv.ward)
juvWardClust <- rect.hclust(juv.ward, k=k) 

plot(juvCommNMS$points,
    pch=21,
    bg=juvKmeanClust$cluster,
)

## still haven't found an elegant solution to this:

cgr <- NULL
for (i in 1:k){
groupNu <- rep(i, length(juvWardClust[[i]]))
cgr <- c(cgr, groupNu)
}
names(cgr) <- names(unlist(juvWardClust))
cgr <- cgr[rownames(juvCommNMS$points)]

rownames(juvCommNMS$points) == names(cgr)

par(mfrow = c(1,2))
plot(juvCommNMS$points,
    pch=21,
    cex=2,
    bg=cgr+4,
)
plot(juv.ward)
juvWardClust <- rect.hclust(juv.ward, k=k, border=1:3 +4) 

## well, that doesn't say anything new
## the ward clustering, the NMS, and the kmeans all disagree

par(mfrow = c(1,2))
plot(juvCommNMS$points,
    pch=21,
    cex=2,
    bg=cgr,
    main='Ward',
)
plot(juvCommNMS$points,
    pch=21,
    cex=2,
    bg=juvKmeanClust$cluster,
    main='kmeans',
)

## this just means we have a messy system that is not easily simplified. 
## it is worth repeating the indicator-species/BC analysis that we tried
## with the parent communities? 

## get indicator species for the three groups of juvenile groups

## 
library(indicspecies)
juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')
juvBray <- vegdist(juvComm)
k=3
juv.ward <- hclust(juvBray, method='ward.D2')
juvWardClust <- rect.hclust(juv.ward, k=k) 
cgr <- NULL
for (i in 1:k){
groupNu <- rep(i, length(juvWardClust[[i]]))
cgr <- c(cgr, groupNu)
}
names(cgr) <- names(unlist(juvWardClust))

juvClustIndSpp <- multipatt(juvComm, cgr, func = 'r.g', control=how(nperm=9999))

summary(juvClustIndSpp)

save(juvClustIndSpp, file='juvClustIndSpp.rda')

## not much to work with there, not enough statistical power
## five species total, 2 for group 1 (forest seedling group)
## 3 species for Group 3 (the regen/rca seedling group) 

## so probably not a fruitful direction here. If there is time/need,
## we can track these species as indicators of the new forest 
## types. But seems like weak sauce. 

## so let's try the PCA approach:

juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')
## transform the abundances
juvComm.hell <- decostand(juvComm, 'hellinger')

## which PCA function to use?

## we'll use the rda() function, which can do PCA if presented 
## with only one function. Just because this is what Borcard uses:

juvCommPCA <- rda(juvComm.hell, scale=TRUE)
save(juvCommPCA, file='juvCommPCA.rda')

summary(juvCommPCA)

## 36 new axes... none of which explains much more than any others
## the first new axis only explains 0.068 of the variance.
## Anyway, we can look at these new axes:


## plot the species
## good site for this: https://fromthebottomoftheheap.net/2012/04/11/customising-vegans-ordination-plots/

l=5
biplot(juvCommPCA, 
main='species',
choices=c(1,2),
display='species',
scaling='species',
#xlim=c(-l,l),
#ylim=c(-l,l),
type='text',
)

## plot the sites
l=5
biplot(juvCommPCA, 
main='sites',
choices=c(1,2),
display='sites',
scaling='sites',
#xlim=c(-l,l),
#ylim=c(-l,l),
type=c('text','points'),
col=cgr,
)
## coloring doesn't work

## I guess one way to make a nice graph is to call up the empty:

biplot(juvCommPCA, 
main='sites',
choices=c(1,2),
display='sites',
scaling=3, ## symmetric for sites and species
type=c('none'),
)
## then plot the points:
points(juvCommPCA, 
display='sites',
col=cgr+4,
scaling=3,
)
## text can be added:
text(juvCommPCA, 
display='sites',
col=1,
scaling=3,
)


## if we want to see the species driving the driving the differences?
l=5
biplot(juvCommPCA, 
choices=c(1,2),
display=c('species','sites'),
scaling='sites',
#xlim=c(-l,l),
#ylim=c(-l,l),
type=c('text','points'),
col=c('black','red'),
)

## how can we pick out the species with the greatest impacts?

## 
par(mfrow=c(1,1))
l=5
biplot(juvCommPCA, 
choices=c(1,2),
display=c('sites'),
scaling='sites',
#xlim=c(-l,l),
#ylim=c(-l,l),
type=c('text'),
)

## it looks to me like site 7.3 is what makes the first pca so important. 
## can we rerun, without this?


## to get the loadings, I guess vegan calls these "scores", 
## as long as they are unscaled. 
## for the species

scores(juvCommPCA, choices = 1:4, display = "species", scaling = 0) 

scores(juvCommPCA, choices = 1, display = "species", scaling = 0) 

## what do we want to do with this?

juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')
juvComm.no7.3 <- juvComm[rownames(juvComm) !="7.3",]
## transform the abundances
juvComm73.hell <- decostand(juvComm.no7.3, 'hellinger')
bb <- rda(juvComm73.hell, scale=TRUE)

summary(bb)


par(mfrow=c(1,2))
l=5
biplot(bb, 
choices=c(1,2),
display=c('sites'),
scaling='sites',
#xlim=c(-l,l),
#ylim=c(-l,l),
type=c('text'),
)

par(mfrow=c(1,2))
biplot(juvCommPCA, 
main='juvCommPCA, sites',
choices=c(1,2),
display='sites',
scaling=3, ## symmetric for sites and species
type=c('none'),
)
## then plot the points:
points(juvCommPCA, 
display='sites',
col=cgr+4,
scaling=3,
)
text(juvCommPCA, 
display='sites',
col=cgr+4,
scaling=3,
)
biplot(bb, 
main='bb, sites',
choices=c(1,2),
display='sites',
scaling=3, ## symmetric for sites and species
type=c('none'),
)
## then plot the points:
points(bb, 
display='sites',
col=cgr+4,
scaling=3,
)
text(bb, 
display='sites',
col=cgr+4,
scaling=3,
)

## yup, it looks like the first PCA in our original PCA ord is 
## used to explain the differences between 7.3 and the other 
## points. 

## don't know why 7.3 is considered so different by the PCA, 
## in our other clusters/ords it is considered pretty normal 
## foresty plot, doesn't stick out. 
## so, going back to the original PCA, maybe the best PCs to 
## use are 2 and 3. They explain almost as much variance as 
## the first. 

biplot(juvCommPCA, 
    choices=c(2,3),
    display='sites',
    scaling=3,
    type='none',
)
text(juvCommPCA, 
    choices=c(2,3),
    display='sites',
    col=cgr+4,
    scaling=3,
)
legend(
    x='bottomright',
    legend=1:3,
    fill=1:3+4,
)

## PCA3 seems mostly there to explain the differences among
## group 6. But it is nice to have a second axis.

## so now what? 

## we can look at the loadings of PCA2, see what species are 
## important, and plot just these. These can be argued to 
## be the species

## we should also see if these line up with any of the indicator
## species 

juvPC2 <- as.vector(scores(juvCommPCA, choices = 2, display = "species", scaling = 0))
names(juvPC2) <- rownames(scores(juvCommPCA, choices = 2, display = "species", scaling = 0))


sum(juvPC2)

summary(juvCommPCA)

## which species had the greatest effect on this axis? in absolute terms

names(sort(abs(juvPC2)))[1:10]

## does this line up with our indicator species?

load('juvClustIndSpp.rda')

juvClustIndSpp

str(juvClustIndSpp)

summary(juvClustIndSpp)

jvclic <- juvClustIndSpp$sign[juvClustIndSpp$sign$p.value <= 0.05,]

rownames(jvclic) %in% names(juvPC2)

juvPC2[rownames(jvclic)]

jvclic

names(sort(abs(juvPC2)))[1:10]

## totally different results...fuck...

head(sort(juvPC2))
tail(sort(juvPC2))

## huh. Lots to think about here. For one, it looks like at least
## some of these "juvenile" species are actually just understory 
## plants, like the palm Hyospathe. 

## so part of the story is just we have a complex canopy in the 
## less disturbed forest. These differences are not necessarily a 
## result of the next generation of canopy trees being different,
## but perhaps a result of the understory being simpler. In the 
## regeneration sites

#####################################################
## side note, these data were requested from TRY:
# 343 Plant life form (Raunkiaer life form)
# 42  Plant growth form
# 3401 Plant growth form detailed consolidated
# 3400 Plant growth form simple consolidated
# 197 Plant functional type (PFT)
#####################################################

## so, what do we here? Seems like we might put this on hold
## till we can get some sort of way of categorizing the growth 
## form of the plants. Until we have this info, not sure
## if I can say whether we are mostly looking at little trees
## or just understory plants.
## or both (most likely)

## the major directions left are the general biodiversity patterns,
## the endemicity

## I would also like to build a model for predicting the forest 
## type from environmental variables, some sort of logit 

## we need to focus on something, so what next, while we 
## wait for data from the try folks. 

############## biodiversity metrics  ###################

## okay, can we get a simpsons index for each subparcel? 
## do this for the full adult community matrix:
## our community matrix...

library(vegan)

comMsimps <- diversity(comM, index='simpson', MARGIN=1)

comM <- read.csv('subParcelComm.csv', row.names='PsubP')
envOnly <- read.csv('envOnly.csv', row.names='PsubP')

comM[1:3,1:3]

## export this:
write.csv(comMsimps, file="comMsimps.csv")
## okay, do hierarchical model in python

## how can we get a mean species richness for each 
## type of land use?

## species richness is basically just rowsums:

rowSums(comM)

all(names(rowSums(comM)) == rownames(envOnly))
all(names(comMsimps) == rownames(envOnly))

aa <- data.frame(envOnly$habitat, rowSums(comM), comMsimps)
colnames(aa) <- c('hab', 'nuSpp', 'simps')

head(aa)

## ugh, have to use hadley packages. I refuse. Goto python:

habSpRich <- aa 
#write.csv(habSpRich, file='habSpRich.csv')

## python

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
import statsmodels.stats.multicomp as mc

help(pd.read_csv)

aa = pd.read_csv('habSpRich.csv', index_col=0)
bb = aa.groupby('hab')
cc = bb.agg(np.mean)
cc.columns = ['meanSppRich','simps']
dd = bb.agg(np.std)
dd.columns = ['StdNuSpp','StdSimps']
ee = pd.merge(cc,dd, left_index=True, right_index=True)
ff = ee.drop(['simps','StdSimps'], axis=1)

## okay, what do we actually want to say here?

## nice graph time. As an example:

import numpy as np
import matplotlib.pyplot as plt


## we want to grouping to be old forest vs. regen vs. RCA

plt.close('all')
fig = plt.figure()
#ax = fig.add_axes([0,0,1,1])
ax = fig.add_subplot(111)
ax.bar(0, ff['meanSppRich']['BC'], yerr=ff['StdNuSpp']['BC'], color = '#d34625', width= 0.1)
ax.bar(0.11, ff['meanSppRich']['BS'], yerr=ff['StdNuSpp']['BS'], color = '#4387b8', width= 0.1)
ax.bar(0.22, ff['meanSppRich']['CLB'], yerr=ff['StdNuSpp']['CLB'], color = '#6aa149', width= 0.1)
ax.bar(0.45, ff['meanSppRich']['RG'], yerr=ff['StdNuSpp']['RG'], color = '#f08620', width= 0.15)
ax.bar(0.61, ff['meanSppRich']['RCA'], yerr=ff['StdNuSpp']['RCA'], color = '#8e68a4', width= 0.15)

## so what are we trying to do here?

## just so the two 

## anova - we need vectors of species abundances for each type of habitat...

RGrich = aa[aa['hab'] == 'CLB']['nuSpp'].copy()
RCArich = aa[aa['hab'] == 'RCA']['nuSpp'].copy()
CLBrich = aa[aa['hab'] == 'CLB']['nuSpp'].copy()
BSrich = aa[aa['hab'] == 'BS']['nuSpp'].copy()
BCrich = aa[aa['hab'] == 'BC']['nuSpp'].copy()

help(stats.f_oneway)

stats.f_oneway(RGrich, RCArich, CLBrich, BSrich, BCrich)
## but an ANOVA may not be appropriate. Count data, etc.
## should be the same as:


stats.kruskal(RGrich, RCArich, CLBrich, BSrich, BCrich)
## KruskalResult(statistic=16.719520034283267, pvalue=0.0021911245666973414)

## can we combine the forest results and compare to the RCA plots?
(CLBrich + BSrich + BCrich)
len(CLBrich) + len(BSrich) + len(BCrich)
CLBrich.append([BSrich,BCrich], ignore_index=True)
allFor = CLBrich.append([BSrich,BCrich])
len(allFor)




stats.kruskal(allFor, RCArich)
## KruskalResult(statistic=3.346398305084755, pvalue=0.067352230850900419)

stats.f_oneway(allFor, RCArich)
## F_onewayResult(statistic=3.7513611463166825, pvalue=0.060225863826738973)
## should be the same as:
stats.ttest_ind(allFor, RCArich, equal_var=False)
## but isn't. The anova is sig, the ttest is not. fuck...

## very different, now do the tukeys?

## looks hard in python

mc.pairwise_tukeyhsd

help(mc.pairwise_tukeyhsd)

## perform multiple pairwise comparison (Tukey HSD)

m_comp = mc.pairwise_tukeyhsd(endog=aa['nuSpp'], groups=aa['hab'], alpha=0.05)
print(m_comp)

help(stats.ttest_ind)

stats.ttest_ind(BCrich, RCArich, equal_var=False)

stats.ttest_ind(BSrich, RCArich, equal_var=False)

## yeah, not sure what we can say here.   

## maybe make a nice plot, without the anova...can't 
## spend too much time here. 

## spending too much time on this right now. just summarize what we 
## have and move on.

## so what do we need? an estimate of the species 

## we need the NMS graphics, the overall turnover diagram, and the SAD

## plan - 
## 1) combine the three NMSes into one readable graphic
## 2) make nice general turnover diagram
## 3) make nice overall SAD, with chao estimators, etc
## write in text around them
## make it matter to the case in discussion  

#### so back into R ####

## 1) combine the three NMSes into one readable graphic

## this will be a bitch. redo the original three graphics, then tweak with inkscape into one graphic.

## nms #1 is our "habitat" NMS - I think we saved the NMS somewhere:

library(vegan)
library(RColorBrewer)

load('fullCommNMS.rda')


plot(nms$points)

## looks right...let's back up a little:

specObs <- read.csv('specObs.csv')
aa = specObs[c("PsubP","habitat")]
colrs <- brewer.pal(5, "Set1")
nondups <- aa[!duplicated(aa),]
nondups <- nondups[order(nondups$PsubP),]
rownames(nondups) <- NULL
nondups$colorNu <- as.numeric(nondups$habitat)
nondups$colrs <- colrs[nondups$colorNu]
nondups$MDS1 <- nms$points[,'MDS1']
nondups$MDS2 <- nms$points[,'MDS2']
nmsInfo <- nondups

#svg(filename='/home/daniel/Documents/LosCed/courtScientificSummaries/mariscal/habitatNMS.svg')
plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$colrs,
    pch=19,
    cex=2.0,
    xlab='',
    ylab='',
)
#text(nmsInfo$MDS1, nmsInfo$MDS2, nmsInfo$PsubP, cex=1.5)
## add a legend
legend( x='bottomleft',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        )
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)
dev.off()

## nms #2 is our "natural cluster" NMS:

cGroup = read.csv('cGroup.csv', row.names='PsubP')

all(nmsInfo$PsubP == row.names(cGroup))

nmsInfo$cGroup <- cGroup$gr 

#svg(filename='/home/daniel/Documents/LosCed/courtScientificSummaries/mariscal/natClustNMS.svg')
plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$cGroup,
    pch=19,
    cex=2.0,
    xlab='',
    ylab='',
)
legend( x='bottomleft',
        legend = 1:4,
        fill = 1:4,
        cex = 1.0,
        title='Cluster',
        )
ordihull(nms, nmsInfo$cGroup, col=1:4, lwd=3)
dev.off()

## and #3 is our combined natural cluster and habitat NMS:

svg(filename='/home/daniel/Documents/LosCed/courtScientificSummaries/mariscal/natClustAndHabNMS.svg')
plot(nmsInfo$MDS1, nmsInfo$MDS2,
    col=nmsInfo$cGroup,
    pch=19,
    cex=2.0,
    xlab='',
    ylab='',
)
#text(nmsInfo$MDS1, nmsInfo$MDS2 - 0.015, nmsInfo$PsubP, cex=1.5)
## add a legend
legend( x='bottomleft',
        legend = 1:4,
        fill = 1:4,
        cex = 1.0,
        title='Cluster'
        )
legend( x='bottomright',
        legend = c('BC', 'BS', 'CLB', 'RCA', 'RG'),
        #fill = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        col = c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'),
        cex = 1.0,
        lty = 1,
        lwd = 3,
        )
ordihull(nms, nmsInfo$habitat, col=c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00'), lwd=3)
dev.off()

## okay, so now I

## 2) make nice general turnover diagram

## do this in python

specObs = pd.read_csv('specObs.csv')
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
pts = gpd.read_file('GIS/ana30meterPlots.geojson')
pts.set_index('PsubP', inplace=True)
pts.sort_index(inplace=True)
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
## sanity check
(pts.index == envOnly.index).all()
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

fig, ax = plt.subplots()
ax.scatter(physDist, bcDist)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_xlabel('distance between plots (m)')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_title('Tree community turnover')

## save this as 
plt.savefig('lcTreeTurnover.svg')

####### get nice species accumulation curves ##############

## now a species accumulation curve

## back in R:

library(vegan)

comM <- read.csv('subParcelComm.csv', row.names='PsubP')

aa <- specaccum(comM, method = "exact")
anaSAC <- data.frame(aa$richness, aa$sd)
colnames(anaSAC) <- c('richness', 'sd')

#pdf(file="anaSubparcelSAC.pdf")
plot(aa)
#dev.off()

anaSpeciesEstimators = specpool(comM)

################################################################################
#    Species     chao  chao.se    jack1 jack1.se    jack2     boot  boot.se  n
#All     343 516.3946 39.15056 483.6557 21.60728 566.8038 404.4403 10.54889 61
###############################################################################

## can we build a model to stretch this out the predicted 
## number of species?

mod1 <- fitspecaccum(aa, "lomolino")
anaPredYs <- predict(mod1, newdata=1:500)

## just checking, does our estimate of ~60 spp in a 
## half hectare from Danilo's work line up with our 
## general model of SAC here from Ana's plots?
## each plot represents 0.09 hectare sampled... so we need
## 0.5 ha * ( 1 plot / 0.09 ha ) = 

0.5 / 0.09 ## = 5.5556. 

predict(mod1, newdata=(0.5/0.09)) ## 96 spp

## huh, we get a much higher prediction from Ana's overall
## model than we observe with Danilo's data. 

## how about a simple 1 ha estimate?

predict(mod1, newdata=(1.0/0.09)) ## 147 spp

## interesting. So I guess the permanent plot isn't 
## necessarily the most representative spot as far 
## as alpha diversity?

#write.csv(predYs, file="predYs.csv", row.names=FALSE)

write.csv(anaSAC, file='anaSAC.csv')
write.csv(anaPredYs, file='anaPredYs.csv')
write.csv(anaSpeciesEstimators, file='anaSpeciesEstimators.csv')

## good stuff. To port it over to pyplotter...


## look at these in pandas/matplotlib
anaSAC = pd.read_csv('anaSAC.csv', index_col=0)
anaPredYs = pd.read_csv('anaPredYs.csv', index_col=0).x
anaSpeciesEstimators = pd.read_csv('anaSpeciesEstimators.csv', index_col=0).loc['All']

anaSAC

anaPredYs

anaSpeciesEstimators

## can we make a nice plot with this?

plt.close('all')
colr='b'
fig, ax = plt.subplots(1,1)
ax.plot(anaSAC['richness'],
            label='observed accumulation',
            color=colr)
ax.fill_between(x=anaSAC.index.to_numpy(),
                 y1=anaSAC.richness - anaSAC.sd,
                 y2=anaSAC.richness + anaSAC.sd,
                alpha=0.4,
                color=colr,
                )
ax.plot(anaPredYs.index.to_numpy(), anaPredYs, 
            color=colr, 
            linestyle=':',
            label='SAC predicted values',
            )
## put the chao estimator on there:
ax.hlines(anaSpeciesEstimators.chao,
    xmin = 0,
    xmax = 500,
    linestyle = "--",
    color = "r",
    label='Chao estimate',
)
ax.fill_between(x=anaPredYs.index,
                 y1=anaSpeciesEstimators.chao - anaSpeciesEstimators["chao.se"],
                 y2=anaSpeciesEstimators.chao + anaSpeciesEstimators["chao.se"],
                alpha=0.2,
                color='r',
                )
## each plot represents 0.09 hectare sampled
## if we want ticks for every 20 hectares sampled
## every 5 ha? 5 ha = 55.5 plots
tk = np.arange(0,6)*55.5
ax.set_xticks(tk)
## the ha values of this would be:
tklabs = np.arange(0,6)*5
tklabs[0] = 1
ax.set_xticklabels(tklabs)
ax.set_xlabel('Parceles medidas en hectareas (ha)')
ax.set_ylabel('número de especies de arboles')
ax.set_xlim(0,300)
ax.legend(loc='lower right')

## great. update notebook with this and the Danilo data below 

#plt.savefig('anaSAC.svg')


## just checking, we have an area surveyed of about 3200 x 3200 m, ~ 1000 ha area surveyed. 





head(specObs)

### check out this for watershed analysis:
https://github.com/mdbartos/pysheds

## hierarchical models
## develop watershed models 


############################ try database ################################

## what do we do now? 

## check out the try data, see if we can figure out what are juvenile 
## and what are tree data

## our copy of the try database is here:
/home/daniel/Documents/analyses/tryData/12154.txt

## how do tell if a plant is a tree or a shrub?

## get our data: 
specObs = pd.read_csv('specObs.csv')

## we're not going to read the massive try database into memory yet, 
## so grep it on the command line:

## for instance, 
grep "Ossaea micrantha" 12154.txt 

## nothing, but:
grep "Ossaea" 12154.txt -n

sed -n "14004161, 14004162p" 12154.txt

## this is taking forever... 

tail -n 1000 12154.txt > tailTest.tsv
## nothing in this


#### build our database from TRY data ####

## it looks to me like this is what we need to find our info:
head -n 1 12154.txt > justGrowthForm.tsv
grep "Plant growth form" 12154.txt >> justGrowthForm.tsv

head justGrowthForm.tsv
tail justGrowthForm.tsv
wc -l justGrowthForm.tsv
## looks right

## make a test file
head 12154.txt -n 1 > test.tsv
grep "Faramea platyneura" justGrowthForm.tsv  >> test.tsv
tail justGrowthForm.tsv >> test.tsv
## sometimes grep seems to cause weird "--" lines to be entered...
grep "\-\-" justGrowthForm.tsv
grep "\-\-" test.tsv
## don't find them, ok onward
## I think we need the following columns:
## 3. Dataset
## 5. SpeciesName
## 7. AccSpeciesName
## 9. ObsDataID
## 11. TraitName
## 15. OrigValueStr

## to cut down to these, first add back in the old headers
cut -f 3,5,7,9,11,15 test.tsv > test2.tsv

## looks okay, do this with our justGrowthForm file:
cut -f 3,5,7,9,11,15 justGrowthForm.tsv > justGrowthForm2.tsv

## there are some strange spots, missing the growth form field:
grep "Plant growth form" -v justGrowthForm2.tsv | wc -l

## so keep only those with growth form?
head -n 1 justGrowthForm2.tsv > justGrowthForm3.tsv
grep "Plant growth form" justGrowthForm2.tsv >> justGrowthForm3.tsv


## some just say "no" for growth form. 
grep "no" -w justGrowthForm3.tsv  | wc -l
grep "No" -w justGrowthForm3.tsv | wc -l

grep "[Nn]o" -wv justGrowthForm3.tsv 

## Get rid of these:
head -n 1 justGrowthForm3.tsv > justGrowthForm4.tsv
grep "[Nn]o" -wv justGrowthForm3.tsv >> justGrowthForm4.tsv


## checks
wc -l justGrowthForm4.tsv

(libreoffice --calc justGrowthForm4.tsv &) &

## I think this file works. simplify name and clear out. 
mv justGrowthForm4.tsv justGrowthForm.tsv

(libreoffice --calc justGrowthForm.tsv &) &

## so back into python. 

## can we load this in and search for our species of interest?

juvSpecObs = pd.read_csv('juvSpecObs.csv')

juvSpecObs.head()

juvSpecObs.genero.values

juvSpecObs['genero'][2]

'Meriania' in juvSpecObs['genero'].values

'Hyospathe' in juvSpecObs['genero'].values

'Hyospathe' in specObs['genero'].values


"Bartsia" in juvSpecObs['genero'].values

## no habit information, either in that file or in the original spreadsheets 
## from Ana

## so the goal here would be to scan our data from TRY for matches in 
## genus and maybe species names 

## sp. is causing problems here...
aa = ~(juvSpecObs['genero'].str.contains('sp\.'))
juvGenera = juvSpecObs['genero'][aa]
juvGenera.reset_index(drop=True, inplace=True)

## now look for every example of each genus we have that 
## is present in the TRY data:
tryData = 'justGrowthForm.tsv' 
with open('foundGenera.txt', 'w') as w:
    for genus in juvGenera:
        with open(tryData, 'r', encoding='latin_1') as f:
            for line in f:
                if re.search(genus, line): 
                    w.write(line)


(libreoffice --calc foundGenera.txt &) &

## looks good. now what?

## step 1 is remove all genera that were found in the adult 
## tree data set, we assume that these are trees. 
## step 2 is to identify all of the remaining as either 
## likely trees or sub-canopy plants
## our analyses can then treat these separately. 

## step 1 remove the species that are in the adult dataset:

## get our data: 
specObs = pd.read_csv('specObs.csv')
juvSpecObs = pd.read_csv('juvSpecObs.csv')

aa = juvSpecObs['genusSpecies'].isin(specObs['genusSpecies'])



## subset to these in the data frame:

bb = juvSpecObs.loc[~aa,:]
## get just the genera for these

bb.genero.unique()

len(bb.genero.unique())

juvUniq = juvSpecObs['genusSpecies'][~aa].copy().unique()

len(juvUniq) ## 67 species


## that means we have 67 species to look at

## some odd things going on here - why is Saccoloma in there? A little fern...

## also palicourea sodiroi...

## notopleura

## just a lot of things that are not juvenile, just small. 

## yeah, a lot of weirdness. 

## how do we deal with this? 

## let's focus the try data on this. We can accept all speciees 
## that are in both the adult and juvenile data, and focus our search 
## on just this set of differences from the  

## start over:
specObs = pd.read_csv('specObs.csv')
juvSpecObs = pd.read_csv('juvSpecObs.csv')
aa = juvSpecObs['genusSpecies'].isin(specObs['genusSpecies'])
juvGen = juvSpecObs.loc[~aa,"genero"]
notSp = ~juvGen.str.contains("sp\.") ## get rid of sp.
juvGen = juvGen[notSp]
juvGenUniq = juvGen.unique()

tryData = 'justGrowthForm.tsv' 
with open('foundGenera.txt', 'w') as w:
    for genus in juvGenUniq:
        with open(tryData, 'r', encoding='latin_1') as f:
            for line in f:
                if re.search(genus, line): 
                    w.write(line)

## works. what now?

## manual curation - throw out everything that probably isn't a tree,
## or at least, won't grow large enough to be considered in our 
## mature plant analysis (10 cm diameter at dbh?)

## then rerun the seedling analysis.

## a lot of data comes from this source:

## check out the sources at:
https://www.try-db.org/de/DatasetDetails.php 

## so go through these. Can we recover the full family/genus/spp?

aa = juvSpecObs['genusSpecies'].isin(specObs['genusSpecies'])
juvGenSp = juvSpecObs.loc[~aa,"genusSpecies"]
juvGenSpUn = juvGenSp.unique() 

#notSp = ~juvGen.str.contains("sp\.") ## get rid of sp.

for i in juvGenSpUn: print(i)

## go through these one-by-one

### Euphorbiaceae sp. ###

## oh jeez. hard to say. Could be anything. Probably Croton or something
'Croton' in juvSpecObs['genero'].values
'Croton' in specObs['genero'].values
## we didn't observe croton in mature trees

## 56 euphorbiaceae observations in adults:

aa = specObs['genusSpecies'].str.contains('Euphorbiaceae')
specObs[aa]

sum(aa)

## but these are all identified at least to genus 
## think we gotta throw this out. 


### Actinidaceae sp. ###

## unless there is something really cool here, this is going
## to be Saurauia. The only Saurauias I know are small trees,
## but that's not saying much. In the full TryData we have 
## trees and shurbs:

grep "Saurauia" justGrowthForm.tsv

## perhaps just as important to note, these are usually pioneer
## type species, I think. I would wager that big ecological change is 
## happening at a site if they are present. And most importantly, 
## they are really understory species, I think. The point here is 
## to eliminate species whose ecological niche is to to persist
## in the understory in more stable forest seres, i.e. species that 
## would have been missed by the mature tree survey.  

## So keep Actinidaceae sp.

### Moraceae sp.1 ###

## oh jeez. again, really hard to know anything from this
## not sure if we should err on the side of exclusion or 
## inclusion here...
## I think we need to be exclusive. We are trying to reduce
## the amount of understory plants that are being tracked. 

## Rubiaceae Notopleura anomothyrsa ##

## this is probably the same notopleura we see path-side 
## if so, shold be thrown out. 

## also, no other notopleuras are seen in the adult data set
aa = specObs['genero'].str.contains('Notopleura')
sum(aa)

aa = juvSpecObs['genero'].str.contains('Notopleura')
sum(aa)
juvSpecObs[aa]
## just two observations, both of anomothyrsa
## throw it out.

### Rubiaceae Psychotria pilosa ###

## psychotria, same deal? mostly shrubs

grep "Psychotria pilosa" foundGenera.txt

## that species is not in there. Are there psychotrias 
## in the adult data:

specObs['genero'].str.contains('Psychotria').any()

aa = specObs['genero'].str.contains('Psychotria')
specObs[aa] ## yup

aa = specObs['genero'].str.contains('Psychotria')

## general agreement that psychotria are shrubs, 
## in tryData and Gentry, and sometimes trees. 
## given this and that the adult data has a lot of 
## this genus, let's keep it. 


### Capparaceae Capparis macrophylla ###

## this species is specifically noted in the tryData,
## as a tree, and the genus as small trees/shrubs by gentry. 

specObs['genero'].str.contains('Capparis').any()

## and we have small trees/trees listed in the adult data:
aa = specObs['genero'].str.contains('Capparis')
specObs[aa] ## yup

## any other species of capparis in juv data?
bb = juvSpecObs['genero'].str.contains('Capparis')
juvSpecObs[bb]
## nope

### Tiliaceae sp. ###

## tiliaceae is almost entirely trees, according to gentry,
## with a few "weedy" shrubs.  

specObs['genusSpecies'].str.contains('Tiliaceae').any()

bb = juvSpecObs['genusSpecies'].str.contains('Tiliaceae')
juvSpecObs[bb]

## these are found in the anthropogenic disturbance plots,
## fits Gentry's note that tiliaceae tend to be in second
## growth, so it probably both a tree and an ecologically
## informative species. 

## keep it

### Urticaceae sp. ###

## urts can be anything. no fucking way. Gotta throw it out. 

### Euphorbiaceae Acalypha aff. diversifolia ###

## definitely observed as a tree in the try data  
## or at least "T", checking now to make sure 

grep "Acalypha diversifolia" foundGenera.txt

## but Gentry says the vast majority are weedy shrubs/herbs
## STRI has a species description site
## https://stricollections.org/portal/taxa/index.php?taxon=Acalypha+diversifolia&formsubmit=Search+Terms

## that is a cool website: stricollections.org

## calls it a tree up to 6m high. I think we gotta keep that. 

### Rubiaceae Palicourea aff. chignul ###

## Palicourea chignul 

## pretty sure palicourea are well covered in the mature data:

aa = specObs['genero'].str.contains('Palicourea')
specObs[aa]

bb = juvSpecObs['genero'].str.contains('Palicourea')
juvSpecObs[bb]

## but there are some weird palicourea out there. P. sodiroi
## is not a tree, for instance

## every source says small trees. keep it.

### Urticaceae Phenax aff. rugosus ###

## not covered by try database. 
## not in Gentry

## definitely not an unknown genus or even species, 
## but can't find any growth form information.

## not sure how to judge this one. There are some pictures... 
## the plants in the pictures look like small shrubs, but 
## hard to tell. 

## EOL lists it as a tree (https://eol.org/pages/5724960)
## I guess this means keep it

### Sapindaceae Paullinia capreolata ###

aa = specObs['genero'].str.contains('Paullinia')
specObs[aa]

## no Paullinias in our adult data

bb = juvSpecObs['genero'].str.contains('Paullinia')
juvSpecObs[bb]

## the species in our juv data are not in the TRY data,
## the vast majority of the other examples are climbers/
## lianas, and Gentry only mentions lianas. 

grep "Paullinia capreolata" foundGenera.txt

grep "Paullinia nobilis" foundGenera.txt

grep "Paullinia" foundGenera.txt

## so gotta throw these out...

### Siparumaceae Siparuna sp. ###
### Siparumaceae Siparuna laurifolia ###

## concerning the genus:
aa = specObs['genero'].str.contains('Siparuna')
specObs[aa]
## yup other Siparunas in there

bb = juvSpecObs['genero'].str.contains('Siparuna')
juvSpecObs[bb]

## as for S. laurifolia...

## EOL has data for this species, says tree (https://eol.org/pages/5402878)
## for the genus, the TRY data overwhelmingly votes tree, with a few shrub/tree entries

## given all that let's keep the Siparunas, and remember to fix this typo before rerunning
## the analyses:

### Siparumaceae Siparuna salviodes ###

## in the juvenile data, this is a typo:
## Siparumaceae Siparuna salviodes
## should be:
## Siparumaceae Siparuna salvioides
## so this one is actually considered in the mature data.
## this is a pain, but will need to be fixed. 

### Asteraceae Neurolaena cf. lobata ###

aa = specObs['genero'].str.contains('Neurolaena')
specObs[aa]
##nada

bb = juvSpecObs['genero'].str.contains('Neurolaena')
juvSpecObs[bb]

## gentry lists this as a weedy herb?
## TRY database lists this once, as a shrub
## this species is "cf. lobata". N. lobata is an herb in the EOF database
## https://eol.org/pages/503028

## we gotta throw this one out.

### Thymelaeaceae Shoenobiblus cf. daphnoides ###

aa = specObs['genero'].str.contains('Shoenobiblus')
specObs[aa]
## nada

grep "Shoenobiblus" foundGenera.txt
## nada

bb = juvSpecObs['genero'].str.contains('Shoenobiblus')
juvSpecObs[bb]

## "Shoenobiblus" not in Gentry
## "Schoenobiblus" is...another typo?

aa = specObs['genero'].str.contains('Schoenobiblus')
specObs[aa] ## yup, its there...
## so yeah, typo...

## don't think we need to correct this? it is the only 
## Schoenobiblus in the juvenile set, and it is doesn't 
## quite match the species in the adult set, though...

bb = juvSpecObs['genero'].str.contains('Schoenobiblus')
juvSpecObs[bb] ## nada

## interesting, they call this a "cf." of the same species
## of the adult matches:

## adults:
"Thymelaeaceae Schoenobiblus daphnoides" 
## vs.
## juveniles:
"Thymelaeaceae Shoenobiblus cf. daphnoides"

## if we had infinite time and resources, we should do some
## barcoding, see if they are the same. But we don't. 

## all the usual sources show this genus as a tree genus:
grep "Schoenobiblus" justGrowthForm.tsv
https://eol.org/pages/5470335
## + gentry

## maybe remember to correct the name when correcting the 
## above typo for Siparuna salvioides

## So keep this species, and regard it as something other 
## than the adult species. 

### Boraginaceae sp. ###

## um, what the hell do we do with this? Borages can be 
## anything. 

aa = specObs['genusSpecies'].str.contains('Boraginaceae')
specObs[aa]

## If they weren't able to call it as a Cordia sp., not sure 
## what it could be, Cordia seems to be the main tree genus..

## I think we gotta throw this out. There is just no information.

### Rubiaceae Psychotria cf. stipularis ###

## I think we just decided above we are keeping all 
## Psychotrias...

## this may have been renamed to Palicourea stipularis...
## which EOL says is a woody climber:

https://eol.org/pages/1108085

## I guess err on the side of caution and throw this out.

### Myrsinaceae Cybianthus humilis ###

## this genus is mostly trees in the TRY data, though
## no exact species match.

## no help from EOL on growth form 

aa = specObs['genusSpecies'].str.contains('Cybianthus')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Cybianthus')
juvSpecObs[bb]

## all this species

## gentry says small trees, shrubs, subcanopy. But big enough
## to be noticed in our mature data? Not sure.
## not a lot of  information. I guess keep it. 

### Solanaceae sp. ###

## no fucking way. this could be anything. 

aa = specObs['genusSpecies'].str.contains('Solanaceae')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Solanaceae')
juvSpecObs[bb]

## gotta ditch it

### Rubiaceae Agouticarpa grandistipula ###

aa = specObs['genusSpecies'].str.contains('Agouticarpa')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Agouticarpa')
juvSpecObs[bb]

## TRY has a different species, a tree or small tree:
grep "Agouticarpa" justGrowthForm.tsv
## EOL lists this species as a tree <https://eol.org/pages/1097921>

## let's keep it.

### Indeterminado sp.22 ###
### Indeterminado sp.21 ###

## i think we have ditch these. Not much we can do with them. 

### Arecaceae Aiphanes erinaceae ###

aa = specObs['genusSpecies'].str.contains('Aiphanes')
specObs[aa]
## no adults

bb = juvSpecObs['genusSpecies'].str.contains('Aiphanes')
juvSpecObs[bb]

## based on this:
Svenning, Jens-Christian, and Henrik Balslev. "The palm flora of the Maquipucuna montane forest reserve, Ecuador." Principes 42 (1998): 218-226.
## I think we gotta exclude this. This is a small palm, not known 
## to exceed 5 cm, at least if I'm reading this article right. 


### Lauraceae Ocotea aff. insularis ###

aa = specObs['genusSpecies'].str.contains('Ocotea')
specObs[aa]

specObs[aa].shape ## lots of hits, 77

specObs[aa].head() ## lots of hits, 77

specObs.loc[aa, 'genusSpecies'].unique()

len(specObs.loc[aa, 'genusSpecies'].unique()) ## eleven Ocoteas in the mature set. 

bb = juvSpecObs['genusSpecies'].str.contains('Ocotea')
juvSpecObs[bb]

## so basically, this is something that looks really similar to 
## some of our adults, but something made them think it was
## different. Jeez. Well, keep it I guess. 

### Asteraceae Criptoniosis cf. occidentalis ###

aa = specObs['genusSpecies'].str.contains('Criptoniosis')
specObs[aa] 
## none

bb = juvSpecObs['genusSpecies'].str.contains('Criptoniosis')
juvSpecObs[bb]
## one site

## I'm betting this is also a typo, and they meant:
"Critoniopsis cf. occidentalis"

## so check that again...
aa = specObs['genusSpecies'].str.contains('Critoniopsis')
specObs[aa] 
##  lots of the C. occidentalis, in several types of plots 

bb = juvSpecObs['genusSpecies'].str.contains('Critoniopsis')
juvSpecObs[bb]

## so keep it, correct typo. 

### Myrtaceae Myrcia splendens ###

## definitely keep this one. Well documented tree

### Melastomataceae sp.4 ###

## throw it out. 

### Myrsinaceae Ardisia cf. colombiana ###

aa = specObs['genusSpecies'].str.contains('Ardisia')
specObs[aa] 
##  none

bb = juvSpecObs['genusSpecies'].str.contains('Ardisia')
juvSpecObs[bb]
## the species they are comparing to, A. colombiana, is
## a pretty tall tree. 
## <https://stricollections.org/portal/taxa/index.php?taxon=Ardisia+colombiana&formsubmit=Search+Terms>

## keep it. 

### Meliaceae Ruagea cf. pubescens ###

aa = specObs['genusSpecies'].str.contains('Ruagea')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Ruagea')
juvSpecObs[bb]

## just keep all the Ruagea species. They are well represented
## in the adult data

### Arecaceae sp. ###

## jesus, what do I do with an unknown little palm?

## throw it out. 

### Fab-faboideae sp.1 ###

## jeezus, I am tired of babysitting this data
## I have to throw this out. 

### Lauraceae Rhodostemonodaphne aff. cyclops ###

aa = specObs['genusSpecies'].str.contains('Rhodostemonodaphne')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Rhodostemonodaphne')
juvSpecObs[bb]

## keep it.

### Lauraceae Endlicheria sp. ###

aa = specObs['genusSpecies'].str.contains('Endlicheria')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Endlicheria')
juvSpecObs[bb]

## keep it

### Piperaceae Piper cf. imperale ###

## this is probably a typo.

"Piperaceae Piper cf. imperale"
## is probably:
"Piperaceae Piper cf. imperiale"

## fix typo
## once we do this, this species is no longer unique to the juvs

aa = specObs['genusSpecies'].str.contains('imperiale')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Endlicheria')
juvSpecObs[bb]


### Indeterminado sp.20 ###

## throw out.

### Malvaceae sp. ###

## throw out. 

### Melastomataceae Topobea dodsonorum ###

aa = specObs['genusSpecies'].str.contains('Topobea')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Endlicheria')
juvSpecObs[bb]

### Moraceae sp.2 ###

## yeah, generally, gotta throw out anything not identified 
## to genus

### Melastomataceae Conostegia cuatrecasii ###

aa = specObs['genusSpecies'].str.contains('Conostegia')
specObs[aa] 

## lots of this genus in the adult dataset
## mostly trees in the TRY database

## keep it


### Myrtaceae Eugenia sp. ###

## Eugenias are trees. Keep. 

### Sapindaceae Paullinia cf. capreolata ###

## throw out, see above Paullinia

### Rubiaceae Palicourea sp. ###

## Palicourea are generally trees, with a few
## exceptions, as climbers on other trees. 
## Keep it.

### Menispermaceae Abuta aff. chocoensis ###

## EOL calls this a liana
## Gentry says this genus almost entirely scandent in NW SA, 
## maybe a few treelets

aa = specObs['genusSpecies'].str.contains('Abuta')
specObs[aa] 
## no adults

bb = juvSpecObs['genusSpecies'].str.contains('Abuta')
juvSpecObs[bb] 
## think we have to throw this out. 

### Rubiaceae Faramea aff. calyptrata ###

## farameas are all trees, right?
## EOL, TRY agrees with this. 
## keep it.  

### Rubiaceae Faramea flavicans ###

## See above. Keep. 

### Myrtaceae Eugenia cf. florida ###

## Keeping Eugenias. 

### Olacaceae Heisteria sp. ###

## Gentry says small/medium trees, except for H. scandens
## TRY agrees

aa = specObs['genusSpecies'].str.contains('Heisteria')
specObs[aa] 
## nada

## keep it. 

### Ochnaceae Ouratea aff. weberbaueri ###

aa = specObs['genusSpecies'].str.contains('Ouratea')
specObs[aa] 

## lots of O. weberbaueri in the adult data
## keep

### Clusiaceae Clusia aff. dixonii ###

## Clusias are pretty much trees and hemi-epiphytes,
## and C. dixonii is listed as a Tree in EOL, via Stevens 2007
## keep it.

### Lauraceae Beilschmedia sp. ###

aa = specObs['genusSpecies'].str.contains('Beilschmedia')
specObs[aa] 

bb = juvSpecObs['genusSpecies'].str.contains('Beilschmedia')
juvSpecObs[bb] 

## typo? "Beilschmedia" should be "Beilschmiedia" 

aa = specObs['genusSpecies'].str.contains('Beilschmiedia')
specObs[aa] ## lots

bb = juvSpecObs['genusSpecies'].str.contains('Beilschmiedia')
juvSpecObs[bb] ## misspelled in all juv entries.

## fix typo, and keep

### Araliaceae Schefflera lasiogyne ###

## Schefflera, hmm. Not normally very big...

aa = specObs['genusSpecies'].str.contains('Schefflera')
specObs[aa] ## 

bb = juvSpecObs['genusSpecies'].str.contains('Schefflera')
juvSpecObs[bb] ## 

## EOL calls it a tree. TRY says trees/small trees for entire genus. 
## A little strange that no Schefflera were observed in the 
## the mature data, though. How can that be? Makes me think that
## they were too small. 

aa = specObs['genusSpecies'].str.contains('Araliaceae')
specObs[aa] ## probably not a typo issue, just no Scheffleras. 

## don't like it. such a common genus, I bet they were 
## not included because they were too small. 
## which means we should throw them out here, I think. 

### Myrtaceae Myrcia cf. aliena ###
### Lauraceae Beilschmedia alloiophylla ###

## typo:
## "Beilschmedia" should be "Beilschmiedia" 

## this is solidly a tree, check out the herbarium label:
<https://fm-digital-assets.fieldmuseum.org/983/545/LAUR_beil_allo_1929039.jpg>

## anyway, fixing the typo will make this go away, it is also
## in the adult dataset, properly spelled. 

### Indeterminado sp.26 ###

## gotta throw it out. 

### Rubiaceae Palicourea aff. sodiroi ###

## they call this a tree, but I have only seen it 
## as a climber. There is probably a reason it was 
## only observed in the juv data

## I'm okay keeping hemiepiphytes etc if they are 
## in the adult set, but not if they are small 
## and only appear in the juv data. At least, 
## as I am using this data. 

aa = specObs['genusSpecies'].str.contains('sodiroi')
specObs[aa] ## 

bb = juvSpecObs['genusSpecies'].str.contains('sodiroi')
juvSpecObs[bb] ## 

## so rather than introduce another small species that may
## indicate another small woody species that may cause 
## artificial differences to appear between the adult and 
## juvenile data sets...

## throw these out.
 
### Rubiaceae Palicourea sodiroi ###

## see above

### Indeterminado sp.27 ###

## useless, out it goes. 

### Rubiaceae Isertia pittieri ###

aa = specObs['genusSpecies'].str.contains('Isertia')
specObs[aa] ## 

bb = juvSpecObs['genusSpecies'].str.contains('Isertia')
juvSpecObs[bb] ## 

## sources say trees. there are plenty of adult Isertias.
## keep it.

### Sapindaceae Paullinia cf. nobilis ###

## see Paullinia capreolata above

### Dennstaedtiaceae Saccoloma inaequale ###

## this is a fern. Not a tree fern. What the fuck. 

### Malvaceae Wercklea sp. ###

## not in TRY, but Gentry says small trees,
## STRI shows a large shrub in this genus, Wecklea ferox
## STRI shows a large shrub in this genus...

aa = specObs['genusSpecies'].str.contains('Wercklea')
specObs[aa]  

## we have W. ferox in the mature data...

## keep it.


### Meliaceae Trichilia sp.1 ###

## lots of Trichilia in the mature data:
aa = specObs['genusSpecies'].str.contains('Trichilia')
specObs[aa]  

## keep it

### Lauraceae Ocotea cernua ###

aa = specObs['genusSpecies'].str.contains('Ocotea')
specObs[aa]

aa = specObs['genusSpecies'].str.contains('cernua')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Ocotea')
juvSpecObs[bb]

## lots of Ocotea in our adults, including something that 
## looks a lot like ("aff") Ocotea cernua

## keep it

### Burseraceae sp. ###

## easy. keep it

### Gesneriaceae sp. ###

## Gesner tree? seems unlikely. ditch it.

### typos ###

"Siparuna salviodes"
## to
"Siparuna salvioides"

"Thymelaeaceae Shoenobiblus cf. daphnoides" 
## to 
"Thymelaeaceae Schoenobiblus cf. daphnoides"

"Asteraceae Criptoniosis cf. occidentalis"
## to 
"Asteraceae Critoniopsis occidentalis"

"Piperaceae Piper cf. imperale"
## is probably:
"Piperaceae Piper cf. imperiale"

"Lauraceae Beilschmedia sp."
## to 
"Lauraceae Beilschmiedia sp."

"Lauraceae Beilschmedia alloiophylla"
## to 
"Lauraceae Beilschmiedia alloiophylla"

### throw out list: ###

## Euphorbiaceae sp. 
## Rubiaceae Notopleura anomothyrsa 
## Urticaceae sp.
## Sapindaceae Paullinia capreolata
## Sapindaceae Paullinia cf. nobilis
## Asteraceae Neurolaena cf. lobata
## Boraginaceae sp. ##
## Rubiaceae Psychotria cf. stipularis
## Solanaceae sp.
## Indeterminado sp.22
## Indeterminado sp.21
## Arecaceae Aiphanes erinaceae
## Melastomataceae sp.4 
## Arecaceae sp.
## Fab-faboideae sp.1
## Indeterminado sp.20 
## Malvaceae sp.
## Moraceae sp.2 
## Sapindaceae Paullinia cf. capreolata
## Menispermaceae Abuta aff. chocoensis 
## Araliaceae Schefflera lasiogyne
## Indeterminado sp.26 
## Rubiaceae Palicourea aff. sodiroi
## Rubiaceae Palicourea sodiroi
## Indeterminado sp.27
## Dennstaedtiaceae Saccoloma inaequale
## Gesneriaceae sp.

## and that's it. So correct typos, and remove the above from the 
## juvenile dataframe

## do this way back in the chunk of code where we made juvSpecObs

## and tomorrow, rerun all analyses involving the juvspecObs:

####### rerun juvenile clustering analysis ########

## let's rerun the juv clustering, both with the 
## new, cleaner data and a closer look

library(stats)
library(vegan)
## where is our community matrix of juveniles?
juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')
## checks
rowSums(juvComm)
any(colSums(juvComm) == 0)
all(colSums(juvComm) > 0)

juvCommBray <- vegdist(juvComm)
juvComm.ward <- hclust(juvCommBray, method='ward.D2')

## originally we tried 3 groups, and it seemed like no 
## real signals there. 2 groups seem to make more sense: 

plot(juvComm.ward)
k=2
juvGrp <- rect.hclust(juvComm.ward, border=c('purple','yellow','blue'), k=k)

## nms of juveniles only:
juvNMS <- metaMDS(juvComm)
#save(juvNMS, file='juvNMS.rda')

par(mfrow=c(1,2))
k=2
plot(juvComm.ward)
juvGrp <- rect.hclust(juvComm.ward,
    border=(1:k+4),
    #border=c(5,6,7),  
    k=k)
jvgr <- NULL
for (i in 1:k){
groupNu <- rep(i, length(juvGrp[[i]]))
jvgr <- c(jvgr, groupNu)
}
names(jvgr) <- names(unlist(juvGrp))
## sort these to match the order of our NMS
jvgr <- jvgr[rownames(juvNMS$points)]
plot(juvNMS$points,
    cex=1.5,
    bg=jvgr+4,
    pch=24,
)
text(juvNMS$points[,'MDS1'],
    juvNMS$points[,'MDS2'] - 0.02,
    rownames(juvNMS$points),
    cex=1)
############### stop here thursday

## debug this tomorrow: 

## regenerate our community matrix of sites with adults and juveniles:
par(mfrow=c(1,1))

comboCommMat <- read.csv('comboMat.csv')

dim(comboCommMat)

edit(comboCommMat)

## why are the PsubP values not in the adult?


jovenes <- comboCommMat[comboCommMat$X == 'joven',]

adultosInjovenes <- comboCommMat$PsubP %in% jovenes$PsubP

adultosDejovenes <- comboCommMat[adultosInjovenes,]

comboInfo <- adultosDejovenes[,1:2]
colnames(comboInfo) <- c("ageClass","PsubP")


## our adult cluster results were here:
cGroup <- read.csv('cGroup.csv')
adultGroupMaps <- cGroup$gr
names(adultGroupMaps) <- cGroup$PsubP
## our juvenile cluster results were here:
load('juvSpecClustMap.rda')
PsubP <- as.character(comboInfo[comboInfo$ageClass == 'adulto','PsubP'])

## this can be a column in our dataframe 
comboInfo$Clust <- c(adultGroupMaps[PsubP], juvSpecClustMap[PsubP] + max(adultGroupMaps[PsubP]))
## make our symbols 
symbs <- vector(length = nrow(comboInfo))
for (i in 1:length(symbs)){
    if (comboInfo[i,'ageClass'] == 'adulto'){
        symbs[i] <- 21} else {
            if (comboInfo[i,'ageClass'] == 'joven'){
                symbs[i] <- 24}
}}
comboInfo$symbs <- symbs
## we also want to retain the original parent cluster
## info for the juveniles:
comboInfo$parentClust <- adultGroupMaps[as.character(comboInfo$PsubP)]

## so can we use this to color our NMS graphic?
load('comboNMS.rda')
MDS1 <- comboNMS$points[,'MDS1']
MDS2 <- comboNMS$points[,'MDS2']
plot(MDS1, MDS2,
    bg=comboInfo$Clust,
    pch=comboInfo$symbs,
    col=comboInfo$parentClust,
    lwd=4.0,
    cex=2.0,
)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)
#ordihull(comboNMS, comboInfo$Clust, col=comboInfo$Clust, lwd=3)
ordihull(comboNMS, comboInfo$Clust, col=names(table(comboInfo$Clust)), lwd=3)
legend( x='bottomright',
        legend = 1:7,
        pch = c(rep(21,4), rep(24,3)),
        pt.bg = 1:7,
        ncol = 2,
        cex = 1.5,
        title='Cluster',
        )


## are we sure we updated everything? Are any of the deleted 

grep "Sapindaceae Paullinia capreolata" comboMat.csv
## seems clean

## there are some issues with the combo matrix - why doesn't the adult section
## have proper a PsubP index? back to python...

aa = pd.read_csv("subParcelComm.csv")
## th

bb = pd.read_csv("juvComm.csv")
## this is missing the PsubP index 

## well, that's sort of the opposite of what I expected...
## so to start with, looks like I left off the the PsubP 
## values for the juvenile dataframe...

## does it behave on R:

library(vegan)
library(RColorBrewer)

comboCommMat <- read.csv('comboMat.csv')
load('comboInfo.rda')

jovenes <- comboCommMat[comboCommMat$X == 'joven',]
adultosInjovenes <- comboCommMat$PsubP %in% jovenes$PsubP
adultosDejovenes <- comboCommMat[adultosInjovenes,]
## get rid of outlier point from community matrix
aa <- adultosDejovenes[adultosDejovenes$PsubP != '1.1',]
## get rid of outlier point from nmsInfo 
aaInfo <- comboInfo[comboInfo$PsubP != 1.1,]
all(aa$PsubP == aaInfo$PsubP)
## get rid of label columns
bb <- aa[,c(3:ncol(aa))]
## check row names
all(bb$PsubP == aaInfo$PsubP)
## redo NMS without outlier:
#comboNMS_noOut <- metaMDS(bb)
load('comboNMS_noOut.rda')

MDS1 <- comboNMS_noOut$points[,'MDS1']
MDS2 <- comboNMS_noOut$points[,'MDS2']
## empty plot
plot(MDS1, MDS2,
    bg=aaInfo$Clust,
    pch=aaInfo$symbs,
    col=aaInfo$parentClust,
    type='n',
    lwd=3.0,
    cex=2.0,
)
text(MDS1, MDS2 - 0.015, aaInfo$PsubP, cex=1.5)
ordiellipse(ord=comboNMS_noOut$points, 
    groups=aaInfo$ageClass, 
    col='gray72',
    kind = c("ehull"),
    draw="polygon", 
    border="grey72", 
    show.groups='adulto',
)
## now put in actual points
points(MDS1, MDS2,
    bg=aaInfo$Clust,
    pch=21,
    col=aaInfo$parentClust,
    lwd=3.0,
    cex=2.0,
)
## ellipse around the juvenile groups
ordiellipse(comboNMS_noOut$points, 
    groups=aaInfo$Clust,
    kind=c("ehull"),
    col=6, 
    show.groups=6,
    lty='dashed', 
    lwd=2,
)
## centroids?

ordiellipse(ord, 
    groups, 
    display="sites", 
    kind = c("sd","se", "ehull"),
    conf, 
    draw = c("lines","polygon", "none"),
    w = weights(ord, display), 
    col = NULL, 
    alpha = 127, 
    show.groups,
    label = FALSE, 
    border = NULL, 
    lty = NULL, 
    lwd=NULL
)

## check to see if there is a relationship
## between habitat type and juvenile cluster relationship
## = relabel hierarchical clustering diagram with 
## habitat type...

#### habitat type in juvenile cluster groups ####

library(stats)
library(vegan)

## where is our community matrix of juveniles?
juvComm <- read.csv('juvComm.csv', header=TRUE, row.names='PsubP')
## checks
rowSums(juvComm)
any(colSums(juvComm) == 0)
all(colSums(juvComm) > 0)

juvCommBray <- vegdist(juvComm)
juvComm.ward <- hclust(juvCommBray, method='ward.D2')

plot(juvComm.ward)
k=3
juvGrp <- rect.hclust(juvComm.ward, border=c(5,6,7), k=k)

## how can we relabel this? 

str(juvGrp)

names(juvGrp)

names(juvGrp[[1]])

names(juvGrp[[2]])

## we did this before like this:
k=3
juvCommBray <- vegdist(juvComm)
juvComm.ward <- hclust(juvCommBray, method='ward.D2')
juvGrp <- rect.hclust(juvComm.ward, k=k, border=c(5,6))
## get out cluster group:
cmem <- vector(length=k)
PsubP <- vector()
gr <- vector()
for (i in 1:k){
    cmem.i <- as.numeric(attr(juvGrp[[i]], "names"))
    PsubP <- c(PsubP, cmem.i)
    gr <- c(gr, rep(i, length(cmem.i)))
    }
cGroup <- data.frame(cbind(PsubP, gr))
## jeezus. there has got to be an easier way to do that.
## we need to get the habitat type of these
envOnly <- read.csv('envOnly.csv')
findG = function(psubp){
    return(envOnly[envOnly$PsubP == psubp,'habitat'])
}
cGroup$habitat <- sapply(cGroup$PsubP, findG)
## order by PsubP, because as is this is not working 
## for labels:
cGroup <- cGroup[order(cGroup$PsubP),]
all(rownames(juvComm) == cGroup$PsubP)

aa <- hclust(juvCommBray, method='ward.D2')
plot(aa, labels=cGroup$habitat)
bb <- rect.hclust(aa, k=k, border=(1:k)+4)

azedia



all(rownames(juvComm) == juvComm.ward$labels)
all(cGroup$PsubP == juvComm.ward$labels)

par(mfrow=c(1,2))
plot(juvComm.ward, labels=cGroup$habitat)
rect.hclust(juvComm.ward, k=k, border=(1:k)+4)
plot(juvComm.ward)
rect.hclust(juvComm.ward, k=k, border=(1:k)+4)
par(mfrow=c(1,1))


## some spot checks
envOnly[envOnly$PsubP == 5.3,]
envOnly[envOnly$PsubP == 8.3,]
envOnly[envOnly$PsubP == 10.2,]
envOnly[envOnly$PsubP == 10.3,]
envOnly[envOnly$PsubP == 4.1,]
envOnly[envOnly$PsubP == 2.4,]

## looks okay. What do we do with this?

### redo juvenile habitat NMS with clust results ###

## does this inform the above NMS, where we examine the habitat type
## of the juveniles?

## we need two vectors - one for adult colors and hulls, and one for child points

## here, we want the adults to retain their habitat info.
## and we want the child points to have both - habitat info, and cluster info
## we already have comboHab, with the habitat info, now make a 
## hybrid adult-habitat and child-cluster info:

## before we did our graphic something like this:


envOnly <- read.csv('envOnly.csv')
adultosDejovenes <- read.csv('adultosDejovenes.csv')
## habitat info, this will give our bg color for the child point, and total color for adults
comboHab <- sapply(adultosDejovenes$PsubP, function(x){ envOnly[envOnly$PsubP == x,'habitat']})
## cluster info. Adults are just given their habitat info:
juvGroups <- vector()
juvGroups = as.character(comboHab[1:37])
juvGroups = c(juvGroups, as.character(cGroup$gr))
## we also need a vector that shows habitat type, but excludes juveniles, 
## for hulls of adults:
adultHab <- comboHab
levels(adultHab) <- c(levels(comboHab), 'juv')
adultHab[38:74] <- 'juv'
## let's bind these all together:
juvClustHabPlotInfo <- data.frame(
                                adultosDejovenes$PsubP, 
                                adultosDejovenes$X, 
                                comboHab, 
                                as.character(juvGroups), 
                                as.character(adultHab),
                                stringsAsFactors=FALSE
                                )
colnames(juvClustHabPlotInfo) <- c(
                                    "PsubP", 
                                    "ageClass", 
                                    "habType", 
                                    "juvCluster", 
                                    "adultHab"
                                    )
juvClustHabPlotInfo <- juvClustHabPlotInfo[juvClustHabPlotInfo$PsubP != 1.1,]


## we can remove our outlier, 1.1, to make it compatible
## with our NMS results

load(file='comboNMS_noOut.rda')
MDS1 <- comboNMS_noOut$points[,'MDS1']
MDS2 <- comboNMS_noOut$points[,'MDS2']

## now how to translate this into plotting colors?

## parent hull colors
hullColrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00', NA)
names(hullColrsMap) <- c('BC', 'BS', 'CLB', 'RCA', 'RG', 'juv')
## points, core colors
pointsCoreColrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00', '5','6','7')
names(pointsCoreColrsMap) <- c('BC', 'BS', 'CLB', 'RCA', 'RG', '1','2','3' )
## points, shell colors:
pointsShellColrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00')
names(pointsShellColrsMap) <- c('BC', 'BS', 'CLB', 'RCA', 'RG')

## check this
#plot(1:5,rep(1,5), pch=21, bg=pointsShellColrsMap, cex=4)
#text(1:5,rep(1,5), names(pointsShellColrsMap))
## works...

png(file='habtype2naturalClusterJuv.png', height=1000, width=1000)
plot(MDS1, MDS2,
    type='n',
)
## ellipse around all parent groups?
ordiellipse(comboNMS_noOut$points,
    groups=juvClustHabPlotInfo$ageClass,
    kind="ehull",
    draw="polygon",
    show.groups=c("adulto"),
    col = 'gray72',
    border=FALSE,
)
## hull around parent habitat groups, but not child points
ordihull(comboNMS_noOut$points,
    groups=juvClustHabPlotInfo$adultHab,
#    show.groups=c("BC","BS","CLB","RCA","RG"),
    col = hullColrsMap[names(table(juvClustHabPlotInfo$adultHab))],
    lwd=4,
)
## get hulls or ellipses on those two child groups:
juvHulls=ordihull(ord=comboNMS_noOut$points,
    groups=juvClustHabPlotInfo$juvClust,
    draw="polygon",
    alpha=25,
    col=c(6,7),
    border=c(6,7),
    show.groups=c("2","3"),
)
## centroids for these
juvCentroids <- t(summary(juvHulls))
points(centroids[,1],centroids[,2], 
    pch=4, 
    cex=3, 
    col=c(6,7))
points(MDS1, MDS2,
    #bg=aaInfo$Clust,
    pch=21,
    bg=pointsCoreColrsMap[juvClustHabPlotInfo$juvCluster],
    col=pointsShellColrsMap[juvClustHabPlotInfo$habType],
    lwd=6.0,
    cex=4.0,
)
text(MDS1, MDS2 - 0.02, juvClustHabPlotInfo$PsubP, cex=1.5)
dev.off()

head(juvClustHabPlotInfo)
## color adults as simple solid circles, child points as
## cores with their cluster info, and their shells from their
## parents

## consider orditest to see if these groupings are meaningfull?

pointsCoreColrsMap[juvClustHabPlotInfo$juvCluster]

pointsCoreColrsMap

juvClustHabPlotInfo$juvCluster

juvClustHabPlotInfo

head(juvClustHabPlotInfo)

cbind(juvClustHabPlotInfo$PsubP, 
    hullColrsMap[juvClustHabPlotInfo$adultHab],
    as.character(juvClustHabPlotInfo$habType))

head(juvClustHabPlotInfo)

    lty='dashed',
    col=6,
)

levels(juvClustHabPlotInfo$adultHab)

head(juvClustHabPlotInfo)

as.character(juvClustHabPlotInfo$adultHab)


## regular colors, all points need this juv or adult:
colrsMap <- c('#E41A1C', '#377EB8', '#4DAF4A', '#984EA3', '#FF7F00')
names(colrsMap) <- names(table(comboHab))
## does this work?
colrs <- colrsMap[comboHab]

ordihull(comboNMS, comboHabAdOnly, col=hullColrsMap, lwd=3)
text(MDS1, MDS2 - 0.02, adultosDejovenes$PsubP, cex=1)

## this is a cool but confusing figure...
## perhaps best to just use the land use + children figure, 
## and add a hull 


## restructure notebook for juvenile analysis to make more sense. 

## in general, the hierarchical clustering doesn't seem to add 
## much to the juveniles. The story is too messy. Just track the 
## original adult group evolution, adult clusters and land uses
## to their new spaces on the NMS. 

## so, cleanup the notebook with +/- publishable figures for the juveniles:
## hierarchical clustering, without forcing groups (just highlight the 
## clade that comes entirely from bosque cerrado

## the main looming analysis is the spatial analysis. Think about that
## next...

########################## watersheds ##########################

## both are interesting. We have an hour, let's start by playing around with 
## watersheds....

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd
import rasterio
import pysheds
import scipy.spatial as sp
import scipy.stats
import rasterio.plot
from pysheds.grid import Grid
import mplleaflet
plt.ion()

## and following the example here: https://mattbartos.com/pysheds/
## and here:
## https://github.com/mdbartos/pysheds/blob/master/examples/quickstart.ipynb

## good mmindless music here: https://youtu.be/l9nh1l8ZIJQ

## I think the basic object of a 
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
fig, ax = plt.subplots(figsize=(8,6))
fig.patch.set_alpha(0)
plt.imshow(grid.dem, extent=grid.extent, cmap='cubehelix', zorder=1)
plt.colorbar(label='Elevation (m)')
plt.grid(zorder=0)
plt.title('Digital elevation map')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()
## pretty
#plt.savefig('img/conditioned_dem.png', bbox_inches='tight')

## get an error with this:
grid.resolve_flats('dem', out_name='inflated_dem')
## is this because we haven't done a flow direction array yet?
## not sure. there is something about this here:
https://www.hatarilabs.com/ih-en/elevation-model-conditioning-and-stream-network-delimitation-with-python-and-pysheds-tutorial

depressions = grid.detect_depressions('dem')
# Plot depressions
plt.imshow(depressions)
## none?

flats = grid.detect_flats('dem')
plt.imshow(flats)
## also none?

aa = ('asdf'
'asdf')

############## tutorial ###################
## let's try the example data for a bit:
## https://www.hatarilabs.com/ih-en/elevation-model-conditioning-and-stream-network-delimitation-with-python-and-pysheds-tutorial

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import geopandas as gpd
import pandas as pd
import pyproj
from pysheds.grid import Grid
import mplleaflet
import copy
import rasterio
import rasterio.plot
import pprint
import fiona
import shapely
import pickle
plt.ion()

### toybox for pickle error ###
#from pysheds.grid import Grid
#import pickle
#
#pathSD = ('/home/daniel/Documents/analyses/losCedrosTrees/anaData/GIS/'
#'ElevationModelConditioningandStreamNetwork'
#'DelimitationwithPythonandPysheds/Rst/20190109125130_1063922483.tif')
#
#grid = Grid.from_raster(pathSD, data_name='dem')
#
### play around with pickle:
#
#pickle.dump(grid, open("testPicklePyshed.p", "wb"))
#
#grid2 = pickle.load(open('testPicklePyshed.p', 'rb'))
#
#grid.view('dem')
#
#grid2.view('dem')
#
### end toybox ###

#Define a function to plot the digital elevation model 
def plotFigure(data, label, cmap='Blues'):
    plt.figure(figsize=(12,10))
    plt.imshow(data, extent=grid.extent)
    plt.colorbar(label=label)
    plt.grid()

elevDem=grid.dem[:-1,:-1]
plotFigure(elevDem, 'Elevation (m)')

plt.figure(figsize=(8,8))
plt.imshow(elevDem)

# Detect depressions
depressions = grid.detect_depressions('dem')

# Plot depressions
plt.imshow(depressions)
## that works, makes me think I really just don't have 
## them in my data

# Fill depressions
grid.fill_depressions(data='dem', out_name='flooded_dem')

# Test result
depressions = grid.detect_depressions('flooded_dem')
plt.imshow(depressions)
## still some spots in the very south?

# Detect flats
flats = grid.detect_flats('flooded_dem')
# Plot flats
plt.imshow(flats)
## also works well, makes me think my data just doesn't 
## have these issues?

grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')

plt.figure(figsize=(8,8))
plt.imshow(elevDem)

plt.figure(figsize=(8,8))
plt.imshow(grid.inflated_dem[:-1,:-1])
## can't really see them with your eye, but I 
## guess they cause hell on the algorithms.

          #N    NE    E    SE    S    SW    W    NW
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)

plotFigure(grid.dir,'Flow Direction','viridis')

# Specify discharge point
x, y = -107.91663,27.83479

# Delineate the catchment
grid.catchment(data='dir', x=x, y=y, dirmap=dirmap, out_name='catch',
               recursionlimit=15000, xytype='label', nodata_out=0)

# Clip the bounding box to the catchment
grid.clip_to('catch')

# Get a view of the catchment
demView = grid.view('dem', nodata=np.nan)

plotFigure(demView,'Elevation')


## works perfectly...of course...
## so the perennial question, what's wrong with my data?

#########################################################

fig, ax = plt.subplots(figsize=(8,6))
fig.patch.set_alpha(0)
plt.imshow(grid.dem, extent=grid.extent, cmap='cubehelix', zorder=1)

plt.colorbar(label='Elevation (m)')
plt.grid(zorder=0)
plt.title('Digital elevation map')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.tight_layout()


grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
## make a color map for our flow directions:
         #N    NE    E    SE    S    SW    W    NW
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
## don't this is necessary, these are the defaults for the flowdir method, anyway
## oh, but we need them to make the color bar for the next graphic...

## create the flowdir 
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
plotFigure(grid.dir,'Flow Direction','viridis')

## why does it look like half the spots are flowing north? 
## anyway, cool, that's the flow direction

## so now, I think we should be able to define upstream and
## downstream, i.e. make catchments. Can we get the 
## catchment for +/- basecamp?

x, y = 747500, (4000 + 1.003*10**7)

x, y = 747500, (3500 + 1.003*10**7)

x, y = 747000, (3500 + 1.003*10**7)

x, y = 748000, (3000 + 1.003*10**7)

x, y = 748722, (590 + 1.0032*10**7)

grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
grid2 = copy.deepcopy(grid)
grid2.catchment(data=grid2.dir, x=x, y=y, dirmap=dirmap, out_name='catch',
               recursionlimit=15000, xytype='label', nodata_out=0)
grid2.clip_to('catch')
## I think you can reverse this with:
#grid2.clip_to('dem')
demView = grid2.view('dem', nodata=np.nan)
plotFigure(demView,'Elevation')

grid.flowdir(data='dem', out_name='dir', dirmap=dirmap, inplace=False)

grid.shape


plotFigure(grid.dem,'viridis')

# Plot the catchment
## okay, that really didn't work...
demView = grid.view('dem', nodata=np.nan)
plotFigure(demView,'Elevation')

## all of our rasters get clipped to 6 pixels or somethiing:

grid.shape

grid2.shape

## why?
## looks like we are really sensitive to the pour point? 

## can we tile this, look at a lot of different pour points?

## output variable of interest is the extent of the grid.


round(grid2.extent[0])

grid2.extent[0:2]
grid2.extent[2:3]

grid2.extent

list(range(grid2.extent[1]
grid2.extent
grid.extent


## okay, let's do this the hard way. This should tell us if it 
## really is a question of the pour point selection, or if our 
## data is screwed:


grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
with open('checkPour.txt', 'w') as f:
    for i in range(745020, 749211):
        for j in range(10031757, 10035794):
            try:
                grid2 = copy.deepcopy(grid)
                grid2.catchment(data=grid2.dir, x=i, y=j, dirmap=dirmap, 
                               out_name='catch', recursionlimit=15000, 
                               xytype='label', nodata_out=0)
                grid2.clip_to('catch')
                f.write(str(i) + ', ' + str(j) + ', ' + str(np.prod(grid2.shape)) + ' \n')
            except (ValueError, IndexError):
                f.write(str(i) + ', ' + str(j) + ', ' + 'pourpoint outside \n')
                
## well, that dumped.

## makes sense, that's lots of cycles:
(749211-745020)*(10035794-10031757)

## 16,919,067 

## let's check what happened before it failed:

pourTest = pd.read_csv('checkPour.txt', header=0, names=['X','Y', 'Pixels'])
pourTest.head()
## oops, I printed some trailing whitespace in the pixel area column:
pourTest.Pixels = pourTest.Pixels.str.strip()
## get rid of outside points:
pourTest = pourTest[~(pourTest.Pixels == 'pourpoint outside')]
pourTest.Pixels = pd.to_numeric(pourTest.Pixels, downcast='integer')
pourTest.shape ## 187,000 coords tested, out of 16,000,000. Shit
pourTest.Pixels.max() 
## 540, okay, not bad. Should be much bigger, but look at this
pourTest = pourTest.sort_values('Pixels')

plt.ion()

plt, ax = plt.subplots(1,1)
ax.hist(pourTest.Pixels, bins=50)

## the vast majority of these pixels yield a catchment of very small 
## size. But let's look at the ones that yield more cells...
 
## the largest catchment is here:
maxes = pourTest.Pixels == pourTest.Pixels.max() 
maxPours = pourTest[maxes].reset_index(drop=True)

## there are also several midsize catchments in this:

mid240 = pourTest.Pixels == 240
mid240Pours = pourTest[mid240].reset_index(drop=True)


pourTest.Pixels.gt(150)

pourTest.Pixels > 150

pourTest.Pixels < 200

pourTest.Pixels > 150 and pourTest.Pixels < 200

pourTest.Pixels.gt(150) and pourTest.Pixels.lt(200) 

mid160 = ((pourTest.Pixels > 150) & (pourTest.Pixels < 200))
mid160Pours = pourTest[mid160].reset_index(drop=True)
## actually, that contains 4 possible catchments:
mid160Pours.Pixels.unique()
## break it up?:

mid156 = (pourTest.Pixels == 156)
mid156Pours = pourTest[mid156].reset_index(drop=True)

## why is the max pours data graphing weird?
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
x, y = maxPours.iloc[0,0:2]
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
grid.catchment(data=grid.dir, x=x, y=y, dirmap=dirmap, out_name=nameOfCatch,
                   recursionlimit=15000, xytype='label', nodata_out=0)

def plotFigure(data, label, cmap='Blues'):
    plt.figure(figsize=(12,10))
    plt.imshow(data, extent=grid.extent)
    plt.colorbar(label=label)
    plt.grid()

## what do these catchments look like?

x, y = maxPours.iloc[0,0:2]

def addCatchment(pt, dat, nameOfCatch, grid=None):
    if grid is None: 
        grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
    x, y = dat.iloc[pt,0:2]
    dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
    grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
    grid.catchment(data=grid.dir, x=x, y=y, dirmap=dirmap, out_name=nameOfCatch,
                   recursionlimit=15000, xytype='label', nodata_out=0)
    return(grid)

muck = addCatchment(pt=10, 
              dat=maxPours, 
              nameOfCatch='maxPours',
              grid=None)

addCatchment(pt=0, 
              dat=mid240Pours, 
              nameOfCatch='mid240Pours',
              grid=muck)

addCatchment(pt=0, 
              dat=mid156Pours, 
              nameOfCatch='mid156Pours',
              grid=muck)


## to look at one of these, export the view as a np.array, with
## the "view" method:

#muck.clip_to('mid156Pours')

#muck.clip_to('dem')

plt.figure()
plt.imshow(muck.view('mid156Pours', nodata=np.nan), extent=muck.extent)
plt.colorbar()

plt.figure()
plt.imshow(muck.view('mid240Pours'), extent=muck.extent)
plt.colorbar()

plt.figure()
plt.imshow(muck.view('maxPours'), extent=muck.extent)
plt.colorbar()

plt.imshow(muck.view('dem'), extent=muck.extent)


fig, axes = plt.subplots(1,3)



plotFigure(aa,'Elevation')

plotFigure(bb,'Elevation')

dir(mid240Pours)



## huh, well it definitely works sometimes. Where are these pour points?

min(maxPours.X) #745020
max(maxPours.X) #745035
min(maxPours.Y) #10035225
max(maxPours.Y) #10035255

## Seems like they all fall in the same area:
fig, ax = plt.subplots(figsize=(8,6))
fig.patch.set_alpha(0)
plt.imshow(grid.dem, extent=grid.extent, cmap='cubehelix', zorder=1)

## for example, max pours:
ax.hlines(max(maxPours.Y), min(maxPours.X), max(maxPours.X), 'red')
ax.hlines(min(maxPours.Y), min(maxPours.X), max(maxPours.X), 'red')
ax.vlines(max(maxPours.X), min(maxPours.Y), max(maxPours.Y), 'red')
ax.vlines(min(maxPours.X), min(maxPours.Y), max(maxPours.Y), 'red')
## weird, that is at the top of the mountain.

## for example, max pours:
ax.hlines(max(maxPours.Y), min(maxPours.X), max(maxPours.X), 'red')
ax.hlines(min(maxPours.Y), min(maxPours.X), max(maxPours.X), 'red')
ax.vlines(max(maxPours.X), min(maxPours.Y), max(maxPours.Y), 'red')
ax.vlines(min(maxPours.X), min(maxPours.Y), max(maxPours.Y), 'red')

## weird, that is at the top of the mountain.
## not sure what is going on here - how can a pour point at the top of a 
## mountain yield a catchment that drains most of the study area?

## anyway, this seems to work, if we have good pour points. 
## how do we find them? I think we really only need two, to 
## get the pour points for the Rio Los Cedros and the Rio Magdalena Chico

## how to get these? I think we need to run through the cells as we did 
## above, but for the southwest corner. 

grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')

grid.extent

eastMin=748750 
eastMax=749200 
northMin=10032300
northMax=10033300

dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)

with open('checkSouthEastPour.txt', 'w') as f:
    for i in range(eastMin, eastMax):
        for j in range(northMin, northMax):
            try:
                grid2 = copy.deepcopy(grid)
                grid2.catchment(data=grid2.dir, x=i, y=j, dirmap=dirmap, 
                               out_name='catch', recursionlimit=15000, 
                               xytype='label', nodata_out=0)
                grid2.clip_to('catch')
                f.write(str(i) + ',' + str(j) + ',' + str(np.prod(grid2.shape)) + '\n')
            except (ValueError, IndexError):
                f.write(str(i) + ',' + str(j) + ',' + 'pourpoint outside\n')

## I think I may need to buffer this map quite a bit to do the river calculations, 
## but we may be able to get the pour points this way...

(eastMax - eastMin) * (northMax - northMin) # = 450,000 coordinate pairs. 

pourTest = pd.read_csv('checkSouthEastPour.txt', header=0, names=['X','Y', 'Pixels'])

## did we have any outside coords?
pourTest.Pixels.dtype
## looks good

len(pourTest.Pixels.unique())
pourTest.Pixels.unique()
## 16 sizes detected, biggest are 180 and 216 cells...
## how many cells in the map, generally?

dir(grid)

## I think this is our size in cells?
grid.size
## or is that an actual area measurement (m2)?
## not sure.
pourTest.shape ## 15000 cells coords tested before failure. ~1/3.

## anyway, find these catchments, see where they map

p216 = (pourTest.Pixels == 216)
pour216 = pourTest[p216]
pour216.reset_index(inplace=True, drop=True)

pour216.head()

def plExt(pr,ax=None,col='red'):
    if ax is not None:
        ax.hlines(max(pr.Y), min(pr.X), max(pr.X), col)
        ax.hlines(min(pr.Y), min(pr.X), max(pr.X), col)
        ax.vlines(max(pr.X), min(pr.Y), max(pr.Y), col)
        ax.vlines(min(pr.X), min(pr.Y), max(pr.Y), col)
    ext={"xmin":min(pr.X),
         "xmax":max(pr.X),
         "ymin":min(pr.Y),
         "ymax":max(pr.Y)
        }
    return(ext)

fig = plt.figure()
ax = fig.add_subplot()
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
ax.imshow(grid.dem, extent=grid.extent)
plExt(pour216, ax=ax)

pour216.X.unique()
## interesting, this is a strip of cells along x=748765. 

## make the catchment from one of them:

pr = pour216
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
grid2 = copy.deepcopy(grid)
grid2.catchment(data=grid2.dir, x=pr.X[0], y=pr.Y[0], dirmap=dirmap, 
               out_name='catch', recursionlimit=15000, 
               xytype='label', nodata_out=0)
grid2.clip_to('catch')

plt.imshow(grid.dem, extent = grid.extent)

lcPoly = gpd.read_file('GIS/lcPoly.geojson')
lcPoly.boundary.plot(color='purple', linewidth=3)

lcPoly.boundary.plot(color='purple', linewidth=3, ax=ax)

## we have to find these damn pour points.

## function for outputing catchment size:

def catchSize(X,Y):
    grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
    grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
    grid2 = copy.deepcopy(grid)
    grid2.catchment(data=grid2.dir, x=X, y=Y, dirmap=dirmap, 
                   out_name='catch', recursionlimit=15000, 
                   xytype='label', nodata_out=0)
    return(grid2.catch.view_size)


catchSize(748810, 10032700)

X,Y = (748710, 10032700)
grid = Grid.from_raster('./GIS/anaPlotDEM.tif', data_name='dem')
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)
grid2 = copy.deepcopy(grid)
grid2.catchment(data=grid2.dir, x=X, y=Y, dirmap=dirmap, 
               out_name='catch', recursionlimit=15000, 
               xytype='label', nodata_out=0)
grid2.clip_to('catch')

grid2.clip_to('dem')

demView = grid.view('dem', nodata=np.nan)

plt.imshow(grid2.catch)


## ugh, this is so fucked. What if we try this on our more raw data? As in, maybe the tile
## that we got that LC DEM from?

## can we read this in?
## I think we are going to need to put the metadata into the dem file itself...

## can we read this?

bigAstpath = "/home/daniel/Documents/LosCed/Ecuador_GIS/losced/ASTGTM2_N00W079/ASTGTM2_N00W079_dem_UTM17S.tif"

X,Y = (748710, 10032700)

X,Y = (747865, 10026800)

grid = Grid.from_raster(bigAstpath, data_name='dem')
plt.imshow(grid.view('dem'))

depressions1 = grid.detect_depressions('dem')

plt.imshow(depressions1)

grid.fill_depressions(data='dem', out_name='flooded_dem')

depressions2 = grid.detect_depressions('flooded_dem')

fig, axs  = plt.subplots(1,2)

depressions2.any() ## there are still a lot. why?:

axs[0].imshow(depressions1)
axs[1].imshow(depressions2)

## just some weirdness on the western border

flats = grid.detect_flats('flooded_dem')

plt.imshow(flats)

grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')


fig, axs  = plt.subplots(1,2)
axs[0].imshow(grid.view('dem'))
axs[1].imshow(grid.view('inflated_dem'))

dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)

grid.flowdir(data='dem', out_name='dir', dirmap=dirmap)

grid.catchment(data=grid.dir, x=X, y=Y, dirmap=dirmap, 
               out_name='catch', recursionlimit=15000, 
               xytype='label', nodata_out=0)
grid.clip_to('catch')

## that blew out the memory...

plt.imshow(grid.view('dem'))

## condense, start pipeline over
bigAstpath = "/home/daniel/Documents/LosCed/Ecuador_GIS/losced/ASTGTM2_N00W079/ASTGTM2_N00W079_dem_UTM17S.tif"
grid = Grid.from_raster(bigAstpath, data_name='dem')
grid.fill_depressions(data='dem', out_name='flooded_dem')
grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap) 
grid.accumulation(data='dir', dirmap=dirmap, out_name='acc', apply_mask=False)

## might as well save this somewhere:
#grid.to_raster('dir', 'test_dir.tif', view=False)
## doesn't work...to ascii?
#grid.to_ascii('dir', 'losCedFullTileFlow.asc')
## and to load our huge ascii file here?
#anaFlowAsciiPath = "/home/daniel/Documents/analyses/bigfiles/anaBigFiles/losCedFullTileFlowNoDat-1.asc"
#grid.read_ascii(anaFlowAsciiPath, 
#                crs=pyproj.proj.Proj('+proj=utm +zone=17 +south +ellps=WGS84 +datum=WGS84 +units=m +no_defs'),  
#                data_name='dir')
## hmmm, it is having trouble with our "nodata" value not being a number, I think...
## can we change this?
#cd /home/daniel/Documents/analyses/bigfiles/anaBigFiles
#cp losCedFullTileFlow.asc losCedFullTileFlowNoDat-1.asc
#sed 's/nan/-1/' losCedFullTileFlow.asc > losCedFullTileFlowNoDat-1.asc
#head -n 6 losCedFullTileFlowNoDat-1.asc
## did that work?
## it's okay, but the two rasters don't quite align anymore:
#grid.dem.extent
#grid.dir.extent
## so maybe better to work from scratch, regenerate the flow direction raster
## maybe pickle it instead?

## use rasterio and take a look, and pick some spots interactively

fig, ax = plt.subplots()
bigAstRast = rasterio.open(bigAstpath)
bigAstFlow = rasterio.open('/home/daniel/Documents/analyses/bigfiles/anaBigFiles/losCedFullTileFlow.asc')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
plt.close('all')
rasterio.plot.show(bigAstFlow)
lcPoly.boundary.plot(color='red', linewidth=3, ax=plt.gca())
hydro.plot(color='Blue', ax=plt.gca())

## yeah, that looks much better. Let's try a catchment:

## let's try to pick a pourpoint interactively:

X,Y = plt.ginput(1)[0]

## now snap it to a high flow point
## https://github.com/mdbartos/pysheds/blob/master/examples/snap_to_mask.ipynb

aa = grid.snap_to_mask(grid.acc > 50, np.array([X,Y]), return_dist=False)

## these should be the main watershed of the study area (the Rio Los 
## Cedros watershed)

grid.catchment(data='dir', x=aa[0], y=aa[1], out_name='catch',
                   recursionlimit=15000, xytype='label')

grid.clip_to('catch')

plt.close('all')
plt.imshow(grid.view('dem'), extent=grid.catch.extent)

plt.imshow(grid.view('acc'), extent=grid.catch.extent)

## that works a lot better. finally. 

## can that be exported as a polygon?

help(grid.polygonize)

shapes = grid.polygonize()
lsh = list(shapes)

pprint.pprint(lsh[0])


## looks like just one polygon, must be the outline?

## can we plot it?

fig, ax = plt.subplots()

plt.plot(lsh[0][0]['coordinates'])

## meh, think we're going to have use shapely and geopandas

## can geopandas read this?

help(shapely.geometry.polygon.Polygon)

type(lsh[0][0])

## it's buried pretty deep in there:

len(lsh[0][0]['coordinates'][0])

pprint.pprint(lsh[0][0]['coordinates'][0])

aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])

plt.close('all')
## make a prettier version to look at?:
bb = aa.buffer(100)
p = gpd.GeoSeries(aa)
p2 = gpd.GeoSeries(bb)
fig, axes = plt.subplots(1,2)
p.plot(ax=axes[0])
p2.plot(ax=axes[1])

## just using fiona fucks up the geojson, it is strict 
## about only allowing WGS84, I want UTM17S

## so, maybe we use geopandas?

p = p.set_crs('EPSG:32717')
p.to_file("./GIS/rioLCwatershed.geojson", driver='GeoJSON')

## yup, good old geopandas found a way, geojson with 
## UTM17S. I love that package.

aa = grid.snap_to_mask(grid.acc > 50, np.array([X,Y]), return_dist=False)

## condense the above pipeline, for jupyter:

## what do we want? We want the two watersheds combined, and then divide
## them in catchments of equal size that can test our hypothesis that 
## plant communities are different in each small watershed.

## 1) get the other watershed, make sure we can do this and that it makes sense.

## 2) develop a method for getting +/- equal k-watersheds. 

## 3) Find the K that makes sense, given sample locations

## 4) conduct turnover analysis

### rio magdalena chico delineation: ###

## as above
fig, ax = plt.subplots()
bigAstRast = rasterio.open(bigAstpath)
bigAstFlow = rasterio.open('/home/daniel/Documents/analyses/bigfiles/anaBigFiles/losCedFullTileFlow.asc')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
plt.close('all')
rasterio.plot.show(bigAstFlow)
lcPoly.boundary.plot(color='red', linewidth=3, ax=plt.gca())
hydro.plot(color='Blue', ax=plt.gca())

X,Y = plt.ginput(1)[0]

snapX, snapY = grid.snap_to_mask(grid.acc > 50, np.array([X,Y]), return_dist=False)
grid.clip_to('dem')
grid.catchment(data='dir', x=snapX, y=snapY, out_name='RMCcatch',
                   recursionlimit=15000, xytype='label')
grid.clip_to('RMCcatch')

plt.close('all')
plt.imshow(grid.view('dem'), extent=grid.RMCcatch.extent)

shapes = grid.polygonize()
lsh = list(shapes)
aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')

p.plot() ## looks right
p.to_file("./GIS/rioMagChicWatershed.geojson", driver='GeoJSON')

### end RMC delineation ###

## now what?

## let's back up a bit. We are going to need to be careful about labeling, etc,
## so "catch" has got to go...

gridPath="/home/daniel/Documents/analyses/bigfiles/anaBigFiles/asterFlowAccum.p"
grid = pickle.load(open(gridPath, "rb"))

## rerun our RLC and RMC delineations with more appropriate names

## in both cases, let's use coordinates that put the pourpoints
## at the border of the reserve

## having some issues with unpickling...

grid = pickle.load(open(gridPath, "rb"))

## looks okay...but now some features don't work...

plt.imshow(grid.view('dem'), extent=grid.dem.extent)

aa = grid.view('dem')

aa = grid.clip_to('dem')

## meh. if we can't pickle, and the ascii grid and 
## tif outfile formats aren't working well for me, 
## wtf do I do? Just rerun the same computer-crashing
## analysis everytime I work on this?

## I think we have to cut the tile down to a smaller 
## size. 

## best way to do this? In or out of pysheds?
## as above

## find a good chunk of the tile to use 

bigAstpath = "/home/daniel/Documents/LosCed/Ecuador_GIS/losced/ASTGTM2_N00W079/ASTGTM2_N00W079_dem_UTM17S.tif"
fig, ax = plt.subplots()
bigAstRast = rasterio.open(bigAstpath)

bigAstFlow = rasterio.open('/home/daniel/Documents/analyses/bigfiles/anaBigFiles/losCedFullTileFlow.asc')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')

plt.close('all')
#rasterio.plot.show(bigAstFlow)
rasterio.plot.show(bigAstRast)
lcPoly.boundary.plot(color='red', linewidth=3, ax=plt.gca())
hydro.plot(color='Blue', ax=plt.gca())

## we'll take the southwest corner: 

neCorner = plt.ginput(1)

## the southwest corner is the SW corner
## of the bounding box of the tile:

dir(bigAstRast.bounds)

bigAstRast.bounds.left
bigAstRast.bounds.bottom

722546.2651 

9999984.6274

## so our points are

766393.3839281744, 10057694.120238958

## and in the command line version of Rasterio:
#rio clip input.tif output.tif --bounds xmin ymin xmax ymax

astTile="/home/daniel/Documents/LosCed/Ecuador_GIS/losced/ASTGTM2_N00W079/ASTGTM2_N00W079_dem_UTM17S.tif"

rio clip $astTile GIS/smallAster.tif --bounds "722546 9999985 766393 10057694" 


## try the above pipelines for this 

## plot with Rasterio for finding the pour points
smallAstpath = "/home/daniel/Documents/analyses/losCedrosTrees/anaData/GIS/smallAster.tif"
plt.close('all')
smallAstRast = rasterio.open(smallAstpath)
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
rasterio.plot.show(smallAstRast)
lcPoly.boundary.plot(color='red', linewidth=3, ax=plt.gca())
hydro.plot(color='Blue', ax=plt.gca())

## build grid
grid = Grid.from_raster(smallAstpath, data_name='dem')
grid.fill_depressions(data='dem', out_name='flooded_dem')
grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap) 
grid.accumulation(data='dir', dirmap=dirmap, out_name='acc', apply_mask=False)

### get RLC polygons ###
## for the LC watershed, this was the pourpoint
X, Y = (748696, 10032703) ## handpicked, not actual points
grid.clip_to('dem')
snapX, snapY = grid.snap_to_mask(grid.acc > 50, np.array([X,Y]), return_dist=False)
snapX, snapY ## (749325.889708213, 10032267.125889309), actual points
## label the catchment particularly this time 
grid.catchment(data='dir', x=snapX, y=snapY, out_name='RLCcatch',
                   recursionlimit=15000, xytype='label')
grid.clip_to('RLCcatch')
shapes = grid.polygonize()
lsh = list(shapes)
aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')
#p.to_file("./GIS/rioLCwatershed.geojson", driver='GeoJSON')


### get RMC polygons ###

handX, handY = plt.ginput(1)[0]

grid.clip_to('dem')

handX, handY ## (748748.2402529713, 10032948.871108945)
snapX, snapY = grid.snap_to_mask(grid.acc > 50, np.array([handX,handY]), return_dist=False)
snapX, snapY ## (748769.2693352941, 10032974.829728188), actual points
## label the catchment particularly this time 
grid.catchment(data='dir', x=snapX, y=snapY, out_name='RMCcatch',
                   recursionlimit=15000, xytype='label')
grid.clip_to('RMCcatch')
shapes = grid.polygonize()
lsh = list(shapes)
aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')
#p.to_file("./GIS/rioMCwatershed.geojson", driver='GeoJSON')

## fine, now what? 

## how to do we find k=ten watersheds of equal size?

##### play with rivers #####

## seems like an essential step here is get the main 
## stream/river areas. These can then be walked up 
## to find the small catchments, I think.

grid.clip_to('RLCcatch')
branches = grid.extract_river_network('RLCcatch', 'acc', threshold=50)
fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    plt.plot(line[:, 0], line[:, 1])

## neat. how do we get the pourpoint and the area of catchment from one of these?

## to get one branch:
aa = branches['features'][0]['geometry']['coordinates']
bb = np.asarray(aa)
fig, ax = plt.subplots()
plt.plot(bb[:, 0], bb[:, 1])

## how do we use these geo coords to look up the accumulations?
## how does the snap function do it?

less /usr/local/lib/python3.8/dist-packages/pysheds/grid.py

## doesn't seem to be very complicated...but not sure how that
## works. I think that the function uses the affine to go between 
## the numpy array and 

snapX, snapY

aa = grid.view('acc')

## not sure where they make the conversion between 

help(grid.acc)

## this is what we need:

grid.nearest_cell(snapX, snapY)

## seems like the way to get the total 
## accumulation for a catchment is to 
## clip to it, get a view of the clipped
## accumulation grid, and take the max value:

grid.clip_to('RLCcatch')
gva = grid.view("acc")
drainage = gva.max()
drainage

grid.clip_to('RMCcatch')
gva = grid.view("acc")
drainage = gva.max()
drainage

def getAcc(gr, catchment):
    gr.clip_to(catchment)
    gva = gr.view("acc")
    drainage = gva.max()
    grid.clip_to('dem')
    return(drainage)

getAcc(grid, "RMCcatch")
getAcc(grid, "RLCcatch")



## just checking, where is this pourpoint? Should be the southeast point 
## of LC, but good to check:

grid.clip_to('RMCcatch')
gva = grid.view("acc")
drainage = gva.max()
drainage
maxAc = np.where(gva == gva.max())
yy = maxAc[0].item(0)
xx = maxAc[1].item(0)

fig, ax = plt.subplots()
RMCdem = grid.view("dem")
plt.imshow(RMCdem)
## possible pourpoint here:
ax.plot(xx, yy, 'bo', markersize=10)

## we've also found our two watershed accumulations:

getAcc(grid, "RMCcatch") ## 11576
getAcc(grid, "RLCcatch") ## 15836
## these are the number of cells (30x30m2 each) that drain 
## into these pour points. 

getAcc(grid, "RMCcatch") + getAcc(grid, "RLCcatch") ## 27412 cells in both watershed

getAcc(grid, "RMCcatch") / (getAcc(grid, "RMCcatch") + getAcc(grid, "RLCcatch"))
## Rio Magdalena takes up 42% of their combined area, 

getAcc(grid, "RLCcatch") / (getAcc(grid, "RMCcatch") + getAcc(grid, "RLCcatch"))
## Rio Magdalena takes up 58% of their combined area

## so, works. If we want to go through all of the branch 
## points on our rivers and check their catchment size?

dir(maxAc[0])

grid.clip_to('RLCcatch')

plt.imshow(grid.view('dem'), extent=grid.dem.extent)

#branches = grid.extract_river_network('RLCcatch', 'acc', threshold=50)
## extract_river_network defaults to a catchment size of 100, try this 

grid.clip_to('RLCcatch')
RLCcatchview = grid.view('RLCcatch')
RLCdemview = grid.view('dem')
RLCcatchview.extent ## this is the extent we need.
RLCdemview.extent ## this is the extent we need.
## I think this just means you have to make a view 
## even after clipping, to get your true extent for 
## a sub-watershed...

grid.dem.extent


dir(RLCview)

branches = grid.extract_river_network('RLCcatch', 'acc', threshold=50)

help(grid.extract_river_network)

fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])

ax.imshow(

## don't know why I can't lay the dem raster on this

## river layer extent: 744000 - 749000
## RLC extent:         722546 - 766426

## why is the river layer so small?

## our branch points should be the ends/beginnings of these lines. How to extract?

branches[0]['geometry']['coordinates'][0] ## first point on a line

branches[0]['geometry']['coordinates'][-1] ## last point on a line

X,Y = branches[0]['geometry']['coordinates'][0] ## first point on a line

X,Y = branches[0]['geometry']['coordinates'][-1] ## last point on a line

## function to get the catchment size for each point

def getCatchSize(x,y,grid,watershed, out='tempCatch'):
    grid.clip_to(watershed)
    grid.catchment(data='dir', x=x, y=y, out_name=out,
                       recursionlimit=15000, xytype='label')
    grid.clip_to(out)
    catchsize = grid.view(out).size
    grid.clip_to('dem')
    delattr(grid, out)
    return(catchsize)

def plotEndpoints(num, ax=plt.gca()):
    X1,Y1 = branches[num]['geometry']['coordinates'][0] ## first point in list
    X2,Y2 = branches[num]['geometry']['coordinates'][-1] ## last point in list
    ax.plot(X1,Y1, 'bo', markersize=10)
    ax.plot(X2,Y2, 'ro', markersize=10)

## so this works for plotting the rivers over the watershed:

grid.clip_to('RLCcatch')
fig, ax = plt.subplots()
plt.imshow(grid.view('dem'), extent=grid.view('dem').extent)
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])


plotEndpoints(-4, ax=ax)

## seems like these line segments are actually listed lowest-elevation-to-highest?
## meh, not so clean... so how do we "creep" up this to find our subwatersheds?

## not sure I understand...check the tiny branch...

handX, handY = plt.ginput(1)[0]

snapX, snapY = grid.snap_to_mask(grid.acc > 50, np.array([handX,handY]), return_dist=False)
getCatchSize(snapX, snapY, grid, 'RLCcatch') 


## maybe walking up the rivers isn't the right approach here.
## Bartwell may have already solved a lot of the problem here simply 
## by including a threshold size. We know the total number of pixels
## pouring into our watershed:

getAcc(grid, "RLCcatch") ## 15836

## so if we six sub watersheds of roughly equal size, we should be able to 
## set our threshhold to 15 836 / 6 :

thresh = round( 15836 / 6)

branches = grid.extract_river_network('RLCcatch', 'acc', threshold=thresh)

branches = grid.extract_river_network('RLCcatch', 'acc', threshold=1000)

branches = grid.extract_river_network('RLCcatch', 'acc', threshold=2000)

branches = grid.extract_river_network('RLCcatch', 'acc', threshold=50)

fig, ax = plt.subplots()

for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])

## is there a better way to plot this? can we make a geopanda object?

dir(shapely.geometry.linestring)

shapely.geometry.LineString(branches[0])

grid.clip_to('RLCcatch')
shapes = grid.polygonize()
lsh = list(shapes)

aa = shapely.geometry.LineString(branches[0]['geometry']['coordinates'])

p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')

branches[0]['geometry']['coordinates']

dir(shapely.geometry.multilinestring)

lsh[0][0]['coordinates'][0]

aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])

p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')

p.plot()


ax, fig = plt.subplots()

### start watersheds over, think about problem ###

## let's condense the above, and start over. 

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import random
import geopandas as gpd
import pandas as pd
import scipy.spatial as sp
import pyproj
from pysheds.grid import Grid
import copy
import rasterio
import rasterio.plot
import pprint
import shapely
import geojson
import networkx
from shapely.validation import explain_validity
from sklearn.linear_model import LinearRegression

import pymc3 as pm

import theano 
from theano import shared
import scipy.stats as stats
import arviz as az


sudo apt install libopenblas-base

sudo apt install libopenblas-dev

plt.ion()

### data for plots ###
rioLCWatershedPoly = gpd.read_file('GIS/rioLCwatershed.geojson')
rioMCWatershedPoly = gpd.read_file('GIS/rioMCwatershed.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
smallAstRast = rasterio.open(smallAstpath)
anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
RLCgpd = gpd.read_file("./GIS/rioLCpredicted.geojson")
RMCgpd = gpd.read_file("./GIS/rioMCpredicted.geojson")
mainRLC = gpd.read_file("./GIS/mainRLC.geojson")
mainRMC = gpd.read_file("./GIS/mainRMC.geojson")
RLCCatchPolys = gpd.read_file('GIS/RLCCatchPolys.geojson')
RMCCatchPolys = gpd.read_file('GIS/RMCCatchPolys.geojson')
curatedMicrocuencas = gpd.read_file('GIS/losCedcatchpolys_subjective_closed.geojson')
##---------------##

## get our dem as a grid object for pysheds
smallAstpath = "/home/daniel/Documents/analyses/losCedrosTrees/anaData/GIS/smallAster.tif"
grid = Grid.from_raster(smallAstpath, data_name='dem')
## clean it up as per recommendations by pyshed.
grid.fill_depressions(data='dem', out_name='flooded_dem')
grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
## make our flowdir and accumulation 
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)
grid.accumulation(data='dir', dirmap=dirmap, out_name='acc', apply_mask=False)

XL, YL = (748696, 10032712) ## Rio Los Cedros
XM, YM = (748696, 10032942) ## Rio Magdalena Chico
snapXL, snapYL = grid.snap_to_mask(grid.acc > 50, np.array([XL,YL]), return_dist=False)
snapXM, snapYM = grid.snap_to_mask(grid.acc > 50, np.array([XM,YM]), return_dist=False)
grid.clip_to('dem')
grid.catchment(data='dir', x=snapXL, y=snapYL, out_name='RLCcatch',
                   recursionlimit=15000, xytype='label')
grid.catchment(data='dir', x=snapXM, y=snapYM, out_name='RMCcatch',
                   recursionlimit=15000, xytype='label')

## plot two watersheds we have so far:
rioLCWatershedPoly = gpd.read_file('GIS/rioLCwatershed.geojson')
rioMCWatershedPoly = gpd.read_file('GIS/rioMCwatershed.geojson')
lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
smallAstRast = rasterio.open(smallAstpath)
alpha = {'alpha':0.5}
fig, ax = plt.subplots(figsize=(10,10))
lcPoly.boundary.plot(color='red', linewidth=3, ax=ax)
rioLCWatershedPoly.boundary.plot(ax=ax, color='Green', linewidth= 4)
rioLCWatershedPoly.plot(ax=ax, color='Green', **alpha)
rioMCWatershedPoly.boundary.plot(ax=ax, color='purple', linewidth= 4)
rioMCWatershedPoly.plot(ax=ax, color='Purple', **alpha)
rasterio.plot.show(smallAstRast, ax=ax)
hydro.plot(color='Blue', ax=ax)
ax.set_xlim(lcPoly.bounds.iloc[0,0], lcPoly.bounds.iloc[0,2]+1000)
ax.set_ylim(lcPoly.bounds.iloc[0,1]-300, lcPoly.bounds.iloc[0,3])



grid.clip_to('RMCcatch') ## there it is. Clipping to the 
## subwatershed seems to be very necessary...
branches = grid.extract_river_network('RMCcatch', 'acc')

fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])


grid.clip_to('RLCcatch') 
branches = grid.extract_river_network('RLCcatch', 'acc')
fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])
## yup, fixed it. 

## these can go to geopanda objects, too:

branches = grid.extract_river_network('RLCcatch', 'acc')

aa = shapely.geometry.LineString(branches[0]['geometry']['coordinates'])

## how do we iterate over the geojson and make shapely out of each
## feature?

## how about:

grid.clip_to('RLCcatch') 
branches = grid.extract_river_network('RLCcatch', 'acc')
listOfLSes = [ shapely.geometry.LineString(i['geometry']['coordinates']) for i in branches['features'] ]
RLC_multistring = shapely.geometry.MultiLineString(listOfLSes)
RLCgpd = gpd.GeoSeries(RLC_multistring)
RLCgpd = RLCgpd.set_crs('EPSG:32717')
#RLCgpd.to_file("./GIS/rioLCpredicted.geojson", driver='GeoJSON')

grid.clip_to('RMCcatch') 
branches = grid.extract_river_network('RMCcatch', 'acc')
listOfLSes = [ shapely.geometry.LineString(i['geometry']['coordinates']) for i in branches['features'] ]
RMC_multistring = shapely.geometry.MultiLineString(listOfLSes)
RMCgpd = gpd.GeoSeries(RMC_multistring)
RMCgpd = RMCgpd.set_crs('EPSG:32717')
#RMCgpd.to_file("./GIS/rioMCpredicted.geojson", driver='GeoJSON')

RLCgpd.bounds

streamsClippedRLC = gpd.clip(hydro, RLCgpd.boundary)

RLCgpd.boundary

plt.close('all')

fig, ax = plt.subplots()
streamsClippedRLC.plot(ax=ax)

RLCgpd.plot(ax=ax)

RLCgpd.boundary

streamsClippedRMC = gpd.clip(hydro, RMCgpd)


fig, ax = plt.subplots(1,2, figsize=(10,5))
## RLC
RLCgpd.plot(color='Black', ax=ax[0])
hydro.plot(color='Blue', ax=ax[0])
ax[0].set_xlim(RLCgpd.bounds.minx[0],RLCgpd.bounds.maxx[0])
ax[0].set_ylim(RLCgpd.bounds.miny[0],RLCgpd.bounds.maxy[0])
circCent = (745145, 10034629)
circ = plt.Circle(circCent, radius=300, alpha=0.5, color='red')
ax[0].add_artist(circ)
## RMC
RMCgpd.plot(color='Black', ax=ax[1])
hydro.plot(color='Blue', ax=ax[1])
ax[1].set_xlim(RMCgpd.bounds.minx[0],RMCgpd.bounds.maxx[0])
ax[1].set_ylim(RMCgpd.bounds.miny[0],RMCgpd.bounds.maxy[0])

## so now what? we need to divy up watersheds in a way that make sense. 

## put points on there, and start chopping up the watersheds


RLCgpd.plot(color='Black', ax=ax[0])
hydro.plot(color='Blue', ax=ax[0])
ax[0].set_xlim(RLCgpd.bounds.minx[0],RLCgpd.bounds.maxx[0])
ax[0].set_ylim(RLCgpd.bounds.miny[0],RLCgpd.bounds.maxy[0])

## if we want to divide up the watersheds along the primary 
## tributories, how do we do this?

## the junctions must be the endpoints:

def plotEndpoints(num, ax=plt.gca()):
    X1,Y1 = branches[num]['geometry']['coordinates'][0] ## first point in list
    X2,Y2 = branches[num]['geometry']['coordinates'][-1] ## last point in list
    ax.plot(X1,Y1, 'bo', markersize=10)
    ax.plot(X2,Y2, 'ro', markersize=10)

## maybe the best thing to do is to define the largest branch,
## and get the watersheds of the secondary tributaries 

grid.clip_to('RLCcatch') 
branches = grid.extract_river_network('RLCcatch', 'acc')
fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])

plotEndpoints(71, ax=ax)


## let's see if we can clip away the upper watershed

handX, handY = plt.ginput(1)[0]

handX, handY ## (745877.5533121668, 10034590.348697523)

snapX, snapY = grid.snap_to_mask(grid.acc > 50, np.array([handX,handY]), return_dist=False)

snapX, snapY ## (745862.474054495, 10034604.327270983)

grid.catchment(data='dir', np.array(snapX, snapY), out_name='upperLC_Catch',
                   recursionlimit=15000, xytype='label')

grid.clip_to('upperLC_Catch')
shapes = grid.polygonize()
lsh = list(shapes)
aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
p = gpd.GeoSeries(aa)
p = p.set_crs('EPSG:32717')

#p.to_file("./GIS/rioMCwatershed.geojson", driver='GeoJSON')
## look at it

plt.close('all')
rioLCWatershedPoly = gpd.read_file('GIS/rioLCwatershed.geojson')
fig, ax = plt.subplots()
rioLCWatershedPoly.plot(ax=ax, color='red')
RLCgpd.plot(color='Black', ax=ax)

p.plot(ax=ax, color='blue')

## goal is to define a matrix of watersheds across the extent of study,
## roughly equal size

## starting with RLC watershed, we need the main river stem. 

## this is easily defined for the lower part of the water shed,
## and we don't need the upper. Get the branches for that...

## we can start by defining all the catchments of the primary 
## tributaries on the lower RLC. I'll define the pourpoints manually

## bring up plot of model and observed RLC:

plt.close('all')
fig, ax = plt.subplots()
RLCgpd.plot(color='Black', ax=ax)
hydro.plot(color='Blue', ax=ax)
lcPoly.plot(color='green', ax=ax, alpha=0.5)
anaPt.plot(ax=ax, color='purple')
ax.set_xlim(RLCgpd.bounds.minx[0],RLCgpd.bounds.maxx[0])
ax.set_ylim(RLCgpd.bounds.miny[0],RLCgpd.bounds.maxy[0])

coords = plt.ginput(n=-1, timeout=0)

## add a couple more
coords.append(plt.ginput(n=-1, timeout=0))

## get colorset to match

coords_snapped = grid.snap_to_mask(grid.acc > 50, coords, return_dist=False)

def getCatchGPD(grid, coords):
    grid2 = copy.deepcopy(grid)
    snapX, snapY = grid.snap_to_mask(grid.acc > 50, coords, return_dist=False)
    grid2.catchment(y=snapY, x=snapX, data='dir', out_name=('littleCatch'),
                       recursionlimit=15000, xytype='label')
    grid2.clip_to('littleCatch')
    shapes = grid2.polygonize()
    lsh = list(shapes)
    aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
    p = gpd.GeoSeries(aa)
    p = p.set_crs('EPSG:32717')
    return(p)

## use this function to make a polygon-based geoseries of our catchments
rlccatchpolys = [ getCatchGPD(grid, i).geometry[0] for i in coords ]
RLCCatchPolys = gpd.GeoSeries(rlccatchpolys, crs='EPSG:32717')

## get plot colors
Xcolors = [ random.choice(list(mcolors.XKCD_COLORS.values())) for i in range(len(coords)) ] 

plt.close('all')

## summary plot so far

fig, ax = plt.subplots()
RLCCatchPolys.plot(ax = ax, cmap='Dark2')
RLCgpd.plot(color='Black', ax=ax)
hydro.plot(color='Blue', ax=ax)
anaPt.plot(color='green', ax=ax)
x, y = zip(*coords)
ax.plot(x, y, "bo")
ax.set_xlim(RLCgpd.bounds.minx[0],RLCgpd.bounds.maxx[0])
ax.set_ylim(RLCgpd.bounds.miny[0],RLCgpd.bounds.maxy[0])
riverCorridor.plot(ax=ax, alpha=0.5) ## created below
for i,_ in enumerate(RLCCatchPolys):
    ax.text(coords[i][0], coords[i][1], str(i))

RLCCatchPolys.area

## actually, that is starting to look good...

## a lot of error, though. how do we reconcile this? it starts to matter, if we are trying to
## create polygons of roughly equal size...
## seems like maybe the dem isn't meant for this local of a delineation.
## it's what we got, go with it.

### define main stem of RLC

## our main stem up until the waterfall is pretty obvious. How do we extract this as some sort of 
## geoseries?

breakat = plt.ginput(n=1, timeout=0)
breakat_snapped = grid.snap_to_mask(grid.acc > 50, breakat[0], return_dist=False)

## get the main stem of the river by upping the accumumlation threshold
grid.clip_to('RLCcatch') 
branch = grid.extract_river_network('RLCcatch', 'acc', threshold=3000)
line = np.asarray(branches['features'][0]['geometry']['coordinates'])
## get rid of rows north of break point:
masc = line[:,1] < breakat_snapped[1]
lineSouth = line[masc,:]
## make this into a geoseries
shLine = shapely.geometry.LineString(lineSouth)
mainRLC = gpd.GeoSeries(shLine)
mainRLC = mainRLC.set_crs('EPSG:32717')

#mainRLC.to_file('GIS/mainRLC.geojson', driver='GeoJSON')
## buffer it 
riverCorridor = mainRLC.buffer(200)

## take intersection of buffer and tributaries as pourpoints

## 

## looks like trying to catch the small tributary south of the LC campus failed, just catches the next one over.  
## if we go with the buffering plan, this little catchment will probably mostly get swallowed. 

RLCgpd
riverCorridor

aa = RLCgpd.difference(riverCorridor)


riverCorridor.geometry[0].boundary.coords

dir(RLCgpd.geometry)

len(RLCgpd.geometry[0])

list(RLCgpd.geometry[0][0].coords)

bb = RLCgpd.intersection(riverCorridor)

## our intersection should have the points of intersection 
## as the ends of the line segments, no?

## just the first line segment
bb.geometry[0][0].coords

line = np.asarray(bb.geometry[0][0].coords)

plt.close('all')
fig, ax = plt.subplots()
ax.plot(line[:, 0], line[:, 1], color='red')

bb.plot(ax = ax)

## okay, the endpoint of this should be a common
## coordinate between both the line segment and the
## buffered polygon:

cc = list(bb[0][0].coords) ## our line segment coords, as x,y tuples

dd = list(riverCorridor.geometry[0].boundary.coords) ## our river buffer, as x,y tuples

plt.close('all')
riverbuffoutline = np.asarray(dd)
fig, ax = plt.subplots()
ax.plot(riverbuffoutline[:, 0], riverbuffoutline[:, 1], color='red')

[ i in dd for i in cc ] ## all false - no shared points?

sum([ i in cc for i in dd ] ) ## also all false. 

## so that method doesn't work. I am thinking about this wrong way.

## how about the intersections between the boundary line string of the 
## polygon and the river line segments? 

plt.close('all')
fig, ax = plt.subplots()
RLCgpd.plot(ax=ax)
riverCorridor.boundary.plot(ax=ax)

dir(riverCorridor.boundary)

aa = riverCorridor.boundary.intersection(RLCgpd)
aa.plot(ax=ax, color='green', markersize=10)

## much better. can we use these as the pour points for our catchments?
## actually, I think we will want to river buffer polygons: one for 
## the riparian zone around the river that we are calling its own watershed,
## and another, tighter buffer that defines the pour points. 

## so the tighter one, for pourpoints. Start with 50 m?  Might be too tight...let's see...

pourPtsPoly = mainRLC.buffer(50)

plt.close('all')

fig, ax = plt.subplots()
RLCgpd.plot(ax=ax)

pourPts = pourPtsPoly.boundary.intersection(RLCgpd)
pourPts = pourPts.explode()
pourPts.reset_index(inplace=True, drop=True)
pourPts.plot(ax=ax, color='green', markersize=10)
mainRLC.plot(ax=ax, color='purple')

## use these as pourpoints, repeating the above pipeline.
## we're going to need to break some of these catchments up
## but for now, start with these


## get plot colors
Xcolors = random.choices(list(mcolors.XKCD_COLORS.values()), k=len(pourPts))
cmap = mcolors.ListedColormap(Xcolors)

## get our catchments as a geoseries, using getCatch function defined above,
## and our new pourpoints:

grid2 = copy.deepcopy(grid)

coords2 = [ i.coords[0] for i in pourPts ]


rlccatchpolys = [ getCatchGPD(grid, i).geometry[0] for i in coords ]
RLCCatchPolys2 = gpd.GeoSeries(rlccatchpolys2, crs='EPSG:32717')
## looks like that works

## do our summary plot again:

gpd.read_file("./GIS/mainRMC.geojson")

plt.close('all')

fig, ax = plt.subplots()
RLCCatchPolys2.plot(ax = ax, cmap=cmap)
RLCgpd.plot(color='Black', ax=ax)
hydro.plot(color='Blue', ax=ax)
anaPt.plot(color='green', ax=ax)
x, y = zip(*coords2)
ax.plot(x, y, "bo")
mainRLC.plot(ax=ax, color='purple', linewidth=5)

ax.set_xlim(RLCgpd.bounds.minx[0],RLCgpd.bounds.maxx[0])
ax.set_ylim(RLCgpd.bounds.miny[0],RLCgpd.bounds.maxy[0])
riverCorridor.plot(ax=ax, alpha=0.5) 
for i,_ in enumerate(RLCCatchPolys):
    ax.text(coords[i][0], coords[i][1], str(i))

## looking better everytime...
RLCCatchPolys.area

## all watersheds are at least in the same area of 
## magnitude, except the river corridor, which we
## are going to chop up anyway. 

## can we clean up the pipeline and repeat for the 
## RMC watershed? 

## take a break after that and write the humboldt,
## then 

## where to start... the notebook is up to the river 
## extraction point for both RLC and RMC. 

## get main stem of the river, both catchments:

## RLC main stem, lower
grid.clip_to('RLCcatch') 
branch = grid.extract_river_network('RLCcatch', 'acc', threshold=3000)
line = np.asarray(branch['features'][0]['geometry']['coordinates'])

## get our most upstream point (for RLC, +/- at the old waterfall)
fig, ax = plt.subplots()
RLCgpd.plot(color='Black', ax=ax)
hydro.plot(color='Blue', ax=ax)
#breakat = plt.ginput(n=1, timeout=0)
breakat_snapped = grid.snap_to_mask(grid.acc > 50, breakat[0], return_dist=False)

breakat_snapped ## 745893.39740855, 10033958.6773012

masc = line[:,1] < breakat_snapped[1]
lineSouth = line[masc,:]

## make this into a geoseries
shLine = shapely.geometry.LineString(lineSouth)
mainRLC = gpd.GeoSeries(shLine)
mainRLC = mainRLC.set_crs('EPSG:32717')

## RMC main stem, lower

## other than a large tributary to east that is outside
## the study area

plt.close('all')

grid.clip_to('RMCcatch') 

branch = grid.extract_river_network('RMCcatch', 'acc', threshold=3000)
line = np.asarray(branch['features'][0]['geometry']['coordinates'])

## get breakpoint
fig, ax = plt.subplots()
RMCgpd.plot(color='Black', ax=ax)
hydro.plot(color='Blue', ax=ax)

breakat = plt.ginput(n=1, timeout=0)
breakat_snapped = grid.snap_to_mask(grid.acc > 50, breakat[0], return_dist=False)

breakat_snapped ## array([  746975.71480033, 10035711.15579062])

masc = line[:,1] < breakat_snapped[1]
lineSouth = line[masc,:]

## make this into a geoseries

shLine = shapely.geometry.LineString(lineSouth)

mainRMC = gpd.GeoSeries(shLine)
mainRMC = mainRMC.set_crs('EPSG:32717')

mainRMC = gpd.read_file('GIS/mainRMC.geojson')

## check tributaries for RMC, don't think there were many that
## qualified:

## pour point for each tributary:
pourPtsPoly = mainRMC.buffer(50)

fig, ax = plt.subplots()
pourPtsPoly.plot(ax=ax)
RMCgpd.plot(ax=ax, color='green')

pourPts = pourPtsPoly.boundary.intersection(RMCgpd)
pourPts = pourPts.explode()
pourPts.reset_index(inplace=True, drop=True)

## make subwatersheds:
coords = [ i.coords[0] for i in pourPts ]
grid.clip_to('dem')

## this can be a little memory intensive:
rmccatchpolys = [ getCatchGPD(grid, i).geometry[0] for i in coords ]
RMCCatchPolys = gpd.GeoSeries(rmccatchpolys, crs='EPSG:32717')
RMCCatchPolys.to_file('GIS/RMCCatchPolys.geojson', driver='GeoJSON')

def genXKCDColors(nu):
    Xcolors = random.choices(list(mcolors.XKCD_COLORS.values()), k=nu)
    cmap = mcolors.ListedColormap(Xcolors)
    return(cmap)

## look at the RMC subwatersheds
fig, ax = plt.subplots()
cmap=genXKCDColors(len(RMCCatchPolys))
RMCCatchPolys.plot(ax=ax, cmap=cmap)
cmap=genXKCDColors(len(RLCCatchPolys))
RLCCatchPolys.plot(ax=ax, cmap=cmap)
RMCgpd.plot(ax=ax)
RLCgpd.plot(ax=ax)

## do we need a corridor?
rlcCorridor = mainRLC.buffer(200)
rmcCorridor = mainRMC.buffer(200)
rlcCorridor.plot(ax=ax, alpha=0.5)
rmcCorridor.plot(ax=ax, alpha=0.5)


anaPt.plot(ax=ax, color='black') 

##########################################

## because there is no easy way to keep these pyshed objects
## from session to session, so rebuild  

## okay, so now what?

## so, we want to be able to categorize the sample sites by water shed. 
## as is, are all points are included in a watershed? 

## I think it is time to hand curate the watersheds

## or not. Not sure how to split these large watersheds 

## seems like we need some sort of system for declaring a new microwatershed.

## first rule seems to be, if a flowing stream comes off the main stem, 

## where did we make the calls for whether 

smallAstpath = "/home/daniel/Documents/analyses/losCedrosTrees/anaData/GIS/smallAster.tif"
grid = Grid.from_raster(smallAstpath, data_name='dem')
grid.fill_depressions(data='dem', out_name='flooded_dem')
grid.resolve_flats(data='flooded_dem', out_name='inflated_dem')
dirmap = (64,  128,  1,   2,    4,   8,    16,  32)
grid.flowdir(data='inflated_dem', out_name='dir', dirmap=dirmap)
grid.accumulation(data='dir', dirmap=dirmap, out_name='acc', apply_mask=False)
XL, YL = (748696, 10032712) ## Rio Los Cedros
XM, YM = (748696, 10032942) ## Rio Magdalena Chico
snapXL, snapYL = grid.snap_to_mask(grid.acc > 50, np.array([XL,YL]), return_dist=False)
snapXM, snapYM = grid.snap_to_mask(grid.acc > 50, np.array([XM,YM]), return_dist=False)
grid.clip_to('dem')
grid.catchment(data='dir', x=snapXL, y=snapYL, out_name='RLCcatch',
                   recursionlimit=15000, xytype='label')
grid.catchment(data='dir', x=snapXM, y=snapYM, out_name='RMCcatch',
                   recursionlimit=15000, xytype='label')
grid.clip_to('RLCcatch')
branches = grid.extract_river_network('RLCcatch', 'acc')

## looks like there is a default threshhold of 100 cells, which 
## how do we use this to find our flow accumulation, it fit our 
## observed rivers pretty well. 

## but what now? play around with splitting the water sheds, find out 
## what our "ideal" size might be, and be consistent

## so what we want here is a point and click function to get relative 
## size of 

## bring up our river system as is, and funnel clicks through a cell calculator

fig, ax = plt.subplots()
for branch in branches['features']:
    line = np.asarray(branch['geometry']['coordinates'])
    ax.plot(line[:, 0], line[:, 1])

## we need to be able pick a point with a mouse, and make a catchment

## we have the above function getCatchGPD, can we raise it from the dead for a
## moment 

## what did it do? 

## takes the grid object, a set of coords, spit out a geoseries with a polygon for 
## a sub-watershed  

## for instance:

#grid.clip_to('dem')

grid.clip_to('RLCcatch')

grid.size

grid.shape

coords = [748696, 10032712] ## Rio Los Cedros
aa = getCatchGPD(grid, coords)

aa.area 
## this agrees with the qgis area calculations...

grid.cellsize
## this sort of makes sense, I think the pixels from the dem are 
## 30m on a side. 

grid.size

aa.area / grid.cellsize**2

## that doesn't make sense to me, the grid size and 

## anyway, we can use the area output as our metric for 
## splitting, but we need to think in terms of these 
## actual areas, a bit far from the pysheds. 

## I think we need a max cutoff. We have a min, sort of,
## in that tributary that touches the main rivers as observed on our topo
## is given its own watershed. But we need a max size, for the 
## stream complexes, to define our neighborhoods and get our 
## adjaceny

## how can we make this useful for splitting up the watersheds? 
## make it report the number of cells, can this be 
## held as info in the geoseries?

## hmm, looks like we're going to have to make a geopanda? this could mess up our previous code...
## so let's not mess with  getCatchGPD()
## just build something else with it. 

## combine our catchements 

catches = 

RLCCatchPolys

RMCCatchPolys

aa = pd.concat([RLCCatchPolys,RMCCatchPolys])
aa.reset_index(inplace=True, drop=True)

## and what do we want here? to detect which are above a certain 
## threshold?

## this would really be better done in pysheds, in terms of cells

## maybe a new function, that generates pyshed grid objects on the fly,
## that we check for size 

## basically break this function up a little:

## make a duplicate grid with the sub catchment
def makeCheckCatches(grid, coords):
    grid2 = copy.deepcopy(grid)
    snapX, snapY = grid.snap_to_mask(grid.acc > 50, coords, return_dist=False)
    grid2.catchment(y=snapY, x=snapX, data='dir', out_name=('littleCatch'),
                       recursionlimit=15000, xytype='label')
    grid2.clip_to('littleCatch')
    return(grid2)

## create geopandas series from this
def makegeo(grid, catchname='littleCatch'):
    grid.clip_to(catchname)
    shapes = grid.polygonize()
    lsh = list(shapes)
    aa = shapely.geometry.polygon.Polygon(lsh[0][0]['coordinates'][0])
    p = gpd.GeoSeries(aa)
    p = p.set_crs('EPSG:32717')
    return(p)

## get coords of a matplot map
def getCoordClick():
    return(list(plt.ginput(1)[0]))

## try these out:

## plot figure first 

RLCgpd.plot()

coords = getCoordClick()


aa = makeCheckCatches(grid, coords)
bb = makegeo(aa)

## 
fig, ax = plt.subplots()
bb.plot(ax=ax)

## this should create a grid object with view already set to sub watershed of 
## interest

## so we need to pick a threshold max size, and any of the subcuencas that exceeds it, 
## we need to break up into smaller watersheds.  

## our existing watersheds are:

fig, ax = plt.subplots()
autoCuencas = (pd.concat([RLCCatchPolys,RMCCatchPolys]).
                reset_index(drop=True))
colmap = genXKCDColors(autoCuencas.shape[0])
autoCuencas.plot(ax=ax, cmap=colmap)


## ugh. when is a watershed "adjacent"? 
## seems like when they share a ridgeline. 
## what is causing the problems here are the rivers. 
## cuencas that are on the same side are obvious.
## but the central, shared riparian areas cause all the 
## confusion. If you treat the river margins are a single
## continuous watershed, it unites all the subcuencas so 
## are all just two steps away from each other.
## if you break it up, it is highly subjective and/over
## inflates the steps between water sheds. 
## eliminating it (bring all the subwatersheds into 
## continuity with no river buffer) also sometimes 
## unites watersheds that are otherwise distance to 
## each other. 
## the basic hypothesis is that dispersal across watersheds
## can be difficult. 
## comes back to a max size cutoff, because I think the 
## best solution here is generate a river buffer and then divide 
## up evenly to max polygon size chunks.  

## so what is a good max?

## our golden area is the south, where the watersheds are +/- regularly spaced 
## and sized. Can we get an average size from these?
 
## I think when we get these, we'll just need the shapely polygon.touches method to make an 
## adjacency matrix

## here's a draft set of rules:

## base set of watershed polygons are set as in the notebook:

## 0. a model of the los cedros rivers was generated using the pysheds package.
## from this model, 

## 1. initially every tributary that directly enters the Rio Los Cedros
## and Rio Magdalena Chico from the south/west side gets its own watershed,
## within a small buffer of the 

## 2. we only go as far as the waterfalls, roughly. Meaning that for now the 
## watersheds of the upper los cedros are left alone. They are too big, but 
## it doesn't matter because, only their lower regions are important for the 
## the study.

## 3. these predicted watersheds are hand-curated, to match observed actual river 
## topology. There were some errors in the predicted river flows.

## 3. where significant sub-watersheds remain hidden,  we creep up
## the buffer size from the stream to split them. As in, the first candidate 
## for splitting is from the "bottom-up" of a stream. The first, and only,
## splitting event of this type occurred by increasing the river buffer from 
## 50m to 200m, resulting in a new sub-watershed in the north central portion of 
## the RLC watershed

## 4. We try to eliminate the river buffer, as in it does not get its own 
## watershed status, and other watersheds are expanded within reason to the 
## river. This is to bring watersheds that are directly across from each other
## on the rivers. But this is not forced beyond a reasonable extent, 
## such that we do not attempt to cover all surface area of the map, there
## are blank areas allowed that do not belong to any watershed. Technically
## these are tiny microcuencas that feed directly into the river. 

## Also, some of these microcuencas areas are included in adjacent sub-watersheds 
## of which they are not technically tributaries. I think? Did I do this?

## the magdalena chico is really tough. It is one long strip of really steep mountainside,
## with a few small streams. There are some large flats that are pasture, and some 
## really steep hillsides. If we consider it one continuous watershed, this 
## seems incongruous with our microcuencas carved from the RLC...

## not sure. for the moment, I have basically considered the RMC a single
## watershed, but broken it apart at an especially steep cliff. I also took the 
## liberty of including the points in the southeast portion of Ana's study, 
## even though they are across the RMC. Some inconsistencies, but I think they
## are in the spirit of testing the general idea that it is hard to get around
## in the tropical Andes, slowing dispersal. 

########## adjacency matrix for watersheds ############

## and now, it looks like we need to get an adjacency matrix. I don't see a ready
## made function or package for this in my usual libraries. Shouldn't be too 
## hard to code with shapely or geopandas...famous last words...

curatedMicrocuencas.plot()

aa = gpd.GeoSeries(curatedMicrocuencas.iloc[0])
bb = gpd.GeoSeries(curatedMicrocuencas.iloc[1])

fig, ax = plt.subplots()
aa.plot(ax=ax, color='blue')
bb.plot(ax=ax, color='red')

aa.touches(bb) ## no. why not?

aa.intersects(bb) ## true. 

aa.overlaps(bb) ## true. why? They should be complementary, share boundaries but not internal space

## where is the intersection?

cc = aa.intersection(bb)


cc.plot(ax=ax, color='yellow')

cc.area

aa.touches(bb).geometry

aa.intersects(bb).geometry


## is this going to cause problems at corners etc.? probably not, we have pretty coarse, obvious polygons

## we're going to be using NetworkX, <https://networkx.org/documentation/stable/index.html>
## looks like that package accepts dictionaries of descriptors. 
## So let's just march forward, let's make an adjacency matrix, maybe as a numpy array?

## to make on row, take one polygon and iterate it through all the others:

def checkAdjacency(microcuencaA, microcuencaB):
    '''use two, single row geoseries'''
    if microcuencaA[0].touches(microcuencaB[0]):
        a=True
    elif microcuencaA[0].intersects(microcuencaB[0]):
        a=True
    elif microcuencaA[0].overlaps(microcuencaB[0]): 
        a=True
    else: a=False
    return(a)

checkAdjacency(aa, bb)

curatedMicrocuencas

ee = gpd.GeoSeries(curatedMicrocuencas.iloc[10])

ee.plot(ax=plt.gca(), color='red')

checkAdjacency(aa, ee)

checkAdjacency(ee, aa)

## right along their boundary. Probably some sort of projection issue, or
## minor mistake with the digitizing. 

## anyway, don't think it matters as long as we can establish some sort of 
## contact, and prevent false positives

## is there a way to do this recursively?

i=0
checkAdjacency(curatedMicrocuencas.iloc[i], curatedMicrocuencas.iloc[i+10])


for j in curatedMicrocuencas.index: 
    print(j)
    [ checkAdjacency(curatedMicrocuencas.iloc[i], curatedMicrocuencas.iloc[j]) for i in curatedMicrocuencas.index ]
    
for i in curatedMicrocuencas.index:
    print(i)
    checkAdjacency(curatedMicrocuencas.iloc[i], curatedMicrocuencas.iloc[j])

curatedMicrocuencas

curatedMicrocuencas.is_valid

## breakdown here:
i=8
j=6
checkAdjacency(curatedMicrocuencas.iloc[i], curatedMicrocuencas.iloc[j])

dir(curatedMicrocuencas.iloc[i][0])

curatedMicrocuencas.iloc[i][0].is_valid ## nope

curatedMicrocuencas.iloc[j][0].is_valid ## yup

## hunt down the invalidities
i=8

explain_validity(curatedMicrocuencas.iloc[i][0]) ## self-intersection

curatedMicrocuencas.iloc[i].plot()

#i=8 ## fixed
#i=15 ## fixed
explain_validity(curatedMicrocuencas.iloc[i][0]) ## self-intersection
fig, ax = plt.subplots()
curatedMicrocuencas.plot(ax=ax)
gpd.GeoSeries(curatedMicrocuencas.iloc[i]).plot(ax=ax, color='red')
plt.scatter(746450.017781465, 10035311.4677141, c='blue')

## okay, all are now valid, try again:  

lOfadj = []
for j in curatedMicrocuencas.index: 
    print(j)
    lOfadj.append(
        [ checkAdjacency(curatedMicrocuencas.iloc[i], curatedMicrocuencas.iloc[j]) for i in curatedMicrocuencas.index ]
        )

## so this should be our adjaceny matrix:
adjMat = pd.DataFrame(lOfadj)


## do we need to change this to 0s and 1s?

## can we do a sanity check here? show map:

fig, ax = plt.subplots(1,2)
colmap = genXKCDColors(curatedMicrocuencas.shape[0])
curatedMicrocuencas.plot(ax=ax[0], cmap=colmap)
for i in curatedMicrocuencas.index:
    ax[0].annotate(s=i,
                xy=curatedMicrocuencas.loc[i][0].centroid.coords[0],
                )

## looks promising, spotchecks work out. 

## what's next? get this into a format that NetworkX can handle.

## NetworkX can solve our problems
## if so do it, update notebook

## what does networkX need? Our nodes are the polygons, and edges 
## are physical adjancencies

## graph object:

mapG = networkx.graph()

## nodes

mapG.add_nodes_from(range(1,17))

list(range(1,17))

mapG.nodes ## looks like it worked

## but can we do this directly from our adjacency matrix

help(networkx.from_pandas_adjacency)

gMicrocuencas = networkx.from_pandas_adjacency(adjMat)
print(networkx.info(gMicrocuencas))
layout = networkx.spring_layout(gMicrocuencas)

fig, ax = plt.subplots(1,2)
colmap = genXKCDColors(curatedMicrocuencas.shape[0])
curatedMicrocuencas.plot(ax=ax[0], cmap=colmap)
for i in curatedMicrocuencas.index:
    ax[0].annotate(s=i,
                xy=curatedMicrocuencas.loc[i][0].centroid.coords[0],
                )

## reuse our colormap 
networkx.draw(gMicrocuencas, 
              layout, 
              with_labels=True, 
              node_color=colmap.colors,
              ax=ax[1])

## okay, nice, can we get a table of shortest paths? djikstra's algorithm 
## is readymade in networkx:

help(networkx.dijkstra_path_length)

networkx.dijkstra_path_length(gMicrocuencas, 0, 5)

networkx.dijkstra_path_length(gMicrocuencas, 0, 13)

## great, so we can use this to make a distance matrix, I think...

[ i for i in range(17) ]

jumps = []
for j in range(17):
    jumps.append([ networkx.dijkstra_path_length(gMicrocuencas, i, j) for i in range(17) ])

watershedDistMat = pd.DataFrame(jumps)


help(watershedDistMat.to_pickle)

#watershedDistMat.to_csv('watershedDistMat.csv')

watershedDistMat = pd.read_csv('watershedDistMat.csv', header=0, index_col=0) 
watershedDistMat.columns = watershedDistMat.columns.astype('int')

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')

## great! how do we use this for a turnover diagram?

## well, this isn't quite our distance matrix. We need a sample x sample
## comparison. So 

## and we need to make sure this is in the formats needed for vegan. 

anaPt

## start on this tomorrow, update notebook for now.  

## and so now we need to create a function that finds the number of 
## watershed jumps between any two sample sites. 

curatedMicrocuencas

curatedMicrocuencas.iloc[0][0]

anaPt.within(curatedMicrocuencas.iloc[0])

anaPt.within(curatedMicrocuencas.iloc[0][0]).any()

anaPt.within(curatedMicrocuencas.iloc[13][0]).any()

## given a point, how do we get it's microcuenca?

## our unique ID is the PsubP
aa = anaPt.set_index('PsubP')
bb = aa.geometry.apply(lambda x: curatedMicrocuencas.contains(x))

## looks like that worked. Do all of the sample sites belong to one,
## and just one, polygon?

(bb.sum(axis=1) == 1).all()
## looks right.  

## simplify:
cc = pd.melt(bb, var_name='microcuenca', ignore_index=False) 
dd = (cc[cc.value]
        .drop('value', axis=1).sort_index())

## we might want to add this to our environmental matrix

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
#envOnly['microcuenca'] = dd['microcuenca']
#envOnly.to_csv('envOnly.csv')

## so now, how to make a "hops" distance matrix between sites?

psubp1 = 1.2
psubp2 = 7.7

microcuenca1 = envOnly.loc[psubp1,'microcuenca']

microcuenca2 = envOnly.loc[psubp2,'microcuenca']

type(microcuenca1)

envOnly.loc[psubp2,'microcuenca']

[microcuenca1, microcuenca2]

[str(microcuenca1), str(microcuenca2)]


watershedDistMat.loc[str(microcuenca1), str(microcuenca2)]

## huh, this works. 
watershedDistMat.loc[microcuenca1, microcuenca2]
## we want our columns names to be integers, not strings

watershedDistMat.loc['9','13']

psubpHops = []
for i in envOnly.microcuenca:
    psubpHops.append(
        [ watershedDistMat.loc[i, j] for j in envOnly.microcuenca ]
    )

psubpHopsDF = pd.DataFrame(psubpHops, index=envOnly.index, columns=envOnly.index)


## spotchecks look good. save it:

psubpHopsDF.to_csv('psubpHopsDF.csv')

#####################

help(pd.read_csv)

psubpHopsDF = pd.read_csv('psubpHopsDF.csv', index_col='PsubP')

## can we use this to do a turnover graph? 

## we need to use the above as a physical distance matrix
## and then we need a bray-curtis distance for ecological distance

subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')

bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
## not sure how these outputs work. Muck around with numpy triangular matrices

aa = sp.distance.squareform(bcDist)


bb = np.array([[0,1,2],[3,4,5],[6,7,8]])
np.triu(bb, k=0)

help(np.triu)

help(np.triu_indices)

np.triu_indices(bb, k=0)

cc = bb[np.triu_indices(bb.shape[0], k = 0)]

## okay, so can we check the outputs of pdist this way? 

dd = aa[np.triu_indices(aa.shape[0], k=1)]
len(bcDist)
len(dd)

## they match. so likely the output of the pdist function 
## is a triangular matrix with no diagonal. 
## I don't know if they use the upper or lower triangle,
## don't think it matters, but let's see:

dd[0] == bcDist[0]

## upper, I guess, or it doesn't matter

## so we need the upper triangle of our physical
## distance matrix:

psubpHopsNP = np.array(psubpHopsDF)
psubpHopsTriU = psubpHopsNP[np.triu_indices(psubpHopsNP.shape[0], k=1)]

len(psubpHopsTriU)
len(bcDist)

## I guess we can just make a model from these:

X, Y = psubpHopsTriU.reshape(-1,1), bcDist.reshape(-1,1)
fig, ax = plt.subplots()
ax.scatter(psubpHopsTriU, bcDist)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title('Overall Turnover')

## this would be better shown as a violin or box plot at each distance.
## how can we do this?

## the violin of one bin (hops = 1):

aa = X[X == 1]
bb = Y[X == 1]

fig, ax = plt.subplots()
ax.violinplot(bb)

## but we want to do this for all the physical distance bins

X, Y = psubpHopsTriU.reshape(-1,1), bcDist.reshape(-1,1)

## can we do this with series instead of np arrays?

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)


plt.close('all')
aa = [ YS[XS == i].to_list() for i in np.unique(psubpHopsTriU) ] 

[ np.mean(i) for i in aa ]

fig, ax = plt.subplots()
## that works...add points?
XSjitted = (XS
            .apply(np.random.normal, args=(0.05,))
            .apply( lambda x: x+1)
            )
ax.scatter(XSjitted, YS, alpha=0.15, color='red')
ax.violinplot(aa, showmeans=True)
ax.set_xticklabels(['',0,1,2,3,4])
ax.set_title('Overall Microcuenca Turnover')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_xlabel('distance in watershed hops')

## looks okay. Can we draw lines between the means?


## and the sub categories would be better modeled as a
## hierarchical model of landuse types and disturbance
## types (anthropogenic vs. natural). 

## after that, the MEM analysis needs to be looked at again. 
## it's not that useful

### subdivide by habitat types ###

psubpHopsDF = pd.read_csv('psubpHopsDF.csv', index_col='PsubP')
subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')

## we have an overall model of regression 

## but we need to give each type of habitat the ability to 
## have its own rate of turnover (B1 coefficient). 

## first separate and visualize

## how do we divide the above community data? 

subParcelComm.head()

## make a dictionary, keys are habitat types and values are lists of PsubP sample names

habGroups = envOnly.groupby('habitat')

habGroups.groups
## huh, the groupby object looks like a dictionary...can we treat it that way?

envOnly.loc[ habGroups.groups['BC'] ]

subParcelComm.loc[ habGroups.groups['BC'] ]

## seems so...

## for each group we need to construct a turnover diagram, 
## so repeat the above pipeline for each submatrix:

i = 'RCA'
## physical distance
psubpHopsDF_i = psubpHopsDF.loc[ habGroups.groups[i], habGroups.groups[i].astype('str') ]
psubpHopsNP_i = np.array(psubpHopsDF_i)
psubpHopsTriU_i = psubpHopsNP_i[np.triu_indices(psubpHopsNP_i.shape[0], k=1)]

## bc distance
subParcelComm_i = subParcelComm.loc[ habGroups.groups[i] ]
bcDist_i = sp.distance.pdist(subParcelComm_i, metric='brayCurtis')

## plot
X, Y = psubpHopsTriU_i.reshape(-1,1), bcDist_i.reshape(-1,1)
fig, ax = plt.subplots()
ax.scatter(X, Y)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title(i)

psubpHopsDF.index

psubpHopsDF_i
## oops, only subsetting rows, not columns....does this matter?

## apply this to all groups

## plotting function for old turnover diagrams
def oldTurnPlot(physDist, ecoDist, ax, tit=None):
    ax.scatter(physDist, ecoDist)
    X, Y = physDist.reshape(-1,1), ecoDist.reshape(-1,1)
    ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
    ax.set_title(tit)

plt.close('all')
psubpHopsTriU_byHab = []
bcDist_byHab = []
fig, axes = plt.subplots(2,3, sharey=True, sharex=True )
axes = axes.flatten()
for nu,i in enumerate(habGroups.groups.keys()):
    print('i='+i)
    print('nu='+str(nu))
    ## physical distance
    psubpHopsDF_i = psubpHopsDF.loc[ habGroups.groups[i], habGroups.groups[i].astype('str') ]
    psubpHopsNP_i = np.array(psubpHopsDF_i)
    psubpHopsTriU_i = psubpHopsNP_i[np.triu_indices(psubpHopsNP_i.shape[0], k=1)]
    psubpHopsTriU_byHab.append(psubpHopsTriU_i)
    ## bc distance
    subParcelComm_i = subParcelComm.loc[ habGroups.groups[i] ]
    bcDist_i = sp.distance.pdist(subParcelComm_i, metric='brayCurtis')
    bcDist_byHab.append(bcDist_i)
    ## plot it
    #oldTurnPlot(physDist=psubpHopsTriU_i, 
    #            ecoDist=bcDist_i, 
    #            ax=axes[nu], 
    #            tit=i,
    #                )
    violinTurnPlot(physDist=psubpHopsTriU_i, 
                ecoDist=bcDist_i, 
                ax=axes[nu], 
                tit=i,
                    )

## plotting function for new-style turnover diagrams
fig, ax = plt.subplots(figsize=(10,10))

def violinTurnPlot(physDist, ecoDist, ax, tit=None):
    XS, YS = pd.Series(physDist), pd.Series(ecoDist)
    aa = [ YS[XS == i].to_list() for i in np.unique(physDist) ]
    ## that works...add points?
    XSjitted = (XS
                .apply(np.random.normal, args=(0.03,))
                .apply( lambda x: x+1)
                )
    ax.scatter(XSjitted, YS, alpha=0.15, color='red')
    ax.violinplot(aa, showmeans=True)
    _=ax.set_xticklabels(['',0,1,2,3,4])
    ax.set_title(tit)


plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))

aa = violinTurnPlot(physDist=psubpHopsTriU_byHab[0], 
            ecoDist=bcDist_byHab[0], 
            ax=ax, 
            tit=None)

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)
aa = [ YS[XS == i].to_list() for i in np.unique(psubpHopsTriU) ]
fig, ax = plt.subplots(figsize=(10,10))
## that works...add points?
XSjitted = (XS
            .apply(np.random.normal, args=(0.03,))
            .apply( lambda x: x+1)
            )
ax.scatter(XSjitted, YS, alpha=0.15, color='red')
ax.violinplot(aa, showmeans=True)
_=ax.set_xticklabels(['',0,1,2,3,4])
ax.set_title('Overall Microcuenca Turnover')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_xlabel('distance in watershed hops')
[ np.mean(i) for i in aa ]

## can we build a model? 
## I think we should work the one pertinent example I have on hand for this.
## Martin 2018 has an example of a non-linear function, with variable 
## with variance?

dataPath=(
"/home/daniel/Documents/learn/Bayesian/martinBayesianAnalysis/"
"Bayesian-Analysis-with-Python-Second-Edition/data/babies.csv"
)

data = pd.read_csv(dataPath)

data.plot.scatter('Month', 'Lenght')

data.head()

with pm.Model() as model_vv:

α = pm.Normal('α', sd=10)
β = pm.Normal('β', sd=10)
γ = pm.HalfNormal('γ', sd=10)
δ = pm.HalfNormal('δ', sd=10)

x_shared = shared(data.Months.Values * 1.)

## this is not working. I think our version of python is too new?

## we've  got get a venv going, I think
## not working. try condas

## in terminal, our condas environment is called (see below)
conda activate bayesAna

python3

## I think we need the following:
import numpy as np
import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
import pymc3 as pm
from theano import shared
import scipy.stats as stats
import arviz as az
import geopandas as gpd
import scipy.spatial as sp
from sklearn.linear_model import LinearRegression
from theano import shared
from scipy import stats

## I think we need to make an environment for the project

## try out python3.7. Don't think we have this locally installed, 
## unless anaconda put it on there. But try.

conda create --name bayesAna python=3.7

conda install pymc3

conda activate bayesAna

## using this new environment, does pymc3 behave?

## looks good...


## how do we use conda with jupyter? 

## I think we need to install jupyter within the environment...

## these websites may be useful
## https://bytesofcomputerwisdom.home.blog/2019/03/31/jupyter-notebook-running-the-wrong-python-version/
## https://stackoverflow.com/questions/58068818/how-to-use-jupyter-notebooks-in-a-conda-environment

## this file looks like this:
## /usr/share/jupyter/kernels/python3/kernel.json

cd /usr/share/jupyter/kernels/python3/

{
 "argv": [
  "/usr/bin/python3",
  "-m",
  "ipykernel_launcher",
  "-f",
  "{connection_file}"
 ],
 "display_name": "Python 3",
 "language": "python"
}

## there may be a another kernel spec here:
ls ~/.local/share/jupyter/kernels/python3/kernel.json 
## doesn't exist, not the problem.

## our python 3.7 for the bayesAna env is stored here

ls /home/daniel/anaconda3/envs/bayesAna/bin


## can we use this for the above kernel spec file?
## as long as we are using the bayesAna environment, we can use to find our kernels

jupyter kernelspec list

## which points here
cd /home/daniel/anaconda3/envs/bayesAna/share/jupyter/kernels/python3

## which has a kernel.json file that lists:

ls -l /home/daniel/anaconda3/envs/bayesAna/bin/python

## ... which is a symlink to python3.7
## very labrynthine, but correct 

## huh, it works all of a sudden

## well, keep on

## so back to our example above...p138 in the martin book

dataPath=(
"/home/daniel/Documents/learn/Bayesian/martinBayesianAnalysis/"
"Bayesian-Analysis-with-Python-Second-Edition/data/babies.csv"
)
data = pd.read_csv(dataPath)

data.plot.scatter('Month', 'Lenght')

data.head()

with pm.Model() as model_vv:
    α = pm.Normal('α', sd=10)
    β = pm.Normal('β', sd=10)
    γ = pm.HalfNormal('γ', sd=10)
    δ = pm.HalfNormal('δ', sd=10)
    x_shared = shared(data.Month.values * 1.)
    μ = pm.Deterministic('μ', α + β * x_shared**0.5) 
    ε = pm.Deterministic('ε', γ + δ * x_shared)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=data.Lenght) 
    trace_vv = pm.sample(1000, tune=1000)

##  

data.plot.scatter('Month', 'Lenght', alpha=0.5)
ax=plt.gca()
μ_m = trace_vv['μ'].mean(0)
ε_m = trace_vv['ε'].mean(0)

ax.plot(data.Month, μ_m, c='k')
ax.fill_between(data.Month, μ_m + 1 * ε_m, μ_m - 
                1 * ε_m, alpha=0.6, color = 'C1')
ax.fill_between(data.Month, μ_m + 2 * ε_m, μ_m - 
                2 * ε_m, alpha=0.4, color = 'C1')

plt.xlabel('x')
plt.ylabel('y', rotation=0)

## that worked, I think

## so adapt this to our situation. ε is our variance
## and we think it has its own dependence on the the 
## x value (x = distance between comparisons). 
## so it will have its own linear model that is   
## a function of x. 
## our deterministic model is asymptotic, y = x/(k+x), 
## where x varies between 0 and 1. 

from sympy import symbols
from sympy import Function

x, y, z, t = symbols('x y z t')
k, m, n = symbols('k m n', integer=True)
f, g, h = symbols('f g h', cls=Function)

x = symbols('x')

k=1

a = x/(k+x) 
b = x/(k+x) + 0.5
c = (x/(k+x) + 0.5)*(3/4)

p1 = plot((a), show=False)
p2 = plot((b), show=False)
p3 = plot((c), show=False)
p1.xlim = (0,15)
p1.ylim = (0,2)
p2[0].line_color = 'red'
p3[0].line_color = 'green'
p1.append(p2[0])
p1.append(p3[0])
p1.show()

## having trouble with the asymptote
## just use pyplot

plt.close('all')
meanBC = [0.8202427173225738, 0.8803880579382168, 0.8718442882110651,
         0.8910368558226932, 0.881123053179186]
x = pd.Series(np.linspace(0,10,100))
y = x.apply(lambda x: x/(1+x))
y1 = x.apply(lambda x: x/(10+x))
#y2 = x.apply(lambda x: (2*x)/(1+x))
y3 = x.apply(lambda x: (x+3)/(10+x+3))
fig, ax = plt.subplots()
ax.plot(x,y, color='red')
ax.plot(x,y1, color='blue')
#ax.plot(x,y2, color='black')
ax.plot(x,y3, color='purple')
ax.scatter(range(0,5), meanBC)
ax.set_xlim(0,10)
ax.set_ylim(0,1)

## how do modify the y intercept? as in, shift the 
## plot to the left or right, without modifying the ymax
## we do this by adding a constant to x whereever it appears
## but that is too complex. Let's keep it simple, 
## with a classic michaelis-menton with a k constant

## while we're at it, let's revisit the 
## pure distance comparisons (vs the microcuenca distance)
## this might be easier to model in our first effort 

specObs = pd.read_csv('specObs.csv', index_col='PsubP').sort_index()
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP').sort_index()
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
pts = ((gpd.read_file('GIS/ana30meterPlots.geojson')
        .set_index('PsubP')
        .sort_index())
        )
## sanity check
(pts.index == envOnly.index).all()
(pts.index == subParcelComm.index).all()

## shape and plot the data
plt.close('all')
fig, ax = plt.subplots()
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.scatter(physDist, bcDist, alpha=0.2, s=50)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title('Overall Turnover')
## any lines from our function work well here?
x = pd.Series(np.linspace(0,3700,500))
y = x.apply(lambda x: x/(1+x))
y1 = x.apply(lambda x: x/(100+x))
y2 = x.apply(lambda x: (x+100)/(100+(x+100)))
#ax.plot(x,y, color='red', linewidth=3)
ax.plot(x,y1, color='blue', linewidth=3)
ax.plot(x,y2, color='black', linewidth=3)
ax.set_xlim(0,1000)
ax.set_ylim(0,1)
## looks promising. so how to fit this as a model?

## if we treat it as a true michaelis-menton model,
## I think that k and the variance ε should be treated 
## as probabilities, but we should start simple here   
## for start, can we make a simple model around this
## asymptote function. 

## what form is is our data? this is our BC distances

y = bcDist 
x = physDist
with pm.Model() as model_g:
    ε = pm.Normal('ε', mu=0, sd = 0.1)
    μ = pm.Deterministic('μ', x/(x+1))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=y)
    trace_g = pm.sample(2000, tune=1000)

## this is not working, something is wrong with the 
## expression when declaring μ as a deterministic 
## variable..

## does this work better if we don't declare the 
## μ separately?

with pm.Model() as model_g:
    ε = pm.Normal('ε', mu=0, sd = 0.1)
    y_pred = pm.Normal('y_pred', mu= x/(x+100), sd=ε, observed=y)
    trace_g = pm.sample(2000, tune=1000)

## works better but sampler fails, says "bad initial energy", more specifically:
## "Bad initial energy, check any log probabilities that are inf or -inf, nan or very small: y_pred   -inf "

plt.hist(y, 100)

## so somehow my likelihood is yielding a negative infinity

## not sure. try some toy data? 

xx = pd.Series(range(0,3500,100))
yy_real = xx/(xx + 100)
εε = np.random.normal(0, 0.05, size=len(xx))
yy = yy_real + εε

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(xx,yy, alpha=0.4)
ax.plot(xx,yy_real, 'k')
ax.set_ylim(0,1.3)

## now fit a model...

with pm.Model() as model_g:
    #ε = pm.Normal('ε', mu=0, sd = 0.1) ## doesn't work 
    #ε = pm.HalfNormal('ε', sigma = 0.05) ## why does that miss the mark so much?
    ε = pm.Gamma('ε', mu=0.05, sigma = 0.01)
    y_pred = pm.Normal('y_pred', mu= xx/(xx+100), sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)

## ah, the issue was that I was using a normal dist for the 
## variance around the curve. Switched to gamma as recommended by
## the fellow on my posting here:
aa = specObs['genusSpecies'].str.contains('Boraginaceae')
specObs[aa]

## If they weren't able to call it as a Cordia sp., not sure 
## what it could be, Cordia seems to be the main tree genus..

## I think we gotta throw this out. There is just no information.

### Rubiaceae Psychotria cf. stipularis ###

## I think we just decided above we are keeping all 
## Psychotrias...

## this may have been renamed to Palicourea stipularis...
## which EOL says is a woody climber:

https://eol.org/pages/1108085

## I guess err on the side of caution and throw this out.

### Myrsinaceae Cybianthus humilis ###

## this genus is mostly trees in the TRY data, though
## no exact species match.

## no help from EOL on growth form 

aa = specObs['genusSpecies'].str.contains('Cybianthus')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Cybianthus')
juvSpecObs[bb]

## all this species

## gentry says small trees, shrubs, subcanopy. But big enough
## to be noticed in our mature data? Not sure.
## not a lot of  information. I guess keep it. 

### Solanaceae sp. ###

## no fucking way. this could be anything. 

aa = specObs['genusSpecies'].str.contains('Solanaceae')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Solanaceae')
juvSpecObs[bb]

## gotta ditch it

### Rubiaceae Agouticarpa grandistipula ###

aa = specObs['genusSpecies'].str.contains('Agouticarpa')
specObs[aa]

bb = juvSpecObs['genusSpecies'].str.contains('Agouticarpa')
juvSpecObs[bb]

## TRY has a different species, a tree or small tree:
grep "Agouticarpa" justGrowthForm.tsv
## EOL lists this species as a tree <https://eol.org/pages/1097921>

## let's keep it.

### Indeterminado sp.22 ###
### Indeterminado sp.21 ###

## i think we have ditch these. Not much we can do with them. 

### Arecaceae Aiphanes erinaceae ###

aa = specObs['genusSpecies'].str.contains('Aiphanes')
specObs[aa]
## no adults

bb = juvSpecObs['genusSpecies'].str.contains('Aiphanes')
juvSpecObs[bb]

## based on this:
Svenning, Jens-Christian, and Henrik Balslev. "The palm flora of the Maquipucuna montane forest reserve, Ecuador." Principes 42 (1998): 218-226.
## I think we gotta exclude this. This is a small palm, not known 
## to exceed 5 cm, at least if I'm reading this article right. 


### Lauraceae Ocotea aff. insularis ###

aa = specObs['genusSpecies'].str.contains('Ocotea')
specObs[aa]

specObs[aa].shape ## lots of hits, 77

specObs[aa].head() ## lots of hits, 77

specObs.loc[aa, 'genusSpecies'].unique()

len(specObs.loc[aa, 'genusSpecies'].unique()) ## eleven Ocoteas in the mature set. 

bb = juvSpecObs['genusSpecies'].str.contains('Ocotea')
juvSpecObs[bb]

## so basically, this is something that looks really similar to 
## some of our adults, but something made them think it was
## different. Jeez. Well, keep it I guess. 

### Asteraceae Criptoniosis cf. occidentalis ###

aa = specObs['genusSpecies'].str.contains('Criptoniosis')
specObs[aa] 
## none

bb = juvSpecObs['genusSpecies'].str.contains('Criptoniosis')
juvSpecObs[bb]
## one site

## I'm betting this is also a typo, and they meant:
"Critoniopsis cf. occidentalis"

## so check that again...
aa = specObs['genusSpecies'].str.contains('Critoniopsis')
specObs[aa] 
##  lots of the C. occidentalis, in several types of plots 

bb = juvSpecObs['genusSpecies'].str.contains('Critoniopsis')
juvSpecObs[bb]

aa = np.array(xx)/(np.array(xx)+kdraws[:,np.newaxis]) + edraws.reshape(-1,1)

plt.close('all')
fig, ax = plt.subplots()
plt.plot(x, aa.T, c='gray', alpha=0.5, zorder=1)
## add the mean values plot:
ax.plot(xx, (xx/(xx+κ_m) + ε_m), color='k', zorder=2)
## add data
ax.scatter(xx,yy,zorder=3, alpha=0.5)

## looks good. if this were a real model, we would actually try to
## catch the variance more with our estimates of epsilon.

## now, can we add in variance that varies with distance?

xx = pd.Series(range(0,3500,100))
k = 100
yy_real = xx/(xx + k)
εε = np.random.normal(0, 0.05, size=len(xx))
## but now we want to start out with wide 
## variance and narrow with increasing distance
εvv5 = εε*(5*xx/max(xx))
εvv-5 = εε*(-5*xx/max(xx))
y0 = yy_real + εε
y1 = yy_real + εvv5
y2 = yy_real + εvv-5
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(xx,y0,c='green', alpha=0.3)
ax.scatter(xx,y1,c='blue', alpha=0.3)
#ax.scatter(xx,y2,c='red', alpha=0.3)
ax.plot(xx, yy_real, c='black')

######### gamma dist ##########
## side note what does a gamma look like?:
fig, ax = plt.subplots()
import scipy.special as sps
shape, scale = 1, .1  
s = np.random.gamma(shape, scale, 1000)
count, bins, ignored = plt.hist(s, 50, density=True)
y = bins**(shape-1)*(np.exp(-bins/scale) /  # doctest: +SKIP
                     (sps.gamma(shape)*scale**shape))
ax.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP
############################

## anyway try again. seems like we need a negative linear relationship.
## our y-intercept will be a quantity that estimates the spread
## of the data that we are seeing at the start of the curve,
## and our slope will describe how that spread tightens with
## distance:

plt.close('all')

xx = np.array(range(0,3500,100))
#yy = np.full(len(xx),1) ## for straight line.
yy = xx/(100+xx)
## normalize xx, so not insane
xnorm = pd.Series(xx/xx.max())
## here we try a starting SD of 0.2, and a rate of tightening of 0.2
ee = xnorm.apply(lambda x: np.random.normal(0,.2-0.2*x)).values
fig, ax = plt.subplots()
ax.plot(xx, yy, c='k')
ax.scatter(xx, yy+ee, c='blue')
ax.set_ylim(0,1.2)

xx = np.array(range(0,3500,100))
yy_real = xx/(100+xx)
## normalize xx, so not insane
xnorm = pd.Series(xx/xx.max())
## make our noise. Linear function where y-intercept is our max initial variance
## our neg slope is the rate at which the variance decreases with distance
## so here we try a starting SD of 0.2, and a rate of tightening of 0.2
ee = xnorm.apply(lambda x: np.random.normal(0,.2-0.2*x)).values
fig, ax = plt.subplots()
yy = yy_real+ee
ax.scatter(xx, yy, c='blue')
ax.set_ylim(0,1.2)

## Let's try to model this as is.
## in this case, ε will change with x... following Martin,
## we turn epsilon into a deterministic function

with pm.Model() as model_vv:
    γ = pm.Gamma('γ', mu=0.2, sigma = 0.01) ## y-int, = initial maximum spread of variance
    δ = pm.Normal('δ', mu=(-0.2), sd=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', γ + δ*xnorm)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)

## lots of divergence...syntax is good but doesn't work
## the samplers can't handle the posterior, I guess.  
## maybe the gamma? try bounding a normal?

with pm.Model() as model_vv:
    γ = pm.Gamma('γ', mu=0.2, sigma = 0.01) ## y-int, = initial maximum spread of variance
    δ = pm.Normal('δ', mu=(0.2), sd=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', γ + δ*xnorm)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)


## what could be wrong? maybe the gamma: 

az.plot_trace(trace_g, var_names = ['μ', 'ε'])
## that's a big nope. Something is very wrong.


### okay, back after a couple of weeks. where were we?...

## got a response of the pymc site: 

https://discourse.pymc.io/t/modeling-noising-parameter-that-decreases-negatively-varying-variance/6728

## they recommend exponentiating the variance function

## how do we do this?


## but I think that means we change the behavior of γ and δ  
## originally γ estimated at -.2, with sd of 0.05 
## originally δ estimated -.2, with sd of 0.05 

## doing a little algebra, the exponeniated version gives us
γ_mean = np.log(0.2)
δ_mean = -0.00134475
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)

## how does this look on a graph?
plt.close('all')
fig, ax = plt.subplots()

xx = np.arange(1,3501,20)
yy_real = xx/(100+xx)
γ_mean = np.log(0.2)
δ_mean = -0.0004
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)
δ_real = a(xx)
#ax.plot(xx, δ_real, c='red')
## δ is sensitive. I think we need something like δ = 0.0008
## anyway, its just a prior. put a wide spread on it 
## toy data again:
## to add in noise? in any given spot, the predicted variance is a function of x:
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)
ee = np.random.normal(0, a(xx))
yy = yy_real + ee

plt.close('all')
fig, ax = plt.subplots()
ax.plot(xx, yy_real)
ax.scatter(xx,yy, c='r')



np.log(0.2)


with pm.Model() as model_vv:
    γ = pm.Gamma('γ', mu=1.5, sigma = 0.5) ## analagous to initial spread of variance (take natural log)
    δ = pm.Normal('δ', mu=0.0008, sd=0.0002) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)

## still zeroes or neg inf or something like that. where is the problem? 
## not sure.


##################### clean up pymc discourse model ########################

successful = []
notSuccessful = []

xx = np.array(range(0,3500,100))
yy_real = xx/(100+xx)
## normalize xx
xnorm = pd.Series(xx/xx.max())
ee = xnorm.apply(lambda x: np.random.normal(0,.2-0.2*x)).values
yy = yy_real+ee
with pm.Model() as model_vv:  
    γ = pm.Gamma('γ', mu=0.2, sigma = 0.01) ## y-int, = initial maximum spread of variance
    δ = pm.Normal('δ', mu=(-0.2), sd=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', γ + δ*xnorm)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)

successful.append(yy)

notSuccessful.append(yy)

meh, works 

################

## back to exponential model

xx = np.arange(1,3501,20)
yy_real = xx/(100+xx)
γ_mean = np.log(0.2)
δ_mean = -0.0004
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)
ee = np.random.normal(0, a(xx))
yy = yy_real + ee

with pm.Model() as model_vv:
    γ = pm.Gamma('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (take natural log)
    δ = pm.Normal('δ', mu=-0.0004, sd=0.0002) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(2000, tune=1000)

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(xx,yy)
ax.plot(xx,yy_real, c='black')

### suggested changes by folks at pymc discourse:

https://discourse.pymc.io/t/modeling-noising-parameter-that-decreases-negatively-varying-variance/6728/8

xx = np.arange(1,3501,20)
yy_real = xx/(100+xx)
γ_mean = np.log(0.2)
δ_mean = -0.0004
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)
ee = np.random.normal(0, a(xx))
yy = yy_real + ee
with pm.Model() as model_vv:
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (take natural log)
    #γ = pm.Gamma('γ', mu=1.5, sigma = 0.5) ## gamma version
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    #ε = pm.Deterministic('ε', np.power(np.e, -γ + δ*xx)) ## use with gamma, neg
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    #trace_g = pm.sample(2000, tune=1000)
    trace_g = pm.sample(init='adapt_diag')


## whoah, actually works now. can we visualize?
plt.close('all')
μ_m = trace_g['μ'].mean(0)
ε_m = trace_g['ε'].mean(0)
fig, ax = plt.subplots()
ax.scatter(xx,yy)
ax.plot(xx, μ_m, c='k')
#ax.plot(xx, yy_real)
ax.fill_between(xx, μ_m + 1*ε_m, μ_m - 1*ε_m, alpha=0.6, color='C1')
ax.fill_between(xx, μ_m + 2*ε_m, μ_m - 2*ε_m, alpha=0.4, color='C1')

####### our data
## great! can we try this on our data?

specObs = pd.read_csv('specObs.csv', index_col='PsubP').sort_index()
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP').sort_index()
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
pts = (gpd.read_file('GIS/ana30meterPlots.geojson')
        .set_index('PsubP')
        .sort_index()
      )
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

lw=5
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(physDist, bcDist, s=100, alpha=0.3)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=lw)
ax.set_title('Overall Turnover')
## any lines from our function work well here?
x = pd.Series(np.linspace(0,3700,500))
#y = x.apply(lambda x: x/(1+x))
y1 = x.apply(lambda x: x/(100+x))
#ax.plot(x,y, color='orange', linewidth=lw)
ax.plot(x,y1, color='red', linewidth=lw)

## our observed would be our bcDist variable
## x-axis is  physDist  

xx = physDist.copy()
with pm.Model() as model_vv:
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_g = pm.sample(init='adapt_diag')

## looks okay. how do we plot this? I think we need to sort this...
μ_m = trace_g['μ'].mean(0)
ε_m = trace_g['ε'].mean(0)
aa = pd.DataFrame({ "μ_m": μ_m, 
                    "ε_m": ε_m,
                    "physDist": physDist, 
                    "bcDist": bcDist})
bb = aa.sort_values(by=['physDist'])

## get ppc:
ppc = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_vv)

## plot ppc ε
plt.close('all')
fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(bb.physDist,bb.bcDist, c='b', s=5)
ax.plot(bb.physDist, 
        bb.μ_m, 
        c='r', 
        linestyle= "solid", 
        label=r'$\mu$',
        linewidth=4)
az.plot_hpd(physDist, ppc['y_pred'], credible_interval=0.5, color='orange', ax=ax)
az.plot_hpd(physDist, ppc['y_pred'],  color='orange', ax=ax)
ax.legend(bbox_to_anchor=(0.95,0.05), loc='lower right')
ax.set_title(r'$\epsilon$ as normal dist')

az.r2_score(bcDist, ppc['y_pred'])

## plot by standard devs 
#fig, ax = plt.subplots()
#ax.scatter(bb.physDist,bb.bcDist)
#ax.plot(bb.physDist,bb.μ_m, c='k')
#ax.fill_between(bb.physDist, bb.μ_m + 1*bb.ε_m, bb.μ_m - 1*bb.ε_m, alpha=0.6, color='C1')
#ax.fill_between(bb.physDist, bb.μ_m + 2*bb.ε_m, bb.μ_m - 2*bb.ε_m, alpha=0.6, color='C2')



## actually, that went suprisingly well...
## but what about our family of errors? 

#### use different family of error ###
 
## can we try a gamma? 

xx = physDist.copy()
with pm.Model() as model_gamma:
    γ = pm.Normal('γ', mu=1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, -γ + δ*xx))
    y_pred = pm.Gamma('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_g = pm.sample(init='adapt_diag')

μ_m = trace_g['μ'].mean(0)

trace_g['δ'].mean()

az.plot_trace(trace_g, var_names = ['γ','δ','κ'])

az.plot_trace(trace_g)

trace_g['μ'].mean(0)

len(trace_g['μ'])

trace_g['μ'][0]

ε_m = trace_g['ε'].mean(0)

aa = pd.DataFrame({ "μ_m": μ_m, 
                    "ε_m": ε_m,
                    "physDist": physDist, 
                    "bcDist": bcDist})
model_vvSort = aa.sort_values(by=['physDist'])

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(model_vvSort.physDist,model_vvSort.bcDist)
ax.fill_between(model_vvSort.physDist, model_vvSort.μ_m + 1*model_vvSort.ε_m, model_vvSort.μ_m - 1*model_vvSort.ε_m, alpha=0.6, color='C1')
ax.fill_between(model_vvSort.physDist, model_vvSort.μ_m + 2*model_vvSort.ε_m, model_vvSort.μ_m - 2*model_vvSort.ε_m, alpha=0.4, color='C2')

## that doesn't work, I think because we're just putting mirror images of the 
## gamma sigma on both sides of our curve. We have to look at the shape whole posterior dist,

## anyway, I think that will produce the wrong direction for our long tail, 
## with the highest probably density under (less than) μ. How do we check this 
## out? For instance, what is the distribution around x=500, y (or μ)?
## I think there are two ways to answer this, one by using 

az.plot_trace(trace_g, var_names=['γ'])

az.plot_trace(trace_g, var_names=['δ'])

az.plot_trace(trace_g, var_names=['κ'])

trace_g['κ'].mean() ## interesting, the data moved our prior for κ to 135.

az.plot_trace(trace_g, var_names=['μ']) ## yeah, doesn't work.
## i think I'm thinking about deterministic variables the wrong way. 

az.plot_trace(trace_g, var_names=['γ','δ','κ']) 


trace_g['μ']

trace_g['μ'].shape

## each data we gave has 
len(trace_g['μ'][0]) 

## so, we should be able to get the distribution for μ at x = 500, 
## but I don't think this quite right, we need to make proper 
## posterior predictive inferences. 

## A little unclear, but following Martin's example on p185:

ppc = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_gamma)

## as a first, coarse test we can show how the distribution of 
## our predicted y values compares to our actual data, across
## all observations:

plt.figure(figsize=(8,3))
data = [bcDist, ppc['y_pred']]
labels = ['data','model_gamma']
for i, d in enumerate(data):
    mean = d.mean()
    err = np.percentile(d, [25, 75])
    print(err)
    plt.errorbar(x=mean, y=-i, xerr=[[err[0]], [err[1]]], fmt='o')
    plt.text(mean, -i+0.1, labels[i], ha='center', fontsize=14)


## looks pretty good. 

## can we get an R2 for this curve? It's not a classic linear equation,
## wonder if it works:

az.r2_score(bcDist, ppc['y_pred']) ## r2 = 0.516

## how does this compare to the r2 of the linear function from 
## scipy?

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

LinearRegression().fit(X, Y).score(X, Y) ## r2 = 0.0565
stats.linregress(physDist,bcDist) ## p = 6 x 10^-25

## not a perfect comparison, because I think the 
## definitions of R2 are pretty different in the
## frequentist and bayesian methods. I'm not sure
## how the two methods compare  mathematically.

## to really compare, I guess we would want to build
## a simple linear model. Then we would have to think 
## a lot about how to model the variance, etc. 

## maybe we go there if the reviewers want, but 
## leave it for now. 



## get our mu by average the trace from the 
## posterior:

μ_m = trace_g['μ'].mean(axis=0)
## sort them both, so as not to confuse the pyplot line:
ε_m = trace_g['ε'].mean(axis=0)
aa = pd.DataFrame({ "μ_m": μ_m, 
                    "ε_m": ε_m,
                    "physDist": physDist, 
                    "bcDist": bcDist})
model_gammaSort = aa.sort_values(by=['physDist'])

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(model_gammaSort.physDist,model_gammaSort.bcDist, c='black', s=5)
ax.plot(model_gammaSort.physDist, model_gammaSort.μ_m)
az.plot_hpd(physDist, ppc['y_pred'], credible_interval=0.5, color='orange')
az.plot_hpd(physDist, ppc['y_pred'],  color='orange')


## why is this different than using the trace of the posterior?
## statistical reasons, I think. The bayesian approach is to 
## update the posterior with the new data. It has to "register"
## the new point with the likelihood, and bend it with the prior, etc. 
## I think it basically involves updating the whole system with 
## the new point.  

## but, we haven't fed any new points to it...is it just running all the 
## old x values through the new posterior  for us? this seems most 
## likely. in this case, what is this new data...the new MAP y-value for each 
## of the x values, given our posterior.

ppc['y_pred'].shape
trace_g['γ'].mean()
trace_g['δ'].mean()
trace_g['κ'].mean()

gamma_m = trace_g['γ'].mean()
delta_m = trace_g['δ'].mean()
kappa_m = trace_g['κ'].mean()

model_vvSort

model_vvSort.head()

model_vvSort.tail()

plt.close('all')

fig, ax = plt.subplots()
plt.scatter(physDist, bcDist, c='b', alpha=0.4)
## credible interval. Not sure what difference here is from the hpd...
sig = az.plot_hpd(physDist, trace_g['μ'], credible_interval=0.99, color='green')

xx = model_vvSort.physDist.copy()

plt.plot(xx, xx/(xx+kappa_m), c='k',)

az.plot_hpd(physDist, ppc['y_pred'], color='gray')

## meh, still symmetric...why? shouldn't the gamma dist take care of this?

#### constructing question about symmetric variance ####

## this is over my head, get help.

## what would this look like? toy data and model should follow
## our existing example at 
## https://discourse.pymc.io/t/modeling-noising-parameter-that-decreases-negatively-varying-variance/6728/8

xx = np.arange(1,3501,20)
yy_real = xx/(100+xx)
γ_mean = np.log(0.2)
δ_mean = -0.0004
a = lambda x: np.power(np.e, δ_mean*x + γ_mean)
ee = np.random.normal(0, a(xx))
yy = yy_real + ee

with pm.Model() as model_gammaToy:
    γ = pm.Gamma('γ', mu=1.5, sigma = 0.5) ## gamma version
    δ = pm.Normal('δ', mu=-0.0004, sigma=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=100, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, -γ + δ*xx)) ## use with gamma, neg coefficient
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=yy)
    trace_g = pm.sample(init='adapt_diag')

## get mu, posterior predictive 
trace_g['μ'].mean(axis=0)
ppc = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_gammaToy)

## now visualize with some different credible intervals

plt.close('all')
fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(xx,yy, c='blue', alpha=0.5)
ax.plot(xx,yy_real, c='k')


az.plot_hpd(xx, ppc['y_pred'], credible_interval=0.5, color='orange')
az.plot_hpd(xx, ppc['y_pred'], color='orange')

## can we make the question now?

## for the data graphic
lw=5
plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(physDist, bcDist, s=100, alpha=0.3)
ax.plot(xx, trace_g['μ'].mean(axis=0), c='k', linewidth=4)



#### linear model of turnover ####

## because I am a child with this, a toy model first:

## toy data linear
N = 100
alpha_real = 2.5
beta_real = 0.9
eps_real = np.random.normal(0, 0.5, size=N)
x = np.random.normal(10, 1, N)
y_real = alpha_real + beta_real*x
y = y_real + eps_real

_, ax = plt.subplots()
ax.scatter(x, y)
ax.set_xlabel('x')
ax.set_ylabel('y', rotation=0)
ax.plot(x, y_real, 'k')
plt.tight_layout()

with pm.Model() as linModel:
    α = pm.Normal('α', mu=0, sd=10)
    β = pm.Normal('β', mu=0, sd=1)
    ε = pm.HalfCauchy('ε', 5)
    μ = pm.Deterministic('μ', α + β*x)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=y)
    trace_linModel = pm.sample(2000, tune=1000)

az.plot_trace(trace_linModel, var_names=['α', 'β', 'ε'])

## real data linear

## our actual turnover data is recreated above, "####### our data"

physDist
bcDist 
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

## what are our priors?
## the least squares fit is:

LinearRegression().fit(X, Y).predict(X)

lr = LinearRegression().fit(X, Y)

R2 = lr.score(X, Y) ## 0.05, pretty horrible, but I guess that's the point.

## but to do this as a bayesian model. I think we can make a vague prior
## from the Draper et al (2019) paper (https://doi.org/10.1002/ecy.2636)
## Their curves for terra firme Amazonian forest (A of figure 4) 
## look pretty similar to ours. As in, a fairly shallow linear coefficient β
## and a y-int of similarity α ~ 0.1 or 0.2 (in our case, 0.8-0.9). 
## I think we need a shrinking error term, just as we have with the
## asymptote model.

## in this case, a shallow β would be a drop in similarity of ~0.02 hellinger
## distance in 500m. They have plotted results as similarity, in Hellinger distance 
## which therefor I think  goes from sqrt(2) (~1.4) for maximum similarity, 
## and 0 for total disimilarity. I gather it is a chord-type similarity, that 
## tries to scale abundances, focusing on congruencies/relative-abundances 
## I think that as things approach zero, it probably going to behave fairly 
## similarly to BC distance. 

## so, set our α = 0.85, our β = (0.02/500), or .00004

## toy data
N = 200
maxDist = 3500
xx = np.linspace(1,maxDist,N)
alpha = 0.85
beta = 0.00004
γ_mean = np.log(0.2) ## initial spread
δ_mean = -0.0004 ## rate of tightening
a = lambda x: np.power(np.e, δ_mean*xx + γ_mean)
#ee = np.random.normal(0, a(physDist))
ee = np.random.normal(0, a(physDist))
yy_real = alpha + beta*x
yy = yy_real + ee

fig, ax = plt.subplots()
ax.scatter(xx, yy)
ax.plot(xx, yy_real, color='k')

## okay, but we need that epsilon term in the same
## format as the above term for our asymptote function
## a negatively-varying variance

## well, generally this might be sort of a dead end. It seems
## like a lot of analyses frame the discussion with 
## exponential decay, as loglinear expressions

## so what do we report? we have a working model, report it. 
## maybe just show the model with a graphic of the first SD or something.

## we also need some sort of verification that the model 
## works, so maybe just some posterior predictive checks.

## and maybe check our priors - what can be gleaned from 
## the literature, and does our estimate of M_k = 200 work as
## a prior

## so maybe the first step here is to update the notebook
## with the working model, so I can keep track of where I
## actually am in all this. 

## then check the priors again. Is this technically a hierarchical 
## model?

## get posterior predictive check

## summary graphic?

np.power(np.e, np.log(2)) + np.power(np.e, np.log(3))

help(pm.model_to_graphviz)

## needs graphviz, which I have for my up2date python install, 
## but not for bayesAna environment... to install it we need 
## something like:

conda install -n bayesAna python-graphviz=0.14

## this is an older version of the graph-viz, let's see how 
## it does.
aa = pm.model_to_graphviz(model_vv)
aa.render('testNetGraph')
## meh, underwhelming

## but I'm pretty convinced this is not a hierarchical model,
## we don't have different groups with independent priors. 
## you could "flatten" it, and plug in all the priors to
## the same level 

#### skewed normal variance model ####

## now we need to account for the hanging "curtain"
## of the data, in that the variance around μ is 
## dense at the mean but drops off steeply above it
## and has a long tail toward lower values

## as per the discussion at:
https://discourse.pymc.io/t/gamma-noise-prior-upside-down/7249
## the answer might be to assign a skewed normal 
## distribution here. 

## Example source code available here: https://docs.pymc.io/api/distributions/continuous-21.py 

def plotSkewNorm():
    plt.close('all')
    #plt.style.use('seaborn-darkgrid')
    x = np.linspace(-4, 4, 200)
    alpha=-1
    loc=0.9
    scale=0.1
    for alpha in [-1, 0, 1]:
        pdf = stats.skewnorm.pdf(x, [alpha], loc=loc, scale=scale)
        plt.plot(x, pdf, label=r'$\mu$ = {}, $\sigma$ = {}, $\alpha$ = {}'.format(loc, scale, alpha))
    plt.vlines([loc,1],0,10,colors=['red','black'], linestyles=['dashed','solid'])
    plt.xlabel('x', fontsize=12)
    plt.ylabel('f(x)', fontsize=12)
    #plt.ylim(0,1)
    plt.xlim(0,1.5)
    plt.legend(loc=2)

plotSkewNorm()

plt.style.use('classic')

## seems like σ=0.1 works okay, might need to loosen it since 
## we don't have a good source for the prior 

## and for the skew, α=-1, also looks okay. 

## so how do we implement it in the model?
## do we need to modify our SD in the for ε?
## what does it look like if we just change
## the distribution to a skewed normal, 
## with a prior for the skew term α. 

xx = physDist.copy()
with pm.Model() as model_skewNormal:
    γ = pm.Normal('γ', mu=1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    α = pm.Normal('α', mu=-1.0, sigma = 0.5)
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, -γ + δ*xx))
    y_pred = pm.SkewNormal('y_pred', mu=μ, sd=ε, alpha=α, observed=bcDist)
    trace_sn = pm.sample(init='adapt_diag')

## okay, how does that look? 

az.plot_trace(trace_sn, var_names = ['γ','δ','κ','α']) 

## whoah, why did α shift so hard? 
## and why did kappa drop so hard?

## look at the model generally:


aa = pd.DataFrame({ "μ_m": trace_sn['μ'].mean(0),
                    "ε_m": trace_sn['ε'].mean(0),
                    "physDist": physDist,
                    "bcDist": bcDist})

model_skewNormSort = aa.sort_values(by=['physDist'])

ppc_skewNorm = pm.sample_posterior_predictive(trace_sn, samples=2000, model=model_skewNormal)

plt.close('all')
fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(model_skewNormSort.physDist,model_skewNormSort.bcDist, c='b', s=5)
ax.plot(model_skewNormSort.physDist, 
        model_skewNormSort.μ_m, 
        c='r', 
        linestyle= "solid", 
        label=r'$\mu$',
        linewidth=4)
az.plot_hpd(physDist, ppc_skewNorm['y_pred'], credible_interval=0.5, color='orange', ax=ax)

az.plot_hpd(physDist, ppc_skewNorm['y_pred'],  color='orange', ax=ax)

ax.legend(bbox_to_anchor=(0.95,0.05), loc='lower right')
ax.set_title(r'$\epsilon$ as skewed-normal')


#X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
#ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=4, linestyle= "dotted")

az.r2_score(bcDist, ppc_skewNorm['y_pred'])

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
LinearRegression().fit(X, Y).score(X, Y)

## hmm, something is very wrong, methinks. 

trace_sn['α'].shape

trace_sn['γ','δ','κ','α']) 

###### MEMs continued #####

## back into R to look at the MEMs again:

library(vegan)
library(ade4)
library(adespatial)
library(adegraphics)
library(spdep)
library(maptools)
library(sp)

load('sigMEM.rda')
load('lcPoly.rda')
load('lcPolyMoved.rda')
pts <- read.csv('pts.csv')



## to look at the PCNMs in R easily:
## our UTM coords for LC poly
xx <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolE']
yy <- lcPoly@polygons[[1]]@Polygons[[1]]@coords[,'LCpolN']
pJ <- as.matrix(pts[,2:3])
colnames(pJ) <- c('x','y')
mx <- min(pJ[,1])
my <- min(pJ[,2])
pJ <- cbind(pJ[,1] - mx, pJ[,2] - my)
## transform our polygon points with these values?
xx <- xx - mx
yy <- yy - my
s.value(pJ, sigMEM, symbol="circle", Sp=lcPolyMoved)

## great, so, what do we do with these?
## can we get them into python as a geopanda?

## let's first try the easy way, hope all the pt orders
## are conserved (should be):

aa <- cbind(pts,sigMEM) 

write.csv(aa, file='sigMEM.csv', row.names=FALSE)

## now see if a geopandas can be made from this
## back to python:

import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors as mcolors
import statsmodels.api as sm
import geopandas as gpd
import rasterio
import rasterio.plot
import rasterio.mask
import rasterio.crs
import copy
import gdal
import shapely
import shapely.validation
from sklearn.linear_model import LinearRegression
from statsmodels.stats.multitest import multipletests
from matplotlib.patches import Patch


lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
dem = rasterio.open("GIS/anaPlotDEM.tif")
paths = gpd.read_file('GIS/PathsLC.geojson')
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
cotacachi = gpd.read_file('GIS/cotacachiCanton.geojson')
chontalPoly = gpd.read_file('GIS/ChontalBP.geojson')
cebuPoly = gpd.read_file('GIS/cebuBP.geojson')
#forest1990rast = rasterio.open("GIS/forest1990CotacachiCoded.tif")
#forest2018rast = rasterio.open("GIS/forest2018CotacachiCoded.tif")
cotacachiForest1990 = rasterio.open("cotacachiForest1990.tif")
cotacachiForest2018 = rasterio.open("cotacachiForest2018.tif")
losCedrosForest1990 = rasterio.open("losCedrosForest1990.tif")
losCedrosForest2018 = rasterio.open("losCedrosForest2018.tif")
chontalForest1990 = rasterio.open("chontalForest1990.tif")
chontalForest2018 = rasterio.open("chontalForest2018.tif")

aa = pd.read_csv('sigMEM.csv', index_col='PsubP')
mems = gpd.GeoDataFrame(aa, geometry=gpd.points_from_xy(aa.X, aa.Y))
mems.drop(['X', 'Y'], axis='columns', inplace=True)
mems = mems[['MEM1', 'MEM2', 'MEM3', 'MEM4', 'MEM5', 'MEM6', 'MEM7', 'MEM8', 'MEM9', 'MEM10', 'MEM11', 'MEM12', 'MEM13', 'geometry']]
mems.set_crs('EPSG:32717', inplace=True)

## how does this look, if we plot over a los Cedros poly?

plt.close('all')

mem='MEM1'
scale=50
ms = mems[mem].abs()*scale
marg = 650
fig, ax = plt.subplots()
lcPoly.boundary.plot(ax=ax)
mems.plot(column=mem, 
    edgecolor = "black",
    ax=ax, 
    markersize = ms,
    c=mem,
    cmap=cmap,
    )
ax.set_xlim(mems.total_bounds[0]-marg, mems.total_bounds[2]+marg)
ax.set_ylim(mems.total_bounds[1]-marg, mems.total_bounds[3]+marg)
ax.set_title(mem)

## we need a colors that shows pos/neg...
fCols = pd.Series(data=[None]*len(mems[mem]), index=mems.index)
eCols = pd.Series(data=[None]*len(mems[mem]), index=mems.index)
negMask = (mems[mem] <= 0)
fCols[negMask] = 'black'
fCols[~negMask] = 'white'
eCols[~negMask] = 'black'
eCols[negMask] = 'white'


## https://stackoverflow.com/questions/26610389/defining-a-binary-matplotlib-colormap

plt.close('all')
fig, ax = plt.subplots()
ax.scatter(pts.FinalRep_E, pts.FinalRep_N, 
    facecolors=fCols,
    s=20,
    edgecolors=eCols,
)
ax.set_title(mem)

mems[mem] > 0

## we need to simplify to pos/neg for our binary color scheme
cmap = mcolors.ListedColormap(['black','white'])

aa = mems[mem].copy()
aa[aa >= 0 ] = 1
aa[aa < 0 ] = -1
cDict = { -1 : "white", 1 : "black" }
faceCols = [ cDict[i] for i in aa ]
edgeCols = [ cDict[i] for i in -aa ]


## great. can we make this into a function and repeat?

## https://stackoverflow.com/questions/20057260/how-to-remove-gaps-between-subplots-in-matplotlib/45487279

def plotMEM(mem, ax, scale=50, marg=650, lx=745000, ly=10035500, fontsize='large'):
    mem=mem
    ax=ax
    scale=scale
    ms = mems[mem].abs()*scale
    fontsize=fontsize
    marg = marg
    negPos = mems[mem].copy()
    negPos[negPos >= 0 ] = 1
    negPos[negPos < 0 ] = -1
    cDict = { 1 : "white", -1 : "black" }
    edgeCols = [ cDict[i] for i in negPos ]
    cmap = mcolors.ListedColormap(['white','black'])
    mems.plot(column=mem,
        c=negPos,
        cmap=cmap,
        edgecolors=edgeCols,
        ax=ax,
        markersize = ms,
        ## these are to center our color map to -1,1 w/b scheme:
        vmin=-1., vmax=1.,
        )
    ax.set_xlim(mems.total_bounds[0]-marg, mems.total_bounds[2]+marg)
    ax.set_ylim(mems.total_bounds[1]-marg, mems.total_bounds[3]+marg)
    ax.text(lx,ly,mem, fontsize=fontsize)


fontsize=12

plt.close('all')

scale=150

mem = 'MEM12'
fig, ax = plt.subplots()
ax.grid(True)
plotMEM(mem, ax, scale=scale)

plt.close('all')
fig, axes = plt.subplots(3,5, figsize=(10,15), sharex=True, sharey=True)
#plt.subplots_adjust(wspace=0, hspace=0)
memList = mems.columns.to_list()[:-1]
flatAx = axes.ravel()
flatAx[13].set_axis_off()
flatAx[14].set_axis_off()
for i,j in enumerate(memList):
    lcPoly.plot(ax=flatAx[i], color="slategrey")
    plotMEM(j,flatAx[i], scale=scale)

plt.tight_layout()


plt.subplots_adjust(wspace=0, hspace=0)

## okay, that works pretty well. 
## tomorrow or whenever, catch up the notebook
## start check correlations with environmental data

## to start:
## MEM8 looks like a ridge-top vs. valley pattern, something to check

for i in range(13,16):
    print(i)

list(range(13,15))
## plot it over the elevation gradient map:

plt.close('all') 

fig, ax = plt.subplots()
rasterio.plot.show(dem, ax=ax)
lcPoly.boundary.plot(ax=ax, color='black')
plotMEM('MEM8', ax)
marg = 50
ax.set_xlim(mems.total_bounds[0]-marg, mems.total_bounds[2]+marg)
ax.set_ylim(mems.total_bounds[1]-marg, mems.total_bounds[3]+marg)

## and to check the correlation on this?

## we need to get the elevation at these points...

(envOnly.index == mems.index).all() ## looks good


plt.close('all')

plt, ax = plt.subplots()
ax.scatter(envOnly['DEM'], mems['MEM8'])

X =  envOnly['DEM'].to_numpy().reshape(-1,1)
Y =  mems['MEM8'].to_numpy().reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')

stats.pearsonr(envOnly['DEM'], mems['MEM8'])

stats.linregress(envOnly['DEM'], mems['MEM8'])

## some correlation there, r = 0.40, so r2 ~ 0.16 or so?

## do we need to report this in a bayesian way, also?

## probably wouldn't take long, use gaussian error family
## around the linear function

## but let's wait on that...

## how do we quickly do the correlation matrix for all 
## MEMs with elevation?

## pandas has builtin methods df.corr and df.corrwith


envVarsForMEM = envOnly.drop(['microcuenca','habitat','elevacion'], axis='columns')
envVarsForMEM = pd.concat([envVarsForMEM, pd.get_dummies(envOnly.habitat)], axis=1)
memPanda = pd.DataFrame(mems.drop('geometry', axis='columns'))


## ah, that only works if the columns are named the same
## so I think we have to do this the old fashioned way:

## works, env vars as rows ##
memEnvCorrMat = pd.DataFrame(index=envVarsForMEM.columns)
memEnvCorrSigMat = pd.DataFrame(index=envVarsForMEM.columns)
for i in memPanda.columns:
    memEnvCorrMat[i] = envVarsForMEM.corrwith(memPanda.loc[:,i])
    pvals_i = []
    for j in envVarsForMEM.columns:
        _,_,_,p,_ = stats.linregress(envVarsForMEM[j], memPanda.loc[:,i])
        pvals_i += [p]
    memEnvCorrSigMat[i] = pvals_i 
##########

## flip them, rows are mem:
memEnvCorrMat = pd.DataFrame(index=memPanda.columns)
memEnvCorrSigMat = pd.DataFrame(index=memPanda.columns)
for i in envVarsForMEM.columns:
    memEnvCorrMat[i] = memPanda.corrwith(envVarsForMEM.loc[:,i])
    pvals_i = []
    for j in memPanda.columns:
        _,_,_,p,_ = stats.linregress(memPanda[j], envVarsForMEM.loc[:,i])
        pvals_i += [p]
    memEnvCorrSigMat[i] = pvals_i 

memEnvCorrSigMat
memEnvCorrMat

## spot check:

def spotCheck(mem, envVar):
    _,_,r,p,_ = stats.linregress(memPanda[mem], envVarsForMEM[envVar])
    return(r, p)

spotCheck("MEM1", "DEM")
## etc. 
## looks okay.

## can we use the pvalues to mask to only the statistically
## significant relationships?

aa = memEnvCorrSigMat < 0.05
memEnvCorrMat[aa]

## works, but can we do a multiple correction, using statsmodels package:
## https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html

## so for example:
aOrR, pAdj, _, _ = multipletests(memEnvCorrSigMat['slope'], 
                                alpha=0.2, 
                                method='fdr_bh', 
                                returnsorted=False)

for (columnName, columnData) in memEnvCorrSigMat.iteritems():
    print('Colunm Name : ', columnName)
    print('Column Values : ', columnData.values)


memEnvCorrSigAdjMat = pd.DataFrame(index=memPanda.columns)
for (columnName, columnData) in memEnvCorrSigMat.iteritems():
    aOrR, pAdj, _, _ = multipletests(columnData, 
                                alpha=0.2,
                                method='fdr_bh', 
                                returnsorted=False)
    memEnvCorrSigAdjMat[columnName] = pAdj

## look right?
memEnvCorrSigMat 

memEnvCorrSigAdjMat 

## yup. Now maybe use an FDR < 0.1?:

aa = memEnvCorrSigAdjMat < 0.1
memEnvCorrMat[aa]

## I wonder if we need to correct again for the fact 
## that we are testing each environmental variable
## separately?

## ugh, at that point throws this process out, we'll have 
## discarded too much information for it to be useful 
## let's just stick with this correction for the moment. 

### categorical variable - land use: ###

## maybe break up the "habitat" variable into
## dummy vars and make a linear model with them?

envOnly.head()

envOnly.habitat

pd.get_dummies(envOnly.habitat)

## insert this above and rerun the above, in the 
## correlations


## while we're at it, do the PCNMs pick up the 
## "natural" cluster groups?



######### deforestation map #############

## let's look at the protective role of 
## los Cedros. The lifespan of lc has spanned 
## the late-80's to the present. 

## we can get the forest cover data from this 
## website:
http://ide.ambiente.gob.ec/mapainteractivo/

## the files are large. Can we bring them into memory,
## clip them down to size? 

## how much do we want to clip? we need a boundary...
## let's say that we constrain our map to 
## Carchi, Esmereldas, and Imbabura (and a small 
## disputed area between them).

## first, get these province polygons, and a compound
## boundary from them:


forest1990Path = ("/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/"
        "v_ff010_cobertura_vegetal_1990_a/v_ff010_cobertura_vegetal_1990_aPolygon.shp")
forest1990 = gpd.read_file(forest1990Path) ## whoah that be big

pathProvince = "/home/daniel/Documents/LosCed/Ecuador_GIS/limites_administrativas/nxprovincias.shp"
aa = (gpd.read_file(pathProvince).explode().set_index('DPA_DESPRO')
        .loc[['CARCHI','IMBABURA','ESMERALDAS','ZONA NO DELIMITADA']]
        .drop(['DPA_PROVIN','DPA_VALOR','DPA_ANIO',
                'REI_CODIGO','REN_CODIGO','PEE_CODIGO'], axis=1)
    )
## get rid of southern non-delimited regions:
bb = aa.cx[:, 9950000:]
## can we get some sort of union of all these?
bb.insert(0, 'region', ['norte']*bb.shape[0])
cc = bb.dissolve(by='region')

## not perfect, still a multipolygon. not sure why
## the large polygons won't merge. 

## two questions - 
## (1) why can't we single row geodataframe into a geoseries, or get a geoseries out with loc/iloc, just a pd.series?
## (2) why is the above geometry still a multipolygon? We just want a simple polygon...

## not sure why that is so complicated. 

## anyway, might work. let's call this northern ecuador.  

## can we clip our massive forest data with this? 
## following: https://geopandas.org/gallery/plot_clip.html

ee = gpd.clip(forest1990, cc)

## ugh, the forest data is messy. not my job to clean this up...unless I have to
## can we first clip with some coordinates?

forest1990box = forest1990.cx[xmin:xmax, ymin:ymax]

forest1990box.plot()
## https://stackoverflow.com/questions/40385782/make-a-union-of-polygons-in-geopandas-or-shapely-into-a-single-geometry

xmin, ymin, xmax, ymax = cc.total_bounds

boardBox = shapely.geometry.Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin), (xmin, ymin)])

boardBoxGPD = gpd.GeoDataFrame([1], geometry=[boardBox], crs="EPSG:32717")

boardBoxGPD.boundary.plot(ax=plt.gca(), color='purple')


ee = gpd.clip(forest1990, boardBoxGPD)

## not working - there are some invalidities, it is insisting on evaluating before 
## doing the cut, even though they are out of my area of interest.
 
## can we do this with ogr2ogr, in the shell?

boardBoxGPD.drop(0, axis=1, inplace=True) ## integer column, confuses fiona
boardBoxGPD.to_file('boundBoxNorthernEx.shp', driver='ESRI Shapefile')

## in shell:
ogr2ogr -f GeoJSON -clipsrc boundary.geojson datatobeclipped.geojson

forest1990Path="/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/v_ff010_cobertura_vegetal_1990_a/v_ff010_cobertura_vegetal_1990_aPolygon.shp"


ogr2ogr \
    -spat 599920 9987285 886203 10160819 \
    -f "ESRI Shapefile" test.shp \
    $forest1990Path

## nope, some problems with the area_super column. Let's drop it and 
## see what happens

forest1990.drop("area_super", axis=1, inplace=True)
#forest1990.to_file('forest1990.shp', driver='ESRI Shapefile')

## and back to shell:

ogr2ogr \
    -spat 599920 9987285 886203 10160819 \
    -f "ESRI Shapefile" test.shp \
    forest1990.shp

ogr2ogr \
    -clipsrc -5.8 41 10 51.5 \
    -f "ESRI Shapefile" \
    test.shp \
    forest1990.shp

## have to remember to clear this file before doing commit

## did this work?:

littleForestbox = gpd.read_file('test.shp')

littleForestbox.head()

littleForestbox.shape

littleForestbox.plot()

ee = gpd.clip(littleForestbox, cc)


## let's look a little closer at the invalidities
## try this:

forest1990 = gpd.read_file('forest1990.shp')
vcheck = forest1990.is_valid
## we might need this for later:
#vcheck.to_pickle('forest1990vcheck.p')


invals = forest1990[~vcheck] ## shit. thousands

## which of these are in our bounding box?
inValsNorth = invals.cx[xmin:xmax, ymin:ymax] ## shit 416

## what does it look like if we remove these?

smallForest = forest1990[vcheck].cx[xmin:xmax, ymin:ymax] 
inValsNorth.plot()
## yeah, something is weird. Let's not remove them unless we 
## really need to.

## this will almost certainly break things, but let's try:
#buffForest = forest1990.buffer(0.00000001)

buffInvalsNorth = inValsNorth.buffer(0.00000001)

## yeah, none of that works. 

## new approach - cotacachi canton?

pathCanton = "/home/daniel/Documents/LosCed/Ecuador_GIS/limites_administrativas/nxcantones.shp"
cotacachi = (gpd.read_file(pathCanton)[['DPA_DESCAN','geometry']]
        .set_index('DPA_DESCAN')
        .loc[['COTACACHI']]
    )


forest1990Path = ("/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/"
        "v_ff010_cobertura_vegetal_1990_a/v_ff010_cobertura_vegetal_1990_aPolygon.shp")
forest1990 = gpd.read_file(forest1990Path) ## whoah that be big

vcheck = pd.read_pickle('forest1990vcheck.p')

invals = forest1990[~vcheck]

cotacachi.total_bounds

forest1990.total_bounds

## maybe knock it down a little:
halfForest = forest1990.cx[:, 10020000:]

halfForest.shape
forest1990.shape

## now get every polygon in the all-Ecuador landcover file that 
## contacts cotacachi?
 
## there is an example here : 


s = gpd.GeoSeries(
    [
        shapely.geometry.Polygon([(0, 0), (2, 2), (0, 2)]),
        shapely.geometry.Polygon([(0, 0), (2, 2), (0, 2)]),
        shapely.geometry.Polygon([(10, 10), (2, 2), (0, 2)]),
    ]
)

s.intersection(shapely.geometry.Polygon([(0, 0), (1, 1), (0, 1)]))


#forestXcotacachi = halfForest.geometry.intersects(cotacachi.loc[['COTACACHI']].geometry[0])

## should work, but I want to see what's going on. Iterate manually through them:

forestXcotacachi = []
for i,j in enumerate(halfForest.geometry):
    print(i)
    test = j.intersects(cotacachi.loc[['COTACACHI']].geometry[0])
    forestXcotacachi += [test]

touchCot = halfForest[forestXcotacachi]

#halfForest.to_pickle('halfForest.p')

touchCot.to_pickle('touchCot1990.p')

touchCot = pd.read_pickle('touchCot.p')

len( forestXcotacachi )

fig, ax = plt.subplots()
touchCot.plot(ax=ax)
cotacachi.boundary.plot(ax=ax, color='red')
lcPoly.boundary.plot(ax=ax, color='purple')

aa = touchCot.is_valid
## much better, only 36 invalid polygons

fig, ax = plt.subplots()
touchCot[aa].plot(color='blue',ax=ax)
touchCot[~aa].plot(color='red',ax=ax)
cotacachi.boundary.plot(ax=ax, color='black')
lcPoly.boundary.plot(ax=ax, color='purple')

shapely.validation.explain_validity(touchCot.loc[305017].geometry)

bb = touchCot.loc[305017].geometry.buffer(.00000001)


## so strategies here:

## constrict even farther, to Garcia Moreno, 
## try rasterizing
## fix manually with qgis. 

## I don't really like any of them. I would like 
## the entire canton, I am tired of artificial 
## comparisons with tiny BPs in the Intag. LC
## blows them out of the water for habitat
## and diversity and this needs to be shown. 

## rastering a polgon data set that was probably
## vectorized from a raster seems silly. 

## but correcting by hand may take a while. 
## we will presumably have to repeat for the 
## 2018 data set. Let's see:

forest2018Path = ("/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/"
    "v_ff010_cobertura_vegetal_2018_a/v_ff010_cobertura_vegetal_2018_aPolygon.shp")

forest2018 = gpd.read_file(forest2018Path)

vcheck2018 = forest2018.is_valid

vcheck1990 = pd.read_pickle('forest1990vcheck.p')

sum(~vcheck2018) 
sum(~vcheck1990) 

## there are 1/2 the invalidities in general in this data
## still over 1200 invalid polygons, but seems cleaner. 
## wonder if this is due to better tech and GIS methods
## or if the landscape has just been simplified so much
## by cutting...

## anyway, how many of these invalid polygons are in 
## cotacachi canton?

halfForest2018 = forest2018.cx[:, 10020000:]

forest2018Xcotacachi = []
for i,j in enumerate(halfForest2018.geometry):
    print(i)
    test = j.intersects(cotacachi.loc[['COTACACHI']].geometry[0])
    forest2018Xcotacachi += [test]

halfForest2018.head()

halfForest2018.shape

halfForest2018.plot()


touchCot2018 = halfForest2018[forest2018Xcotacachi]

touchCot2018.shape

touchCot2018.to_pickle('touchCot2018.p')

touchCot2018val = touchCot2018.is_valid

len(touchCot2018val)

sum(~touchCot2018val) ## 15 invalid polygons.
## that's manageable. 

fig, ax = plt.subplots()
touchCot2018.plot(ax=ax)
cotacachi.boundary.plot(color='red', ax=ax)
touchCot2018[~touchCot2018val].plot(color='black', ax=ax)

touchCot2018invals = touchCot2018[~touchCot2018val]

## so not that many, but they are big and complex. repairing might
## not be easy. And they can't be left out. 

## explore options for rasterizing. Geopandas is probably not 
## great here, I think it insists on all polygons being valid 
## before attempting the the conversion. Try lower level 
## osgeo tools.

## let's write out the two maps of landuse polygons that 
## touch cotacachi, from 1990 and 2018:

touchCot1990 = pd.read_pickle('touchCot1990.p')
touchCot2018 = pd.read_pickle('touchCot2018.p')
#touchCot1990.to_file('touchCot1990.shp', driver='ESRI Shapefile')
#touchCot2018.to_file('touchCot2018.shp', driver='ESRI Shapefile')

## and then we want to use gdal tools to try to rasterize
## https://gdal.org/programs/gdal_rasterize.html

touchCot1990.head()
touchCot1990.tail()

## looks like there are different categorical systems:
touchCot1990.cobertura_.unique()
touchCot1990.cobertura1.unique()
touchCot1990.cobertura0.unique()
## I think we want "_" or "1" here.

## just keep this

aa = touchCot1990[["cobertura1","geometry"]].reset_index(drop=True)
bb = pd.Categorical(aa['cobertura1'])
aa["cob1codes"] = bb.codes

aa.head()

#aa.to_file('touchCot1990.shp', driver='ESRI Shapefile')

## convert this to a number for the rasterization?

gdal_rasterize

gdal_rasterize -a cob1codes -tr 100 100 touchCot1990.shp touchCot1990.tif

## that worked surprising well. 
## maybe we should back up, and do the rasterization on the originals
## let's see if that eats up my computer.

forest1990Path = ("/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/"
        "v_ff010_cobertura_vegetal_1990_a/v_ff010_cobertura_vegetal_1990_aPolygon.shp")
forest1990 = gpd.read_file(forest1990Path) ## whoah that be big
aa = forest1990[["cobertura1","geometry"]]
bb = pd.Categorical(aa['cobertura1'])
aa.insert(1,"cob1codes",bb.codes)

#aa.to_file('forest1990coded.shp', driver='ESRI Shapefile')

## we need to keep track of these codes:
codes1990 = bb.copy()
codes1990category = (codes1990.categories
                    .to_series()
                    .reset_index(drop=True)
                        )
codes1990category.to_pickle('codes1990.p')

codes1990

## yeah, that breaks things. 

## what if we change the extent, does that help?
## get the extent of the cotacachi canton:

cotacachi.total_bounds

# [ 720706.87410164, 10022496.54190254,   810014.3900013 , 10064965.42180061]
# xmin, ymin, xmax, ymax

cotacachi.plot()

## back in shell
## Can we get down to a 30x30 m resolution? This was probably the original resolution. 
gdal_rasterize \
    -a cob1codes \
    -te 720000 10020000 811000 10070000 \
    -tr 30 30 \
    forest1990coded.shp \
    forest1990CotacachiCoded.tif


## check it 
forest1990rastTrimmed = rasterio.open("GIS/forest1990CotacachiCoded.tif")
fig, ax = plt.subplots()
rasterio.plot.show(forest1990rastTrimmed, ax=ax)
cotacachi.boundary.plot(ax=ax, color="black")
lcPoly.boundary.plot(ax=ax, color="purple")

## looks good. repeat for the 2018 data:
forest2018Path = ("/home/daniel/Documents/LosCed/GISfilesTooBigForGithub/"
        "v_ff010_cobertura_vegetal_2018_a/v_ff010_cobertura_vegetal_2018_aPolygon.shp")
forest2018 = gpd.read_file(forest2018Path) ## whoah that be big

aa = forest2018[["cobertura0","geometry"]]
bb = pd.Categorical(aa['cobertura0'])
aa.insert(1,"cob0codes",bb.codes)
#aa.to_file('forest2018coded.shp', driver='ESRI Shapefile')

## codes are a little different:
forest2018.cobertura_.unique()
forest2018.cobertura0.unique()
## pretty sure the most comparable codes are
## cobertura0

## we need to keep track of these codes, too:
codes2018 = bb.copy()
codes2018category = (codes2018.categories
                    .to_series()
                    .reset_index(drop=True)
                        )
codes2018category.to_pickle('codes2018.p')

## in shell
gdal_rasterize \
    -a cob0codes \
    -te 720000 10020000 811000 10070000 \
    -tr 30 30 \
    forest2018coded.shp \
    forest2018CotacachiCoded.tif

## check it 
forest2018rast = rasterio.open("GIS/forest2018CotacachiCoded.tif")
xmin, ymin, xmax, ymax = forest2018rast.bounds
fig, ax = plt.subplots()
ccPoly.boundary.plot(ax=ax, color="purple")
rasterio.plot.show(forest2018rastTrimmed, ax=ax)
cotacachi.boundary.plot(ax=ax, color="black")
lcPoly.boundary.plot(ax=ax, color="purple")
chontalPoly.boundary.plot(ax=ax, color="purple")
cebuPoly.boundary.plot(ax=ax, color="purple")
ax.set_ylim(ymin, ymax)
ax.set_xlim(xmin, xmax)

def plotLandUse(rast, tit=''):
    xmin, ymin, xmax, ymax = rast.bounds
    fig, ax = plt.subplots()
    ccPoly.boundary.plot(ax=ax, color="purple")
    rasterio.plot.show(rast, ax=ax)
    cotacachi.boundary.plot(ax=ax, color="black")
    lcPoly.boundary.plot(ax=ax, color="purple")
    chontalPoly.boundary.plot(ax=ax, color="purple")
    cebuPoly.boundary.plot(ax=ax, color="purple")
    ax.set_ylim(ymin, ymax)
    ax.set_xlim(xmin, xmax)
    ax.set_title(tit)

plotLandUse(forest1990rast, 'Forest1990')
plotLandUse(forest2018rast, 'Forest2018')


plt.scatter(1,1,c=5)

cbar = plt.gcf().colorbar(codes1990)
help(rasterio.plot.show)

## can we also get the BP chontal and CC reserve on there?

pathCanton = "/home/daniel/Documents/LosCed/Ecuador_GIS/limites_administrativas/nxcantones.shp"
bpPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/car_bosques_protectores.shp")
snapPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/snap.shp")
chontalPoly = gpd.read_file(bpPath).query("nombre == 'INTAG (EL CHONTAL)'")
cebuPoly = gpd.read_file(bpPath).query("nombre == 'CEBU'")
ccPoly = (gpd.read_file(snapPath).query("nombre == 'Cotacachi Cayapas'")
            .to_crs('EPSG:32717'))
cotacachi = (gpd.read_file(pathCanton)[['DPA_DESCAN','geometry']]
        .set_index('DPA_DESCAN')
        .loc[['COTACACHI']])

## great. 

## we need to bring the color codes together.

codes1990 = pd.read_pickle('codes1990.p')
codes2018 = pd.read_pickle('codes2018.p')

codes1990
codes2018

## we have some problems with the accents in the 1990 data, 
## I can't even reliably replace them with the str tools, for some reason,
## so let's replace them here:
codes1990[11] = "PLANTACION FORESTAL"
codes1990[12] = "PARAMO"
codes1990[13] = "SIN INFORMACION"
codes1990[15] = "VEGETACION ARBUSTIVA"
codes1990[16] = "VEGETACION HERBACEA"

## and update our pickled series with this:
#codes1990.to_pickle('codes1990.p')

codes1990 = pd.read_pickle('codes1990.p')
codes2018 = pd.read_pickle('codes2018.p')
## let's reshape these a bit:
codes1990flip = pd.Series(data=codes1990.index, index=codes1990.values)
codes2018flip = pd.Series(data=codes2018.index, index=codes2018.values)


## best to use a color map that looks like the online version, 
## I guess?
## can change it later if we need to:

## for 2018:

codes2018flip

## each landuse type (NOT the numbercodes of the raster bands)
## needs a unique and informative color
## let's color all agricultural land the same

landUseColDict = pd.Series({
"AREA POBLADA":"#191919",
"AREA SIN COBERTURA VEGETAL":"#bfbac1",
"ARTIFICIAL":"#1676cc",
"BOSQUE NATIVO":"#4d701b",
"GLACIAR":"#76acbf",
"INFRAESTRUCTURA":"#676767",
"NATURAL":"#53c1ea",
"PARAMO":"#69d19b",
"PLANTACION FORESTAL":"#9a3520",
"TIERRA AGROPECUARIA":"#e3d686",
"VEGETACION ARBUSTIVA":"#966b17",
"VEGETACION HERBACEA":"#cec3ae",
"CULTIVO ANUAL":"#e3d686",
"CULTIVO PERMANENTE":"#e3d686",
"CULTIVO SEMI PERMANENTE":"#e3d686",
"PASTIZAL":"#67751b",
"SIN INFORMACION":"#ffffff",
})

colrDF = pd.concat([codes1990flip, codes2018flip,landUseColDict], axis=1)
colrDF.reset_index(drop=False, inplace=True)
colrDF.columns=['landUse','codes1990', 'codes2018','co']
colrDF['codes2018'] = colrDF['codes2018'].astype('Int32')
colrDF['codes2018'] = colrDF['codes2018'].astype('Int32')
colrDF.iloc[13,2] = 13 ## add in a "SIN INFORMACION" to 2018 data
#colrDF.to_pickle('forestRastColrDF.p')


## I think for simplicity with color scales, we need to fill 
## in the NAs with a color - 

## damn this is stupidly complicated...
## anyway, how do we use this for colormaps?

## if we want to plot the 2018 data?

## so now what? A good tutorial is here: 
## https://kodu.ut.ee/~kmoch/geopython2019/L4/raster.html#data-structures-recap-raster-and-vector

forest2018rast = rasterio.open("GIS/forest2018CotacachiCoded.tif")
forest1990rast = rasterio.open("GIS/forest1990CotacachiCoded.tif")

forest1990band = forest1990rast.read(1)
forest2018band = forest2018rast.read(1)

## which land types are present in our little
## bounding box?
uVals2018 = np.unique(forest2018band).astype('int')

## example for legend:

from matplotlib.patches import Patch

from matplotlib.patches import Patch

list(zip(colrs2018.co, colrs2018.landUse))

## make our color map, only types present in this 
## subset of the map


## we have to "patch up" our color map a bit,
## pad it with colors/landuses that aren't in 
## our subsetted data, so that the color map
## has a length that equals the values of our
## various pixels. If we have pixel values 
## going from 0-11, we need twelve color assignments
## so that our colormap doesn't try weird 
## interpolations from the two sides of the map
## being unequal. In this case, we'll use red 
## as our filler color, since it's not used 
## elsewhere:

plt.close('all')

def plot2018rast():
    uVals2018 = np.unique(forest2018band).astype('int')
    colrs2018 = (colrDF.dropna()
                    .set_index('codes2018')
                    .drop('codes1990', axis=1)
                )
    aa = ["#ff0000"]*max(uVals2018 + 1)
for i in uVals2018:
    iCol = colrDF.query("codes2018 == " + str(i)).co.iloc[0] ## get our real colors
    aa[i] = iCol ## put them where they belong 
cmap2018 = mcolors.ListedColormap(aa)
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrs2018.co, colrs2018.landUse) ]
    fig, ax = plt.subplots()
    ax.legend(handles=legend_patches, 
                facecolor="white", 
                edgecolor="white", 
                loc="upper left", 
                bbox_to_anchor=(1, 1))
    rasterio.plot.show(forest2018rast, ax=ax, cmap=cmap2018)
    ax.set_title('Land use 2018')

forest2018rast

xmin, ymin, xmax, ymax = forest2018rast.bounds
ax.set_xlim(xmin, xmax)
ax.set_ylim(ymin, ymax)

#### repeat for 1990:

## This is one is simpler, we have no 
## gaps in our color map, so no padding 
## with unused colors needed...

#plt.close('all')

def plot1990rast():
    uVals1990 = np.unique(forest1990band).astype('int')
    colrs1990 = (colrDF
                    .drop('codes2018', axis=1)
                    .set_index('codes1990')
                )
    cmap1990 = mcolors.ListedColormap(colrs1990.co.to_list())
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrs1990.co, colrs1990.landUse) ]
    fig, ax = plt.subplots()
    ax.legend(handles=legend_patches, 
                facecolor="white", 
                edgecolor="white", 
                loc="upper left", 
                bbox_to_anchor=(1, 1))
    rasterio.plot.show(forest1990rast, ax=ax, cmap=cmap1990)
    ax.set_title('Land use 1990')

fig = plt.gcf()
ax = plt.gca()

pathCanton = "/home/daniel/Documents/LosCed/Ecuador_GIS/limites_administrativas/nxcantones.shp"
bpPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/car_bosques_protectores.shp")
snapPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/snap.shp")

chontalPoly = gpd.read_file(bpPath).query("nombre == 'INTAG (EL CHONTAL)'")
cebuPoly = gpd.read_file(bpPath).query("nombre == 'CEBU'")
ccPoly = (gpd.read_file(snapPath).query("nombre == 'Cotacachi Cayapas'")
            .to_crs('EPSG:32717'))
cotacachi = (gpd.read_file(pathCanton)[['DPA_DESCAN','geometry']]
        .set_index('DPA_DESCAN')
        .loc[['COTACACHI']])

## save these out, the ones we don't have already
ccPoly.to_file("GIS/cotacachiCayapas.geojson", driver='GeoJSON')
cotacachi.to_file("GIS/cotacachiCanton.geojson", driver='GeoJSON')
chontalPoly.to_file("GIS/ChontalBP.geojson", driver='GeoJSON')
cebuPoly.to_file("GIS/cebuBP.geojson", driver='GeoJSON')

ccPoly = gpd.read_file("GIS/cotacachiCayapas.geojson", driver='GeoJSON')
cotacachi = gpd.read_file("GIS/cotacachiCanton.geojson", driver='GeoJSON')
chontalPoly = gpd.read_file("GIS/ChontalBP.geojson", driver='GeoJSON')
cebuPoly = gpd.read_file("GIS/cebuBP.geojson", driver='GeoJSON')

plt.close('all')
fig, ax = plt.subplots()

lcPoly.boundary.plot(ax=ax, color='r')
ccPoly.boundary.plot(ax=ax, color='k')

cotacachi.plot(ax=ax,  
               missing_kwds={'facecolor':"white",
                             "edgecolor":"black"
                             })
## yeah, doesn't work

chontalPoly.boundary.plot(ax=ax, color = 'b')
cebuPoly.boundary.plot(ax=ax, color = 'b')

a, = [1,2,3]

## as per:
## https://www.hatarilabs.com/ih-en/extract-point-value-from-a-raster-file-with-python-geopandas-and-rasterio-tutorial

def getLandVal(rast):
    x,y = plt.ginput(1)[0]
    row, col = forest1990rast.index(x,y)
    numV = rast.read(1)[row, col]
    return(numV)


getLandVal(forest2018rast)

getLandVal(forest1990rast)

colrDF

plt.tight_layout()

## toy example to explore how listed colormaps work:

plt.close('all')
cmapTest = mcolors.ListedColormap(['r','g','b','m','k'])
#cmapTest = mcolors.ListedColormap(['r','g'])
z1 = [1,1,3,2,6]
z2 = [1,2,5,2.1,3]
z3 = [4.5,2,4.5,2.1,3]
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(x=[1,]*len(z1),y=z1, s=500, cmap=cmapTest,
            c=z1)
ax.scatter(x=[1.5]*len(z2),y=z2, s=500, cmap=cmapTest,
            c=z2)
ax.scatter(x=[2]*len(z3),y=z3, s=500, cmap=cmapTest,
            c=z3)

## with our 1990 values:

uVals1990

colrs1990

cmap1990

#cmapTest = mcolors.ListedColormap(['r','g'])

uVals2018 = np.unique(forest2018band).astype('int')
colrs2018 = (colrDF.dropna()
                .set_index('codes2018')
                .drop('codes1990', axis=1)
            )
cmap2018 = mcolors.ListedColormap(colrs2018.co.to_list())

pd.Series(uVals2018).rank()

## we need a color map that will give us a 
## value for all potential 2018 values, not 
## just the ones for our map
## we need one that goes at least to highest
## value of uVals2018, with a non-info color wherever 
## there is a gap in the uVals2018 sequence
## let's use red (#ff0000)
## this is getting really ridiculous

aa = ["#ff0000"]*max(uVals2018 + 1)
for i in uVals2018:
    print(i)
    iCol = colrDF.query("codes2018 == " + str(i)).co.iloc[0]
    aa[i] = iCol

## can we use this?
cmap2018 = mcolors.ListedColormap(aa)


cmap2018 = mcolors.ListedColormap(colrs2018.co.to_list())

#
 

def colrMapP():
    z1 = uVals1990
    z2 = uVals2018
    fig, ax = plt.subplots(figsize=(10,10))
    ax.scatter(x=[1.0,]*len(z1),y=z1, s=500, cmap=cmap1990,
                c=z1)
    ax.scatter(x=[1.5,]*len(z2),y=z2, s=500, cmap=cmap2018,
                c=z2)
    for i in uVals1990:
        ax.text(x=0.95, y=i, s=i)
        ax.text(x=1.05, y=i, s=colrs1990.loc[i]['landUse'])
    for i in uVals2018:
        ax.text(x=1.45, y=i, s=i)
        ax.text(x=1.55, y=i, s=colrs2018.loc[i]['landUse'])
    ax.set_xlim(0.8, 2.0)

#plt.close('all')
colrMapP()

plt.close('all')

fig, ax = plt.subplots(figsize=(20,10))
legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrs1990.co, colrs1990.landUse) ]
ax.legend(handles=legend_patches, 
            facecolor="white", 
            edgecolor="white", 
            loc="upper left", 
            bbox_to_anchor=(1, 1)) 
rasterio.plot.show(forest1990rast, ax=ax, cmap=cmap1990)
lcPoly.boundary.plot(ax=ax, color='r')
ccPoly.boundary.plot(ax=ax, color='k')
cotacachi.boundary.plot(ax=ax, color='k')
chontalPoly.boundary.plot(ax=ax, color = 'b')
cebuPoly.boundary.plot(ax=ax, color = 'b')
xmin, ymin, xmax, ymax = forest1990rast.bounds
ax.set_xlim(xmin, xmax)
ax.set_ylim(ymin, ymax)
ax.set_title("1990 Cotacachi Canton land cover")
anaPt.plot(ax=ax)
paths.plot(color='red', linestyle='--', ax=ax)


def plotCotacachiRast(rast, colrDF, colormap, title=None):
    fig, ax = plt.subplots(figsize=(20,10))
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrDF.co, colrDF.landUse) ]
    ax.legend(handles=legend_patches, 
                facecolor="white", 
                edgecolor="white", 
                loc="upper left", 
                bbox_to_anchor=(1, 1)) 
    rasterio.plot.show(rast, ax=ax, cmap=colormap)
    lcPoly.boundary.plot(ax=ax, color='r')
    ccPoly.boundary.plot(ax=ax, color='k', linestyle='--')
    cotacachi.boundary.plot(ax=ax, color='k')
    chontalPoly.boundary.plot(ax=ax, color = 'b')
    cebuPoly.boundary.plot(ax=ax, color = 'b')
    xmin, ymin, xmax, ymax = forest2018rast.bounds
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    if title is None: ax.set_title("")
    if title is not None: ax.set_title(title + " Cotacachi canton land cover")

plotCotacachiRast(forest1990rast, colrs1990, cmap1990, "1990")

plotCotacachiRast(forest2018rast, colrs2018, cmap2018, "2018")


#### mask rasters with Cotacachi canton boundary ####

## not bad, but maybe try masking to cotacachi polygon?
## we want to clip with the cotacachi polygon

## an example here:
## https://automating-gis-processes.github.io/CSC18/lessons/L6/clipping-raster.html

forest2018rast = rasterio.open("GIS/forest2018CotacachiCoded.tif")
forest1990rast = rasterio.open("GIS/forest1990CotacachiCoded.tif")

forest1990maskArray, forest1990maskTransform = rasterio.mask.mask(dataset=forest1990rast, 
                                                           shapes=cotacachi.geometry,
                                                           nodata=13, ## = "SIN INFORMACION"
                                                           crop=True
                                                           )

## looks good, but how to embed the geo info, and make a new georeffed raster?

forest1990mask_meta = forest1990rast.meta.copy()

forest1990mask_meta.update({"driver": "GTiff",
           "height": forest1990maskArray.shape[1],
           "width": forest1990maskArray.shape[2],
           "transform": forest1990maskTransform,
           "crs": rasterio.crs.CRS.from_epsg(32717)}
          )

## write it out:

with rasterio.open("cotacachiForest1990.tif", "w", **forest1990mask_meta) as dest:
    dest.write(forest1990maskArray)
         
## did that work?

cotacachiForest1990 = rasterio.open("cotacachiForest1990.tif")

fig, ax = plt.subplots()
rasterio.plot.show(cotacachiForest1990, ax=ax)
ax.set_title('1990')

## looks great. We'll probably need to rework our colormaps for all the new zeros. 

#### repeat with the 2018 data:

forest2018maskArray, forest2018maskTransform = rasterio.mask.mask(dataset=forest2018rast, 
                                                           shapes=cotacachi.geometry,
                                                           nodata=13,
                                                           crop=True
                                                           )


forest2018mask_meta = forest2018rast.meta.copy()

forest2018mask_meta.update({"driver": "GTiff",
           "height": forest2018maskArray.shape[1],
           "width": forest2018maskArray.shape[2],
           "transform": forest2018maskTransform,
           "crs": rasterio.crs.CRS.from_epsg(32717)}
          )

with rasterio.open("cotacachiForest2018.tif", "w", **forest2018mask_meta) as dest:
    dest.write(forest2018maskArray)

forest2018mask_meta = forest2018rast.meta.copy()

cotacachiForest2018 = rasterio.open("cotacachiForest2018.tif")

fig, ax = plt.subplots()
rasterio.plot.show(cotacachiForest2018)
ax.set_title('2018')

## also maybe we can get an estimate of the secondary forest in LC - 
## this would be areas that are covered by lc in the 2018 data,
## vs. the deforested areas of 1990

#### zoom in a little to LC and Chontal BPs ####
## can we get the boundaries 

lcPoly.bounds
chontalPoly.bounds

## looks like we want something around:

min(lcPoly.bounds.minx[0], chontalPoly.bounds.minx[0]) 
min(lcPoly.bounds.miny[0], chontalPoly.bounds.miny[0]) 
max(lcPoly.bounds.maxx[0], chontalPoly.bounds.maxx[0]) 
max(lcPoly.bounds.maxy[0], chontalPoly.bounds.maxy[0]) 

def plotBPs(rast, colrDF, colormap):
    minx = 739306.4581787158
    miny = 10031570.357894145
    maxx = 761118.852
    maxy = 10044637.5948
    margin = 1000 ## buffer a little
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrDF.co, colrDF.landUse) ]
    fig, ax = plt.subplots(figsize=(20,10))
    ax.legend(handles=legend_patches, 
                facecolor="white", 
                edgecolor="white", 
                loc="upper left", 
                bbox_to_anchor=(1, 1)) 
    rasterio.plot.show(rast, ax=ax, cmap=colormap)
    lcPoly.boundary.plot(ax=ax, color='r')
    chontalPoly.boundary.plot(ax=ax, color = 'b')
    ax.set_xlim(minx-margin, maxx+margin)
    ax.set_ylim(miny-margin, maxy+margin)
 

colrDF = pd.read_pickle('forestRastColrDF.p')
colrs1990 = (colrDF.set_index('codes1990')
                .drop('codes2018', axis=1)
            )
cmap1990 = mcolors.ListedColormap(colrs1990.co.to_list())

cotacachiForotacachiForest2018.read(1)
uVals2018 = np.unique(cotacachiForest2018band).astype('int')
colrs2018 = (colrDF.dropna()
                .set_index('codes2018')
                .drop('codes1990', axis=1)
            )
aa = ["#ff0000"]*max(uVals2018 + 1)
for i in uVals2018:
    ## get our real colors:
    iCol = colrDF.query("codes2018 == " + str(i)).co.iloc[0]
    aa[i] = iCol ## put them where they belong 
cmap2018 = mcolors.ListedColormap(aa)
cotacachiForest2018band = cotacachiForest2018.read(1)
uVals2018 = np.unique(cotacachiForest2018band).astype('int')
colrs2018 = (colrDF.dropna()
                .set_index('codes2018')
                .drop('codes1990', axis=1)
            )
aa = ["#ff0000"]*max(uVals2018 + 1)
for i in uVals2018:
    ## get our real colors:
    iCol = colrDF.query("codes2018 == " + str(i)).co.iloc[0]
    aa[i] = iCol ## put them where they belong 
cmap2018 = mcolors.ListedColormap(aa)

plotBPs(forest1990rast, colrs1990, cmap1990)

plotBPs(forest2018rast, colrs2018, cmap2018)

##### update plotter to match cotacachi canton mask #####

## redo 37 or whatever. Can we do the above plotting with the 
## masked rasters that just show cotacachi canton? I think
## showing CCER is just distracting, subtracts from the 
## severity of the situation by weighting half the image
## with  the largest chunk of unbroken, actually-protected
## forest in northern Ecuador.


## color maps

## 1990
cotacachiForest1990 = rasterio.open("cotacachiForest1990.tif")
cotacachiForest1990band = cotacachiForest1990.read(1)
uVals1990 = np.unique(cotacachiForest1990band).astype('int')

colrs1990 = (colrDF
               .drop('codes2018', axis=1)
               .set_index('codes1990')
             )

cmap1990 = mcolors.ListedColormap(colrs1990.co.to_list())

## 2018
cotacachiForest2018 = rasterio.open("cotacachiForest2018.tif")
cotacachiForest2018band = cotacachiForest2018.read(1)
uVals2018 = np.unique(cotacachiForest2018band).astype('int')
colrs2018 = (colrDF.dropna()
                .set_index('codes2018')
                .drop('codes1990', axis=1)
            )
aa = ["#ff0000"]*max(uVals2018 + 1)
for i in uVals2018:
    ## get our real colors:
    iCol = colrDF.query("codes2018 == " + str(i)).co.iloc[0] 
    aa[i] = iCol ## put them where they belong 
cmap2018 = mcolors.ListedColormap(aa)

## check them:

z1 = uVals1990
z2 = uVals2018
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(x=[1.0,]*len(z1),y=z1, s=500, cmap=cmap1990,
            c=z1)
ax.scatter(x=[1.5,]*len(z2),y=z2, s=500, cmap=cmap2018,
            c=z2)
for i in uVals1990:
    ax.text(x=0.95, y=i, s=i)
    ax.text(x=1.05, y=i, s=colrs1990.loc[i]['landUse'])

for i in uVals2018:
    ax.text(x=1.45, y=i, s=i)
    ax.text(x=1.55, y=i, s=colrs2018.loc[i]['landUse'])

ax.set_xlim(0.8, 2.0)

## looks good 
## redo maps

cotacachiForest1990 = rasterio.open("cotacachiForest1990.tif")
cotacachiForest2018 = rasterio.open("cotacachiForest2018.tif")

def plotCotacachiRast(rast, colrDF, colormap, title=None, ax=None):
    if ax is None: fig, ax = plt.subplots(figsize=(20,10))
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrDF.co, colrDF.landUse) ]
    ax.legend(handles=legend_patches,
                facecolor="white",
                edgecolor="white",
                loc="upper left",
                bbox_to_anchor=(1, 1))
    rasterio.plot.show(rast, ax=ax, cmap=colormap)
    lcPoly.boundary.plot(ax=ax, color='r')
    chontalPoly.boundary.plot(ax=ax, color = 'b')
    cebuPoly.boundary.plot(ax=ax, color = 'b')
    xmin, ymin, xmax, ymax = forest2018rast.bounds
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    if title is None: ax.set_title("")
    if title is not None: ax.set_title(title + " Cotacachi canton land cover")

## does a single figure work better?
plt.close('all')
fig, axes = plt.subplots(2,1, sharex=True)
plotCotacachiRast(cotacachiForest1990, colrs1990, cmap1990, title=None, ax=axes[0])
plotCotacachiRast(cotacachiForest2018, colrs2018, cmap2018, title=None, ax=axes[1])
## either way. 

### can we plot our forest mem over this 1990 forest raster?

def getColorMap(rast, yearCodes):
    rastband = rast.read(1)
    uVals = np.unique(rastband).astype('int')
    colrs = (colrDF.set_index(yearCodes)
                 .loc[uVals]['co']
                 .to_list()
                 )
    return(mcolors.ListedColormap(colrs))

def getBoundaries(raster):
    aa = np.unique(raster.read(1))
    bb = aa - 0.5
    cc = np.append(bb,aa.max()+0.5)
    return(mcolors.BoundaryNorm(cc, len(aa)))

def plotRast(rast, yearCodes, title=None, ax=None, plotBPs=False):
    if ax is None: fig, ax = plt.subplots(figsize=(20,10))
    rastband = rast.read(1)
    uVals = np.unique(rastband).astype('int')
    colrDF_thisyear = colrDF.set_index(yearCodes)
    colrDF_thisyear = colrDF_thisyear[colrDF_thisyear.index.notnull()].loc[uVals]
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrDF_thisyear.co, colrDF_thisyear.landUse) ]
    ax.legend(handles=legend_patches,
                facecolor="white",
                edgecolor="black",
                loc="upper left",
                bbox_to_anchor=(1, 1))
    colormap =  getColorMap(rast, yearCodes)
    colorBounds = getBoundaries(rast)
    rasterio.plot.show(rast, ax=ax, cmap=colormap, norm = colorBounds)
    if plotBPs == True:
            lcPoly.boundary.plot(ax=ax, color='r')
            chontalPoly.boundary.plot(ax=ax, color = 'b')
            cebuPoly.boundary.plot(ax=ax, color = 'b')
    xmin, ymin, xmax, ymax = rast.bounds
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    if title is None: ax.set_title("")
    if title is not None: ax.set_title(title)

def plotMEM(mem, ax, scale=50, marg=650, lx=745000, ly=10035500, fontsize='large'):
    mem=mem
    ax=ax
    scale=scale
    ms = mems[mem].abs()*scale
    fontsize=fontsize
    marg = marg
    negPos = mems[mem].copy()
    negPos[negPos >= 0 ] = 1
    negPos[negPos < 0 ] = -1
    cDict = { 1 : "white", -1 : "black" }
    edgeCols = [ cDict[i] for i in negPos ]
    cmap = mcolors.ListedColormap(['white','black'])
    mems.plot(column=mem,
        c=negPos,
        cmap=cmap,
        edgecolors=edgeCols,
        ax=ax,
        markersize = ms,
        ## these are to center our color map to -1,1 w/b scheme:
        vmin=-1., vmax=1.,
        )
    ax.set_xlim(mems.total_bounds[0]-marg, mems.total_bounds[2]+marg)
    ax.set_ylim(mems.total_bounds[1]-marg, mems.total_bounds[3]+marg)
    ax.text(lx,ly,mem, fontsize=fontsize)

mems = gpd.read_file('mems.geojson')


plotMEM(mems, ax=ax)

colrDF = pd.read_pickle('forestRastColrDF.p')
fig, ax = plt.subplots(figsize=(20,20))
plotRast(losCedrosForest1990, 'codes1990', title='Los Cedros 1990', ax=ax, plotBPs=False)




plotRast(losCedrosForest2018, 'codes2018', title='Los Cedros 2018', ax=axes[1], plotBPs=False)
plt.tight_layout()

### stats from deforestation maps ###

## how do we get our forest loss figures from these rasters?

## should be as simple as summing the unique values of our band:

## one example on the numpy manual:
rng = np.random.RandomState(10)  
a = np.hstack((rng.normal(size=1000),
               rng.normal(loc=5, scale=2, size=1000)))
plt.hist(a, bins='auto')  # arguments are passed to np.histogram
plt.title("Histogram with 'auto' bins")
plt.gca().text(0.5, 1.0, "Histogram with 'auto' bins")

## apply to cotacachiForest1990band

plt.hist(cotacachiForest1990band, bins='auto')
## that didn't work at all...too computationally expensive

## can we table our values? in pandas, this would be a pretty 
## simple groupby...


cotacachiForest2018band = cotacachiForest2018.read(1)


## for 1990
cotacachiForest1990band = cotacachiForest1990.read(1)
uVals1990 = np.unique(cotacachiForest1990band).astype('int')
cF1990BandSorted = np.sort(fa)
## find number of pixels for each category of land use:
breaks = np.searchsorted(cF1990BandSorted, uVals1990)
breaks = np.concatenate([breaks, [len(fa)]])
totals1990 = breaks[1:] - breaks[:-1] ## neat - subtract every element by the one before
## how do we resurrect our land use codes/labels for this:
colrDF = pd.read_pickle('forestRastColrDF.p').set_index('codes1990')
landUsePixelSums1990 = (pd.Series(data=totals1990, 
                                 index=colrDF.loc[uVals1990].landUse)
                            .drop('SIN INFORMACION')
                        )

## 
def makePixelTotals(raster, codes):
    band = raster.read(1)
    fa = band.flat
    bandSorted = np.sort(fa)
    uVals = np.unique(band).astype('int')
    ## find number of pixels for each category of land use:
    breaks = np.searchsorted(bandSorted, uVals)
    breaks = np.concatenate([breaks, [len(fa)]])
    totals = breaks[1:] - breaks[:-1] 
    colrDF = pd.read_pickle('forestRastColrDF.p').set_index(codes)
    landUsePixelSums = pd.Series(data=totals, 
                                     index=colrDF.loc[uVals].landUse)
    return(landUsePixelSums)

## are these the same size?
landUseCotacachi1990 = makePixelTotals(cotacachiForest1990, 'codes1990')
landUseCotacachi2018 = makePixelTotals(cotacachiForest2018, 'codes2018')

landUseCotacachi1990.sum() ## 4219826
landUseCotacachi2018.sum() ## 4219826
## yup

## what if we drop the no-info cells?
landUseCotacachi1990 = makePixelTotals(cotacachiForest1990, 'codes1990').drop('SIN INFORMACION')
landUseCotacachi2018 = makePixelTotals(cotacachiForest2018, 'codes2018').drop('SIN INFORMACION')
landUseCotacachi1990.sum() ## 1874454
landUseCotacachi2018.sum() ## 1874623
## the 1990 dataset lost ~200 more cells. This makes sense, I probably shouldn't have lumped 
## sin informacion with out-of-cotacachi pixels. Oh well, fix it later if it causes problems.

landUseCotacachi1990.plot.bar()
plt.tight_layout()

## okay, what has been the percent forest loss in cotacachi?:

landUseCotacachi1990.sum() ## 1874454
landUseCotacachi2018.sum() ## 1874623

landUseCotacachi1990['BOSQUE NATIVO'] / landUseCotacachi1990.sum() ## ~52 percent forest cover
landUseCotacachi2018['BOSQUE NATIVO'] / landUseCotacachi2018.sum() ## ~43 percent forest cover
## not quite 10% forest loss in 28 years. 

## each cell is 30m x 30m, or 900 m^2. So in absolute area terms:

landUseCotacachi1990['BOSQUE NATIVO'] * 900 ## 879673500 or 879,673,500 m^2 
## or in hectares:
landUseCotacachi1990['BOSQUE NATIVO'] * 900 / 10000 ## 87967.35 / 87,967 ha

landUseCotacachi2018['BOSQUE NATIVO'] * 900 ## 717385500 or 717,385,500 m2
landUseCotacachi2018['BOSQUE NATIVO'] * 900 / 10000 ## 71738.55 or 71,738 ha

87967.35 - 71738.55 ## 16228.8 ha lost in 30 years. How big is los Cedros again?

bpPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/car_bosques_protectores.shp")

lc = gpd.read_file(bpPath).query("nombre == 'LOS CEDROS'")
lc.area_ha ## 5255.621582

## so ~ an area of forest equivalent to 3 los Cedros have been lost in the last 30 years. 

## can we calculate the same for Los Cedros and Chontal?

landUseCotacachi1990 = makePixelTotals(cotacachiForest1990, 'codes1990').drop('SIN INFORMACION')
landUseCotacachi2018 = makePixelTotals(cotacachiForest2018, 'codes2018').drop('SIN INFORMACION')
landUseLosCedros1990 = makePixelTotals(losCedrosForest1990, 'codes1990').drop('SIN INFORMACION')
landUseLosCedros2018 = makePixelTotals(losCedrosForest2018, 'codes2018').drop('SIN INFORMACION')
landUseChontal1990 = makePixelTotals(chontalForest1990, 'codes1990').drop('SIN INFORMACION')
landUseChontal2018 = makePixelTotals(chontalForest2018, 'codes2018').drop('SIN INFORMACION')
## these BP rasters are created below

landUseCotacachi1990['BOSQUE NATIVO'] / landUseCotacachi1990.sum()

## now, the best way to visualize this? Stacked barcharts showing change

## really, for publication, it would be good to get clever with 
## d3js here. But let's do a basic barchart of comparisons between 
## 1990 and 2018 with matplotlb

## barchart comparisons of forest with absolute area:
lu1990 = [ landUseCotacachi1990, landUseChontal1990, landUseLosCedros1990 ]
lu2018 = [ landUseCotacachi2018, landUseChontal2018, landUseLosCedros2018 ]

## absolute forest covers, m2:
abs1990 = [ (i['BOSQUE NATIVO'] * 900/10000 ) for i in lu1990 ]
abs2018 = [ (i['BOSQUE NATIVO'] * 900/10000 ) for i in lu2018 ]
## percent forest 
perc1990 = [ (i['BOSQUE NATIVO']*100 / i.sum() ) for i in lu1990 ]
perc2018 = [ (i['BOSQUE NATIVO']*100 / i.sum() ) for i in lu2018 ]

## works
labels = ['Cotacachi', 'Chontal', 'Los Cedros']
plt.close('all')
x = np.arange(len(labels))  # the label locations
width = 0.15  # the width of the bars
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, abs1990, width, label='1990')
rects2 = ax.bar(x + width/2, abs2018, width, label='2018')
ax.set_ylabel('hectares')
ax.set_title('Gain/loss of forest, hectares')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

## indev
def barPlotForestChange(data1990, data2018, yUnits, ax=None, annotate=False):
    labels = ['Cotacachi', 'Chontal', 'Los Cedros']
    x = np.arange(len(labels))  # the label locations
    width = 0.15  # the width of the bars
    if ax is None: fig, ax = plt.subplots()
    rects1 = ax.bar(x - width/2, data1990, width, label='1990')
    rects2 = ax.bar(x + width/2, data2018, width, label='2018')
    ax.set_ylabel(yUnits)
    ax.set_title('Gain/loss of forest, ' + yUnits)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.legend()
    return(fig, ax, width, x)

plt.close('all')
fig, ax, width, xes = barPlotForestChange(abs1990, abs2018, "%")
ax.grid(b=True, which='both', axis='y')

fig, ax, width, xes = barPlotForestChange(perc1990, perc2018, "%")

for i,j in enumerate (perc1990): 
    ax.annotate(s=int(round(j, 2)),
                xy=(x[i]-width,j ), 
                xytext=(0,5),
                textcoords = 'offset points',
                )

for i,j in enumerate (perc2018): 
    ax.annotate(s=int(round(j, 2)),
                xy=(x[i],j ), 
                xytext=(0,5),
                textcoords = 'offset points',
                )


## example here:
## https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py

## update notebook 

##### los cedros and chontal masks:

## let's repeat the above pipeline for cutting the raster down to 
## a polygon, this time with LC and Chontal BPs instead of the 
## Cotacachi canton polygon:

### LC

##  LC 2018
LC2018maskArray, LC2018maskTransform = rasterio.mask.mask(dataset=forest2018rast,
                                                           shapes=lcPoly.geometry.explode(),
                                                           nodata=13,
                                                           crop=True
                                                           )
LC2018mask_meta = forest2018rast.meta.copy()
LC2018mask_meta.update({"driver": "GTiff",
           "height": LC2018maskArray.shape[1],
           "width": LC2018maskArray.shape[2],
           "transform": LC2018maskTransform,
           })


with rasterio.open("losCedrosForest2018.tif", "w", **LC2018mask_meta) as dest:
    dest.write(LC2018maskArray)

losCedrosForest2018 = rasterio.open("losCedrosForest2018.tif")

rasterio.plot.show(losCedrosForest2018)

## LC 1990
LC1990maskArray, LC1990maskTransform = rasterio.mask.mask(dataset=forest1990rast,
                                                           shapes=lcPoly.geometry.explode(),
                                                           nodata=13,
                                                           crop=True
                                                           )
LC1990mask_meta = forest1990rast.meta.copy()
LC1990mask_meta.update({"driver": "GTiff",
           "height": LC1990maskArray.shape[1],
           "width": LC1990maskArray.shape[2],
           "transform": LC1990maskTransform,
           })
with rasterio.open("losCedrosForest1990.tif", "w", **LC1990mask_meta) as dest:
    dest.write(LC1990maskArray)


losCedrosForest1990 = rasterio.open("losCedrosForest1990.tif")

rasterio.plot.show(losCedrosForest1990)

### Chontal BP

## Chontal 2018
BPC2018maskArray, BPC2018maskTransform = rasterio.mask.mask(dataset=forest2018rast,
                                                           shapes=chontalPoly.geometry,
                                                           nodata=13,
                                                           crop=True
                                                           )
BPC2018mask_meta = forest2018rast.meta.copy()
BPC2018mask_meta.update({"driver": "GTiff",
           "height": BPC2018maskArray.shape[1],
           "width": BPC2018maskArray.shape[2],
           "transform": BPC2018maskTransform,
           })

with rasterio.open("chontalForest2018.tif", "w", **BPC2018mask_meta) as dest:
    dest.write(BPC2018maskArray)

chontalForest2018 = rasterio.open("chontalForest2018.tif")
rasterio.plot.show(chontalForest2018)

## Chontal 1990

BPC1990maskArray, BPC1990maskTransform = rasterio.mask.mask(dataset=forest1990rast,
                                                           shapes=chontalPoly.geometry,
                                                           nodata=13,
                                                           crop=True
                                                           )

BPC1990mask_meta = forest1990rast.meta.copy()
BPC1990mask_meta.update({"driver": "GTiff",
           "height": BPC1990maskArray.shape[1],
           "width": BPC1990maskArray.shape[2],
           "transform": BPC1990maskTransform,
           })

with rasterio.open("chontalForest1990.tif", "w", **BPC1990mask_meta) as dest:
    dest.write(BPC1990maskArray)

chontalForest1990 = rasterio.open("chontalForest1990.tif")
rasterio.plot.show(chontalForest1990)


## looks good. make our 2018 color maps: 

def getColorMap(rast, yearCodes):
    colrDF = pd.read_pickle('colrDF.p')
    band = rast.read(1)
    uVals = np.unique(band).astype('int')
    colrs = colrDF.copy()
    if yearCodes == "codes2018": colrs.dropna(inplace=True)
    colrs.set_index(yearCodes, inplace=True)
    aa = [ colrs.loc[i].co for i in uVals ]
    listedCmap = mcolors.ListedColormap(aa) 
    return(listedCmap)

#def getColorMap(rast, yearCodes):
#    colrDF = pd.read_pickle('colrDF.p')
#    band = rast.read(1)
#    uVals = np.unique(band).astype('int')
#    print("uVals = ", uVals)
#    colrs = colrDF.copy()
#    if yearCodes == "codes2018": colrs.dropna(inplace=True)
#    colrs.set_index(yearCodes, inplace=True)
#    aa = [ colrs.loc[i].co if i in uVals else "#ff0000" for i in range(max(uVals) + 1)]
#    listedCmap = mcolors.ListedColormap(aa) 
#    return(listedCmap)


BPCcmap2018 = getColorMap(chontalForest2018, "codes2018")
LCcmap2018 = getColorMap(losCedrosForest2018, "codes2018")
CCcmap2018 = getColorMap(cotacachiForest2018, "codes2018")
CCcmap1990 = getColorMap(cotacachiForest1990, "codes1990")
CCcmap1990 = getColorMap(cotacachiForest1990, "codes1990")

LCcmap2018.colors
BPCcmap2018.colors
CCcmap2018.colors
CCcmap1990.colors


getLandVal(forest2018rast)

LCcmap2018.colors
BPCcmap2018.colors
## they are the same...is that right? 

CCcmap2018.colors

## a BPC/LC plotter:
def plotBPRast(rast, poly, yearCodes,  colormap, title=None, ax=None):
    if ax is None: fig, ax = plt.subplots(figsize=(20,10))
    yearColrsDF = (colrDF
                .dropna()
                .set_index(yearCodes)
            )
    #print(yearColrsDF)
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(yearColrsDF.co, yearColrsDF.landUse) ]
    ax.legend(handles=legend_patches,
                facecolor="white",
                edgecolor="white",
                loc="upper left",
                bbox_to_anchor=(1, 1))
    rasterio.plot.show(rast, ax=ax, cmap=colormap, )
    poly.boundary.plot(ax=ax, color='r')
    xmin, ymin, xmax, ymax = rast.bounds
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    if title is None: ax.set_title("")
    if title is not None: ax.set_title(title)


plotBPRast(chontalForest2018, chontalPoly, "codes2018", BPCcmap2018, title=None, ax=None)

plotBPRast(chontalForest2018, chontalPoly, "codes2018", CCcmap2018, title=None, ax=None)

fig, ax = plt.subplots()

BPCcmap2018.colors


rasterio.plot.show(chontalForest2018,  cmap=BPCcmap2018)

mcolors.ListedColormap

### try out boundaryNorm method


plt.close('all')

z = [1,4,5,1,4]
cmap = mcolors.ListedColormap(['r', 'g', 'b'])
boundary_norm = mcolors.BoundaryNorm([0,3,4.5,5], 3)
fig, ax = plt.subplots()
ax.scatter(x = range(1,6), y = [1]*5,
            s=100,
            c=z,
            cmap=cmap, 
            norm=boundary_norm,
            )

## great. How do we apply to our data?

## can we automate the boundary creation?

dd = getColorMap(cotacachiForest1990, 'codes1990')

aa = np.unique(cotacachiForest1990.read(1))

bb = aa - 0.5
cc = np.append(bb,aa.max()+0.5)
boundary_norm = mcolors.BoundaryNorm(cc, len(aa))

dd.colors

rasterio.plot.show(chontalForest2018,  cmap=BPCcmap2018, norm=boundary_norm)

rasterio.plot.show(cotacachiForest2018,  
                    cmap=dd, 
                    norm=boundary_norm,
                    )

## doesn't work, the agro land is blue...


rasterio.plot.show(chontalForest2018,  cmap=BPCcmap2018)

rasterio.plot.show(chontalForest2018,  cmap=CCcmap1990)
rasterio.plot.show(cotacachiForest2018,  cmap=CCcmap2018)
rasterio.plot.show(cotacachiForest1990)
rasterio.plot.show(cotacachiForest1990,  cmap=CCcmap1990)
CCcmap2018.colors

len(CCcmap2018.colors)
len(np.unique(cotacachiForest2018.read(1)))
max(np.unique(cotacachiForest2018.read(1)))

np.unique(chontalForest2018.read(1))

BPCcmap2018.colors

CCcmap2018.colors

## ugh, not sure. we might be able to come up with general 
## solutions...

## so just do individualized cmaps and plots for each?

## first, let's update the maps we have on the jupyter
## with the normboundary method:

## 1990 cotacachi

def getColorMap(rast, yearCodes):
    rastband = rast.read(1)
    uVals = np.unique(rastband).astype('int')
    colrs = (colrDF.set_index(yearCodes)
                 .loc[uVals]['co']
                 .to_list()
                 )
    return(mcolors.ListedColormap(colrs))

def getBoundaries(raster):
    aa = np.unique(raster.read(1))
    bb = aa - 0.5
    cc = np.append(bb,aa.max()+0.5)
    return(mcolors.BoundaryNorm(cc, len(aa)))

plt.close('all')
fig, axes = plt.subplots(2,1)
cc1990cmap = getColorMap(cotacachiForest1990, 'codes1990')
cc1990Bound = getBoundaries(cotacachiForest1990)
rasterio.plot.show(cotacachiForest1990, cmap=cc1990cmap, norm=cc1990Bound, ax = axes[0])
axes[0].set_title('1990')
cc2018cmap = getColorMap(cotacachiForest2018, 'codes2018')
cc2018Bound = getBoundaries(cotacachiForest2018)
rasterio.plot.show(cotacachiForest2018, cmap=cc2018cmap, norm=cc2018Bound, ax = axes[1])
axes[1].set_title('2018')



plt.close('all')

plotCotacachiRast(cotacachiForest1990, 'codes1990', title='1990', ax=None)
plotCotacachiRast(cotacachiForest2018, 'codes2018', title='2018', ax=None)

## can this function also work with our smaller rasters (LC and Chontal?)

## works

def plotRast(rast, yearCodes, title=None, ax=None, plotBPs=False):
    if ax is None: fig, ax = plt.subplots(figsize=(20,10))
    rastband = rast.read(1)
    uVals = np.unique(rastband).astype('int')
    colrDF_thisyear = colrDF.set_index(yearCodes)
    colrDF_thisyear = colrDF_thisyear[colrDF_thisyear.index.notnull()].loc[uVals]
    legend_patches = [ Patch(color=c, label=l) for c,l in zip(colrDF_thisyear.co, colrDF_thisyear.landUse) ]
    ax.legend(handles=legend_patches,
                facecolor="white",
                edgecolor="black",
                loc="upper left",
                bbox_to_anchor=(1, 1))
    colormap =  getColorMap(rast, yearCodes)
    colorBounds = getBoundaries(rast)
    rasterio.plot.show(rast, ax=ax, cmap=colormap, norm = colorBounds)
    if plotBPs == True:
            lcPoly.boundary.plot(ax=ax, color='r')
            chontalPoly.boundary.plot(ax=ax, color = 'b')
            cebuPoly.boundary.plot(ax=ax, color = 'b')
    xmin, ymin, xmax, ymax = rast.bounds
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    if title is None: ax.set_title("")
    if title is not None: ax.set_title(title)


plt.close('all')

plotRast(losCedrosForest2018, 'codes2018', title=None, ax=None, plotBPs=False)

plotRast(losCedrosForest1990, 'codes1990', title=None, ax=None, plotBPs=False)

plotRast(cotacachiForest1990, 'codes1990', title='1990', ax=None, plotBPs=True)
plotRast(cotacachiForest1990, 'codes1990', title='1990', ax=None, plotBPs=False)
plotRast(cotacachiForest2018, 'codes2018', title='2018', ax=None, plotBPs=True)

### check old growth estimate numbers ###

## while we're here, can we check Jose's old growth estimates?

## we have the satellite data showing native forest data available 
## while we're in this

## get masked los cedros 1990 raster:

losCedrosForest1990 = rasterio.open("losCedrosForest1990.tif")
rasterio.plot.show(losCedrosForest1990)

aa = makePixelTotals(losCedrosForest1990, 'codes1990').drop('SIN INFORMACION')
lcPercs = aa.apply(lambda x: x / aa.sum())
## or more simply
lcPercs = aa / aa.sum()

## landUse
## BOSQUE NATIVO          0.957980
## PASTIZAL               0.027991
## TIERRA AGROPECUARIA    0.014029

## so in 1990, the satellite data shows 95% forest cover
## where does Jose get his 80% old growth ?

## question sent. 

## just curious, can we plot our points over this raster image?

anaPt = gpd.read_file('GIS/ana30meterPlots.geojson').set_index('PsubP')

losCedrosForest1990 = rasterio.open("losCedrosForest1990.tif")
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')

aa = pd.concat([anaPt, envOnly], axis=1)


fig, ax = plt.subplots()
rasterio.plot.show(losCedrosForest1990, ax=ax)
aa.plot(column='habitat', categorical=True, legend=True, ax=ax)

anaPt.plot(column=

pd.concat(


#### weather data ####

## why do we say there is no seasonal variation in our 
## documents? 

## there is a Garcia Moreno weather station, I think, website here:
https://www.weather-atlas.com/en/ecuador/garcia-moreno-climate

## they get their data from here:
http://www.serviciometeorologico.gob.ec/

## this leads us to the map of stations at:
http://186.42.174.236/InamhiEmas/

## but this website is not very useful. 



########## old forest logistic regression #######

## can we predict the old forest type by the elevation?

## I think we are going to need to work in the bayes environment for this

## using the notebook import list for bayes:

import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd
import pymc3 as pm
import scipy.spatial as sp
import scipy.stats as stats
import arviz as az

import statsmodels.api as sm

import statsmodels.api as sm

from sklearn.linear_model import LinearRegression
from scipy import stats

## there is a good example to follow in the bayes book, ch4, p140. 

## we need to know which sites are type 3 or type 4 old forest, 
## and their elevation. 

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
cGroup = pd.read_csv('cGroup.csv', index_col="PsubP")
elevGr = pd.concat([envOnly, cGroup], axis=1)[['elevacion', 'gr']]

df = iris.query("species == ('setosa', 'versicolor')")

## come back to this later...

########## log linear model of landuse as predictor of cluster #########

## just realized that the landuse/natural cluster relationship 
## could be modeled as a contingency table and a log-linear model.

## to make the contingency table, we need the cluster and the landuse of
## each plot, I think we need a dataframe of plotname, habitat-type, and 
## cluster number:

help(pd.crosstab)

envOnly[['habitat']]

cGroup = pd.read_csv('cGroup.csv', index_col='PsubP')
landUseClustDF = pd.concat([envOnly[['habitat']], cGroup], axis=1)
landUseClustDF['gr'] = landUseClustDF['gr'].astype('category')
#landUseClustDF.reset_index(inplace=True)

help(pd.crosstab)  

pdtab = pd.crosstab(index=landUseClustDF['gr'], columns=landUseClustDF['habitat'], margins=True)

## or use the statsmodels package
## https://www.statsmodels.org/stable/contingency_tables.html

help(sm.stats.Table.from_data)

table = sm.stats.Table.from_data(landUseClustDF)

print(table.table_orig)

print(table.fittedvalues)

print(table.resid_pearson)

## goodness of fit, chi square:
print(table.test_nominal_association())

## yeah, really far away from independent, 
## but no surprise. How do we build the 
## linear model? 

## if there is no baked in function for doing
## this from a table object, it seems like 
## there must be a way to model this as 
## 16 points of count data, each with 
## a coeffecient from the two columns

## or maybe just the "predictor" column, 
## which is the "habitat" (landuse) variable.

## I don't think we want to include the 
## cluster group column influence, as this 
## is a "dependent" or exogenous variable,
## it represents the current ecological 
## state of the site, didn't exist at the
## time of the disturbance, shouldn't be 
## included in the model. For the same reason, there
## is no need for an interaction term between
## the two columns.

## but if we don't include the dependent 
## variable here (=cluster type), then we
## are simply modeling the abundance of the
## habitat types, aren't we? at least, in 
## model setup. It seems like you have 
## to put both column types in as predictors
## of abundance, then solve for the influence
## of the "independent" variable of interest,
## habitat/land-use. or maybe get an interaction 
## term?


## now my brain really hurts.

## so something like:

## Tij ~ Poisson(λij)  because count data
## log(λi) = μ + αi + βj + α*β + γij   

## make a model for λ:

## μ is the average rate of observations, sort of an intercept 

## αi is effect of habitat, β is the effect of cluster type,  γij is individual variance (sort of an error term)

## do we need to accout for zero inflation? Don't think so.
## zeros are totally explainable here, uncertainty is 
## zero for zeros. 

## so, how do we write this out? my brain still hurts.

## in this case, the exogenous (y, dep) variable are the count data for each cell, 

## For our data, the endogenous (x, indep) variable are the "habitat" type for each cell
## AND the final ecological state, the cluster type. 

## in this framework, we should have 20 data points (5 habitat types, 4 cluster types):

aa = table.table_orig.reset_index()
bb = pd.melt(aa, id_vars=['habitat'])

print(table.table_orig)

glm_Poisson = sm.GLM(exog, hab, family=sm.families.Poisson(link=sm.families.links.log()))


## ugh. Not working. Let's look at an example:

## https://www.statsmodels.org/stable/examples/notebooks/generated/discrete_choice_overview.html

## looks like it would be best to use patsy, = R-like formulas

## to get our design matrices using patsy:

cGroup = pd.read_csv('cGroup.csv', index_col='PsubP')
landUseClustDF = pd.concat([envOnly[['habitat']], cGroup], axis=1)
landUseClusttable = sm.stats.Table.from_data(landUseClustDF)
aa = landUseClusttable.table_orig
aa.reset_index(inplace=True)
landUseClustMelted = pd.melt(aa, id_vars=['habitat'])
landUseClustMelted['gr'] = landUseClustMelted['gr'].astype('category')
landUseClustMelted.columns = ['habitat','gr','count']

landUseClustMelted.head()

## so, what do we need to feed statsmodel?

Y, X = dmatrices('count ~ gr + habitat', data=landUseClustMelted, return_type='dataframe')

## can we do an interaction effect with dummy vars?
Y, X = dmatrices('count ~ gr + habitat + gr*habitat', data=landUseClustMelted, return_type='dataframe')
glm_Poisson = sm.GLM(Y, X, family=sm.families.Poisson(link=sm.families.links.log()))
res = glm_Poisson.fit()

## huh, perfect separation, says the model...makes sense...

## maybe this is why the default table fills in zeroes with 0.5 values...
## I thought that was weird and didn't use it. Let's try it:

cGroup = pd.read_csv('cGroup.csv', index_col='PsubP')
landUseClustDF = pd.concat([envOnly[['habitat']], cGroup], axis=1)
landUseClusttable = sm.stats.Table.from_data(landUseClustDF)

aa = pd.DataFrame(landUseClusttable.table, 
            columns=landUseClusttable.table_orig.columns,
            index=landUseClusttable.table_orig.index,
            )
aa.reset_index(inplace=True)
landUseClustMelted = pd.melt(aa, id_vars=['habitat'])
landUseClustMelted['gr'] = landUseClustMelted['gr'].astype('category')
landUseClustMelted.columns = ['habitat','gr','count']
Y, X = dmatrices('count ~ gr + habitat + gr*habitat', data=landUseClustMelted, return_type='dataframe')
glm_Poisson = sm.GLM(Y, X, family=sm.families.Poisson(link=sm.families.links.log()))
res = glm_Poisson.fit()

## nope, this doesn't work. The indirect approach is not working. 
## let's try multinomial logit. I think for this we take a more 
## direct approach, with our natural cluster as a y var, and 
## our land use as our x.

## if this works, we could extend this to other variables, the MEMs, env vars, etc.

landUseClustDF
Y, X = dmatrices('gr ~ habitat', data=landUseClustDF, return_type='dataframe')

mlogit_mod = sm.MNLogit(Y, X)

mlogit_res = mlogit_mod.fit()

print(mlogit_res.summary())

print(mlogit_res.params)

mlogit_mod.summary()

## all of my pvalues are 1.000. WTF?
## this is not working. 

table.table_orig

## should we jump over to Bayesian methods?

## ugh, that requires so much more thought....

## but I think there is a good example in the Martin book,
## ch4, "softmax regression", p 164

## gotta reload the bayes environment
conda activate bayesAna

import pymc3 as pm
import theano.tensor as tt
import pandas as pd
import seaborn as sns

## folowing martin's example
iris = sns.load_dataset('iris')
y_s = pd.Categorical(iris['species']).codes
x_n = iris.columns[:-1]
x_s = iris[x_n].values
x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)

x_s.shape

y_s.shape

with pm.Model() as model_s:
    α = pm.Normal('α', mu=0, sd=5, shape=3)
    β = pm.Normal('β', mu=0, sd=5, shape=(4,3))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_s)
    trace_s = pm.sample(2000)

## checking the model a bit:
az.plot_forest(trace_s, var_names=['α', 'β'])

data_pred = trace_s['μ'].mean(0)

y_pred = [np.exp(point)/np.sum(np.exp(point), axis=0)
          for point in data_pred]


corrs = trace_s['β'].mean(0)
corrLabs = corrs.round(2)

landUse = ['BS', 'CLB', 'RCA', 'RG']
forestType = [ f'ForestType{i}' for i in range(1,5) ]

fig, ax = plt.subplots()
im = ax.imshow(corrs)

## there is also an example at 
## https://discourse.pymc.io/t/multivariate-multinomial-logistic-regression/5242/2

from sklearn.preprocessing import LabelEncoder, Normalizer, StandardScaler
import numpy as np
import pymc3 as pm
import theano.tensor as tt
import arviz as az
import pandas as pd
import matplotlib.pyplot as plt
from patsy import dmatrices

data = pd.read_csv('/home/daniel/seaborn-data/iris.csv', header=0, names=[0, 1, 2, 3, 'TYPE'])
data['TYPE']= LabelEncoder().fit_transform(data['TYPE'])
y_obs = data['TYPE'].values
x_n = data.columns[:-1]
x = data[x_n].values
x = StandardScaler().fit_transform(x)
ndata = x.shape[0]
nparam = x.shape[1]
nclass = len(data['TYPE'].unique())
print( y_obs.shape, x.shape )

with pm.Model() as hazmat_model:
    X_data = pm.Data('X_data', x)
    y_obs_data = pm.Data('y_obs_data', y_obs)
    alfa = pm.Normal('alfa', mu=0, sd=1, shape=nclass)
    beta = pm.Normal('beta', mu=0, sd=1, shape=(nparam, nclass))
    mu = tt.dot(X_data, beta) + alfa
    p = tt.nnet.softmax(mu)
    yl = pm.Categorical('obs', p=p, observed=y_obs_data)
    trace = pm.sample()

#fig, ax = plt.subplots()
az.plot_trace(trace)

pm.traceplot(idata)
## okay that also works. Now what?

## let's go with Martin's example, see if we can modify for our data:

cGroup = pd.read_csv('cGroup.csv', index_col='PsubP')
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
landUseClustDF = pd.concat([envOnly[['habitat']], cGroup], axis=1)
landUseClustDF.reset_index(inplace=True)
landUseClustDF.columns  = ['site','landUse', 'forestType']
landUseClustDF.set_index('site', inplace=True)
landUseClustDF.to_csv('landUseForesttypeDF.csv')

landUseForestType = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
## maybe we stick with patsy? It was very convenient for the dummy vars
Y, X = patsy.dmatrices('forestType ~ landUse', data=landUseForestType, return_type='dataframe')
y_s = Y.astype('int')
x_s = X.iloc[:,1:]
x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)
x_s.columns = ['BS','CLB','RCA', 'RG']

with pm.Model() as model_s:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(4,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_s)
    trace_s = pm.sample(2000, init='adapt_diag')

## and that don't work, bad initial energy

## time to send out a question?
## some text formating for discourse site:
pdtab = pd.crosstab(index=landUseClustDF['gr'], columns=landUseClustDF['habitat'], margins=True)

## format for markdown:

| |BC  |BS  |CLB  |RCA  |RG  |All |
--- | --- | --- | --- | --- | --- | ---                               
| I  | 0 | 0 | 0 | 4 | 5  |  9 |
| II | 0 | 0 | 0 | 0 | 15 | 15 |
| III| 6 | 7 | 8 | 0 | 0  | 21 |
| IV | 6 | 5 | 4 | 0 | 1  | 16 |
| All| 12| 12| 12| 4 | 21 | 61 |

## side note get jose his polygon
bpPath=("/home/daniel/Documents/LosCed/Ecuador_GIS/"
        "BP_and_parks/mireya/car_bosques_protectores.shp")
lc = gpd.read_file(bpPath).query("nombre == 'LOS CEDROS'")
writePath=('/home/daniel/Desktop/losCedrosBP.shp')
lc.to_file(writePath, driver='ESRI Shapefile')


## and back to this... 
## we know we can drop a column anyway, why not specifically 
## drop the RCA column instead of the BC? Maybe this will help:
 
landUseForestType = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
## use 

luDummy = (pd.get_dummies(landUseForestType, columns=['landUse'])
                    .drop(columns=['forestType','landUse_RCA']))

y_s = landUseForestType['forestType'].astype('int')
x_s = luDummy

x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)

with pm.Model() as model_s:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(4,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_s)
    trace_s = pm.sample(2000, init='adapt_diag')

## and that don't work, bad initial energy. Jeezus I wish they 
## had better error reporting with pymc3. everything is bad initial
## energy. somehow appropriate. 

## can we try statsmodel again, with this approach of dropping the RCA column?

landUseForestType = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
Y = landUseForestType['forestType'] 
luDummy = (pd.get_dummies(landUseForestType, columns=['landUse'])
                    .drop(columns=['forestType','landUse_RCA']))

#Y, X = dmatrices('forestType ~ landUse', data=landUseForestType, return_type='dataframe')

mlogit_mod = sm.MNLogit(Y, luDummy)
mlogit_res = mlogit_mod.fit(maxiter=100) ## doesn't converge but gives nice output

mlogit_res = mlogit_mod.fit(maxiter=300) ## converges but goes to NaN on all results

mlogit_res = mlogit_mod.fit(maxiter=1000) ## converges but goes to NaN on all results

print(mlogit_res.summary())

print(mlogit_res.params)

## the maximum likelihood estimator is failing to converge unless I give it 
## a lot iterations to do so. When it does, all results are NaN. If I do not 
## allow it to converge, I get a standard, nice-looking report

## can I still use these results? 

## is this a problem of having empty (all-zero) rows?

landUseForestType

pdtab = pd.crosstab(index=landUseForestType['landUse'], 
                    columns=landUseForestType['forestType'], 
                    margins=False)

zfilt = luDummy.sum(axis=1) != 0

luDummyNoZeros = luDummy[zfilt]

yNoZeros = Y[zfilt]

(yNoZeros.index == luDummyNoZeros.index).all()

mlogit_mod_noZeros = sm.MNLogit(yNoZeros, luDummyNoZeros)

mlogit_noZ_res = mlogit_mod_noZeros.fit()

mlogit_noZ_res = mlogit_mod_noZeros.fit(maxiter=300) 

## nope, that's not the issue. 

## I guess the real perfect separation in this dataset 
## may not be RCA, but instead landuse category 2, which
## is entirely populated by RG sites. Try removing this
## category from our analysis?

landUseForestType = pd.read_csv('landUseForesttypeDF.csv', index_col='site')

type2filt = (landUseForestType['forestType'] != 2)
luFt_no2 = landUseForestType[type2filt]
Y = luFt_no2['forestType'] 

## keep RCA for the moment...
luDummy = (pd.get_dummies(landUseForestType, columns=['landUse'])
                    .drop(columns=['forestType']))
luDummy = luDummy[type2filt]
mlogit_mod = sm.MNLogit(Y, luDummy)

mlogit_res = mlogit_mod.fit(maxiter=100) ## nope, doesn't work. singular matrix?

## keep going, what if we remove RCA?

luDummy.drop(columns=['landUse_RCA'], inplace=True)
 
mlogit_mod = sm.MNLogit(Y, luDummy)

mlogit_res = mlogit_mod.fit(maxiter=100) ## nope

## ah fucking hell. why is this so hard?

## so again, the only model that seems to work is:
landUseForestType = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
Y = landUseForestType['forestType'] 
luDummy = (pd.get_dummies(landUseForestType, columns=['landUse'])
                    .drop(columns=['forestType','landUse_RCA']))
mlogit_mod = sm.MNLogit(Y, luDummy)
mlogit_res = mlogit_mod.fit(maxiter=100) ## doesn't converge but gives nice output
mlogit_res.summary()

## so we will try to remove cluster group 2, and then use patsy to create 
## the design matrix that should be most compatible with the statsmodels

## no type 2 forest:
luNo2 = (pd.read_csv('landUseForesttypeDF.csv', index_col='site')
            .query("forestType != 2"))
luNo2['forestType'] = luNo2['forestType'].astype('category')
## let patsy set it up for us
_, X = dmatrices('forestType ~ landUse', data=luNo2, return_type='dataframe')
X.drop(columns=['Intercept'], inplace = True)
Y = luNo2['forestType']
Y = Y[X.index]
mlogit_mod = sm.MNLogit(Y, X)
mlogit_res = mlogit_mod.fit(maxiter=100) ## meh, everything is NA

mlogit_res.summary()

## weird. No convergence, model is high significant (I think? LLR p-value),
## but all the individual coefficients are p=1.00? zscores are all right on zero

## try again, also remove the RCA plots?

luNo2 = (pd.read_csv('landUseForesttypeDF.csv', index_col='site')
            .query("forestType != 2")
            .query("landUse != 'RCA'"))
luNo2['forestType'] = luNo2['forestType'].astype('category')
## let patsy set it up for us
_, X = dmatrices('forestType ~ landUse', data=luNo2, return_type='dataframe')


mlogit_mod = sm.MNLogit(Y, X)
mlogit_res = mlogit_mod.fit(maxiter=100) ## doesn't converge but gives nice output
mlogit_res.summary()


## ugh. try once again with the bayesian approach:

iris = sns.load_dataset('iris')
y_s = pd.Categorical(iris['species']).codes
x_n = iris.columns[:-1]
x_s = iris[x_n].values
x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)

x_s.shape
y_s.shape


landUseForesttypeDF.head()
landUseForesttypeDF['forestType']

import matplotlib.pyplot as plt
import numpy as np
import pymc3 as pm
import theano.tensor as tt
import arviz as az
import pandas as pd
from patsy import dmatrices
plt.ion()

### works, but not necessary to drop a landuse with pymc3
# landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
# y_sMe = pd.Categorical(landUseForesttypeDF['forestType']).codes
# _, X = dmatrices('forestType ~ landUse', data=landUseForesttypeDF, return_type='dataframe')
# X.drop(columns=['Intercept'], inplace=True)
# x_sMe = X.values
# 
# with pm.Model() as model_sMe:
#     α = pm.Normal('α', mu=0, sd=5, shape=4)
#     β = pm.Normal('β', mu=0, sd=5, shape=(4,4))
#     μ = pm.Deterministic('μ', α + pm.math.dot(x_sMe, β))
#     θ = tt.nnet.softmax(μ)
#     yl = pm.Categorical('yl', p=θ, observed=y_sMe)
#     trace_sMe = pm.sample(2000, init='adapt_diag')
### 

## whoah, did that actually run without errors?

### do this instead, keep all the levels of land use, easier to interpret: ###
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
y_sMe = pd.Categorical(landUseForesttypeDF['forestType']).codes
luDummy = (pd.get_dummies(landUseForesttypeDF, columns=['landUse']))
luDummy.drop(columns=['forestType'], inplace=True)
x_sMe = luDummy.values

with pm.Model() as model_sMe:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(5,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_sMe, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_sMe)
    trace_sMe = pm.sample(2000, init='adapt_diag')


##########

data_predMe = trace_sMe['μ'].mean(0)
## this gets the posterior probabilities for each type of 
## of forest, so each PusbP site has an array with four 
## probabilities which should sum to one. Log-likelihoods
## so has to be exponentiated:
y_predMe = [np.exp(point)/np.sum(np.exp(point), axis=0)
          for point in data_predMe]

## then we take the forest type with the highest probability 
## for each, because these are the predictions for each site:
predictedForestType = pd.Series(np.argmax(y_predMe, axis=1), index=landUseForesttypeDF.index)
## reformat our observed into series:
observedForestType = pd.Series(y_sMe, index=landUseForesttypeDF.index)
## how often are we right?
(predictedForestType == observedForestType).sum()/len(predictedForestType)

## great. Also curious, since the model can't really tell the two 
## old types of forest apart, what if we lump these?

predictedForestType

np.unique(predictedForestType)

aa = predictedForestType.copy()
bb = observedForestType.copy()
aa[aa == 3] = 2
bb[bb == 3] = 2
(aa == bb).sum()/len(bb)
## 0.90

## wow. When the old forest types are taken together as a single
## type, land use from 1990 predicts ecological state 90% of the 
## time in our system.

## we might consider rerunning the model accordingly, but this 
## might also be a 

## check our r2:

## posterior predictive:
ppc = pm.sample_posterior_predictive(trace_sMe, samples=5000, model=model_sMe)
az.r2_score(y_sMe, ppc['yl'])
## pretty good, r2=0.642

## what else? how do we describe this model mathematically? 

## and how do we visualize the shifts in the priors?

az.plot_trace(trace_sMe, var_names='α')
aa = az.plot_trace(trace_sMe, var_names='β')

## can we plot these smoothed histograms without the trace history plots on the right column?

[ i.set_axis_off() for i in aa[:,1] ]

## nope, have to do this before, and az starts a new figure.

## should be simple enough to just plot a new histogram for each...

## if I remember this right, the intercepts tell us 
## where the model thinks a level/category in the  "belongs" 
## relative to the other categories. Then we can interpret 
## the coefficients as moving the needle towards one 
## response state or another. 

## So our forest types 1 and 2, the more disturbed forest 
## types, are here modeled with negative intercepts, 
## α =~  -1.65 for forest type 1
## α =~  -3.14 for forest type 2
trace_sMe['α'][:,0].mean() ## -1.6547276579370642
trace_sMe['α'][:,1].mean() ## -3.1444516386563435
## but they are not identical, so they have different intercepts.

## and our forest types 3,4 - the "old forest" types are 
## have nearly identical intercepts, around +2.5, because
## the model can't really tell the apart, it can just see 
## that they are different from the disturbed sites:

trace_sMe['α'][:,2].mean() ## 2.32
trace_sMe['α'][:,3].mean() ## 2.33

## but I need to do a little more reading, make sure 
## this is the correct way to do interpret these intercepts

## can we visualize this a little better? To do one smoothed 
## histogram:

## forest type 1:

plt.close('all')
fig, ax = plt.subplots()

X.columns

y_sMe

landUseForesttypeDF['forestType'].to_list()

## works
plt.close('all')
colrs = ['k','r','g','b']
fig, ax = plt.subplots()
for i in range(4):
    (pd.Series(trace_sMe['α'][:,i]
       .copy())
       .sort_values()
       .reset_index(drop=True)
       .plot.density(ax=ax, label=f'Forest Type {i+1}', color=colrs[i])
    )

ax.vlines(trace_sMe['α'].mean(0), ymin=0, ymax=0.15, colors=colrs, linestyles='dashed')
ax.set_xlim(-12,12)
ax.legend()

##

### viz the multinomial function?
observedForestType = pd.Series(y_sMe, index=landUseForesttypeDF.index)

## to start, can we get some sort of scatterplot
## of landuse and forest type:

## data is here:
observedForestType
predictedForestType
landUse = landUseForesttypeDF['landUse']
## turn landUse into codes
landUseCodes = pd.Series(pd.Categorical(landUse).codes, index=landUse.index)
## our codes are confusing...can we fix them? 
## is it as simple trading them out at this stage? 
## we want to switch "3" and "4" codes so that they
## graph pleasantly:
landUseGraphCodes = landUseCodes.copy()
landUseGraphCodes[landUseCodes == 3] = 4
landUseGraphCodes[landUseCodes == 4] = 3
## make a dataframe to not lose touch here:
landUseCodeDF = pd.concat([landUse, landUseCodes, landUseGraphCodes], 
    axis=1)
landUseCodeDF.columns = ['landUse', 'landUseCodes', 'landUseGraphCodes']
landUseCodeDF.drop_duplicates(inplace=True)
## I think both need some jitter for both, so we can observe quantity of sites 
landUseGraphCodesJit = landUseGraphCodes.apply(lambda x: x + np.random.normal(loc=0, scale=0.08))
observedForestTypeJit = observedForestType.apply(lambda x: x + np.random.normal(loc=0, scale=0.03))
predictedForestTypeJit = predictedForestType.apply(lambda x: x + np.random.normal(loc=-0.1, scale=0.03))
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(landUseGraphCodesJit, observedForestTypeJit, 
            alpha=0.5, c='blue', label='observed')
## reset our labels:
yForest = [ f'Forest type {i}' for i in range(1,5) ]
xLandUse = [ landUseCodeDF.set_index('landUseGraphCodes')
                .loc[i].loc['landUse'] for i in range(0,5) ]
ax.scatter(landUseGraphCodesJit, predictedForestTypeJit, 
            alpha=0.8, c='red', label='predicted')
ax.set_yticks([0,1,2,3])
ax.set_xticks([0,1,2,3,4])
ax.set_yticklabels(yForest)
ax.set_xticklabels(xLandUse)
plt.legend()
plt.tight_layout()


## great. Now what about a line from our model results?
## not sure. My brain hurts a bit on that. Something to think 
## about, maybe get some help

## almost out of time. Can we generate a heat map for the coefficients,
## to summarize effect of the various land uses on ecological states?

trace_sMe['β']

trace_sMe['β'].shape

## we want to average all the values for each landUse/clusterNumber
## combination...

trace_sMe['β'].mean(0)

fig, ax = plt.subplots()
plt.imshow(trace_sMe['β'].mean(0))

trace_sMe['β'].mean(0)
## okay, how do we know what's what here?

trace_sMe['α'].mean(0)

## the model was made with our experimental matrix:

luDummy.columns

help(luDummy.columns.str.replace)

luLabs = luDummy.columns.str.replace('landUse_','').to_list()
## so we can assume this is the order of the landuse coefficients,

trace_sMe['α'].mean(0)

trace_sMe['β'].mean(0)

## 

coeffDF = pd.DataFrame(trace_sMe['β'].mean(0), 
                columns = [ f'ForestType{i}' for i in range(1,5) ],
                index  = luLabs,
                )

## not sure...how do we confirm...plot_trace seems to know the 
## difference...

az.plot_trace(trace_sMe, var_names='β')
## actually, not really, doesn't label what is what...

## from the matplotlib docs:

plt.close('all')
corrs = trace_sMe['β'].mean(0)
corrLabs = corrs.round(2)
forestType = [ f'{i}' for i in range(1,5) ]
landUse = luDummy.columns.str.replace('landUse_','').to_list()
fig, ax = plt.subplots()
im = ax.imshow(corrs)
## fix ticks
ax.set_xticks(np.arange(len(forestType)))
ax.set_yticks(np.arange(len(landUse)))
ax.set_xticklabels(forestType)
ax.set_yticklabels(landUse)
# Loop over data dimensions and create text annotations.
for i in range(len(landUse)):
    for j in range(len(forestType)):
        text = ax.text(j, i, corrLabs[i, j],
                        ha="center", 
                        va="center", 
                        fontsize=20, 
                        color="k")


ax.set_xlabel('Forest Type')
fig.tight_layout()
plt.show()

pd.Categorical(landUseForesttypeDF['forestType'])


####### reviewing Ana field methods ########

## back up, see if we understand Ana's methods
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt; plt.ion()
import matplotlib.colors as mcolors
import geopandas as gpd
import rasterio
import rasterio.plot
import rasterio.mask
import rasterio.crs
import shapely
import os
import random
import matplotlib.patches as mpatches
from matplotlib.patches import Patch
from shapely.geometry import Point
plt.ion()

anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
anaPt.set_index('PsubP', inplace=True)
lcPoly = gpd.read_file('GIS/lcPoly.geojson')

specObs = pd.read_csv('specObs.csv')


def sdf(ser):
    aa = specObs[ser].unique()
    aa.sort()
    return(aa)

## there are four "sites", which are also called working blocks by ana:
sdf('site')

## these are then divided into "parcelas", which are independently numbered.
sdf('parcela')

## each of these are then divided into "subparcelas". We 
sdf('PsubP')

## so how were the "parcelas" or "sub-blocks" chosen?

## can we get a map of the "sites"?

## we need a geopandas with each of these categories, plus land use type:


aa = (specObs[ ['site','parcela','subparcela','PsubP','habitat'] ]
        .drop_duplicates()
        .set_index('PsubP')
    )
aa = aa.astype('category')
bb = pd.concat([aa,anaPt[['geometry']]], axis=1)
sampleDesign = gpd.GeoDataFrame(bb, geometry='geometry')

## join this with the geo info


plt.close('all')
fig, ax = plt.subplots()
lcPoly.boundary.plot(color='k', ax=ax)
sampleDesign.plot(column='site', ax=ax, legend=True)
ax.set_xlim(744910, 749066)
ax.set_ylim(10031233, 10035730)

## need an lc poly

## to me it looks like they realized they had 
## insufficient covereage and went back to add
## sample sites in areas of low coverage. 

## while we're here, can we find the site that 
## went from rg back to old forest? 

## this is site 10.1:


rg2oldPt = anaPt.loc[10.1]
markerSty = {'marker':'o',
             'markersize':20,
             'markerfacecolor':'none',
             'markeredgecolor':'purple',
            }
ax.plot(rg2oldPt['FinalRep_E'], rg2oldPt['FinalRep_N'], 
            **markerSty)

## 

############ clean up accumulation curves and permanent plot data #########3

## based on all that has happened, I think we need to clean up the 
## species counts, modeled species richness for the study at large, 

## what do we need for this?

## we need to revisit the code for the section on collectors curves in the 
## notes

## and bring in the work from Danilo's data.

## can we resurrect the Danilo analysis fairly quickly?:

trees = pd.read_csv('Trees2014.csv', index_col='Numero')

len(trees.NombreCientifico.unique())

## can we make a single column out of our genus and species columns?
## this would be cleaner than using the messy NombreCientifico column. 

aa = trees['Genus'] + "_" + trees['Species']

permPlotCCmat = pd.get_dummies(aa)

permPlotCCmat.to_csv('permPlotCCmat.csv', index_label=False, index=False)

## some of these have hanging white space...
aa.str.contains(" ").any()
aa[aa.str.contains(" ")]
## fix these in the original and rerun the above


## I think we actually need to be in R:
R

## okay, forget what specaccum needs...
data(BCI)
sp1 <- specaccum(BCI, method = "exact")
## looks like they need species as columns.
## so for us..back up, do a dummy mat in python and...

library("vegan")

trees = read.csv('Trees2014.csv')

permPlotCCmat = read.csv('permPlotCCmat.csv')

## now what? 
## to estimate the number of species in ~1/2 ha: 
specpool(permPlotCCmat)

## but we want a full hectare, we can't really 
## get there from here. 
## Either way, it looks like the assymptote approaches
## 60 species for this half-hectare. 

daniloSpecaccum <- specaccum(permPlotCCmat, method = "exact")
daniloSACmod <- fitspecaccum(daniloSpecaccum, "lomolino")
daniloPredYs <- predict(daniloSACmod, newdata=1:500)

## vgan has a builtin plotter for this. 
plot(daniloSpecaccum)
## But I think we should port this to matplotlib for better plotting 
## we want the actual data points, the predicted data points, 
## and the chao results

daniloSAC <- data.frame(daniloSpecaccum$richness, daniloSpecaccum$sd)
colnames(daniloSAC) <- c('richness', 'sd')
daniloSpeciesEstimators = specpool(permPlotCCmat) 

write.csv(daniloSAC, file='daniloSAC.csv')
write.csv(daniloPredYs, file='daniloPredYs.csv')
write.csv(daniloSpeciesEstimators, file='daniloSpeciesEstimators.csv')

## look at these in pandas/matplotlib

## back in python
daniloSAC = pd.read_csv('daniloSAC.csv', index_col=0)
daniloPredYs = pd.read_csv('daniloPredYs.csv', index_col=0).x
daniloSpeciesEstimators = pd.read_csv('daniloSpeciesEstimators.csv', index_col=0)
daniloSpeciesEstimators = pd.read_csv('daniloSpeciesEstimators.csv', index_col=0).loc['All']

daniloSAC

daniloPredYs

daniloSpeciesEstimators

## can we make a nice plot with this?

plt.close('all')
colr='g'
fig, ax = plt.subplots()
ax.plot(daniloSAC['richness'],
            label='observed accumulation',
            color=colr)
ax.set_ylabel('número de especies de arboles')
ax.fill_between(x=daniloSAC.index.to_numpy(),
                 y1=daniloSAC.richness - daniloSAC.sd,
                 y2=daniloSAC.richness + daniloSAC.sd,
                alpha=0.4,
                color=colr,
                )
ax.plot(daniloPredYs.index.to_numpy(), daniloPredYs, 
            color=colr, 
            linestyle=':',
            label='SAC predicted values',
            )
## put the chao estimator on there:
ax.hlines(daniloSpeciesEstimators.chao,
    xmin = 0,
    xmax = 500,
    linestyle = "--",
    color = "r",
    label='Chao estimate',
)
ax.fill_between(x=daniloPredYs.index,
                 y1=daniloSpeciesEstimators.chao - daniloSpeciesEstimators["chao.se"],
                 y2=daniloSpeciesEstimators.chao + daniloSpeciesEstimators["chao.se"],
                alpha=0.2,
                color='r',
                )
ax.legend(loc='lower right')
ax.set_ylabel('species')
ax.set_xlabel('trees examined')
ax.set_title('Permanent plot, 0.5 ha resample')




## okay, good, that's Danilo's stuff. Can we clean up Ana's data for the 
## notebook? what do we want to show? The above code, repeated for 
## Ana's data. This is already, done above, at "### get nice species accumulation curves"

## add both to notebook
 
### clean up the by-landuse SAC and estimators:

aa = pd.read_csv('envOnly.csv', index_col='PsubP')[['habitat']]
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
bb = pd.concat([aa, subParcelComm], axis=1)
bbGrouped = bb.groupby('habitat')
habs = list(bbGrouped.groups.keys())

for i in habs:
    cc = bbGrouped.get_group(i)
    observed = (cc != 0).any(axis=0).values
    dd = cc.iloc[:,observed].copy()
    dd.drop('habitat', axis=1, inplace=True)
    dd.to_csv(f'{i}_comm.csv')

## move over to R to use vegan...

library(vegan)
library(repr)


files <- list.files()
comm <- grep('_comm.csv', list.files())
comms <- files[comm]
comtitles <- sub("_comm.csv", "", comms)


for (i in comms){
    j <- j + 1
    comm.i <- read.csv(i, header=TRUE, row.names=1)
    print(i)
    SAC <- specaccum(comm.i, method = "exact")
    plot(SAC, main=comtitles[j], ylab='No. of Spp')
    #capture.output(print(paste('Species estimates for', comtitles[j], sep=" ")),
    #                file="habSRestimates.txt", append = TRUE)
    #capture.output(specpool(comm.i),
    #                file="habSRestimates.txt",
    #                append = TRUE)
    sacDF <- data.frame(cbind(SAC$richness, SAC$sd), row.names=SAC$sites)
    colnames(sacDF) <- c('richness', 'sd')
    #write.csv(sacDF, file=paste(comtitles[j], "SAC.csv", sep="_"))
}

## each habitat types needs two three things - observed data, predicted data, and species estimates.
## get these out of R

## for one
files <- list.files()
comm <- grep('_comm.csv', list.files())
comms <- files[comm]
comtitles <- sub("_comm.csv", "", comms)
for (i in comms){
    comm.i <- read.csv(i, header=TRUE, row.names=1)
    specAccum.i <- specaccum(comm.i, method = "exact")
    #SACmodelfit.i <- fitspecaccum(specAccum.i, "lomolino")
    #predYs.i <- predict(SACmodelfit.i, newdata=1:500)
    SACdf.i <- data.frame(specAccum.i$richness, specAccum.i$sd)
    colnames(SACdf.i) <- c('richness', 'sd')
    speciesEstimators.i = specpool(comm.i)
    habtype=sub("_comm.csv", "", i)
    print(habtype)
    write.csv(SACdf.i, file=paste(habtype, "SAC.csv", sep="_"))
    #write.csv(predYs.i, file=paste(habtype, "predYs.csv", sep="_"))
    write.csv(speciesEstimators.i, file=paste(habtype, "specEst.csv", sep="_"))
}

i="RCA_comm.csv"


comm.i <- read.csv(i, header=TRUE, row.names=1)

specAccum.i <- specaccum(comm.i, method = "exact")
speciesEstimators.i = specpool(comm.i)

predYs.i <- predict(SACmodelfit.i, newdata=1:500)

## meh, leave out the predictions, they seem to be failing on 
## on some of these and I have no faith in the low-sample
## numbers anyway.

## did that work? go over to python and try plotting them.

## let's see if we can make a function out of our plots for danilo/all-hab plots:


## to plot one, with chao results

## python:


sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
specEsts = [ i.replace("_SAC.csv", "_specEst.csv") for i in sacs ]

## brittle, but works for the moment

colr='g'
i = 0

## works
name_i = sacs[i].replace('_SAC.csv','') 
sac_i = pd.read_csv(sacs[i], index_col=0)
specEst_i = pd.read_csv(specEsts[i], index_col=0).loc['All']
specEst_i.index = specEst_i.index.str.replace(".","_")
plt.close('all')
fig, ax = plt.subplots()
ax.plot(sac_i['richness'],
            color=colr)
ax.set_ylabel('número de especies de arboles')
ax.fill_between(x=sac_i.index.to_numpy(),
                 y1=sac_i.richness - sac_i.sd,
                 y2=sac_i.richness + sac_i.sd,
                alpha=0.4,
                color=colr,
                )
ax.set_title(f'{name_i}')
ax.text(1, (sac_i['richness'].max()-3),
            f'Expected species richness = {specEst_i.chao.round()} +/- {specEst_i.chao_se.round()}', 
            #fontsize=15,
        )

sacs = [ i for i in os.listdir() if "_SAC.csv" in i  ]
specEsts = [ i.replace("_SAC.csv", "_specEst.csv") for i in sacs ]

## make a function

def plotSACs(habtype, color='black', 
                xUnits='ha', printChao=False, 
                printTitle=False, ax=None):
    if ax is None: fig, ax = plt.subplots()
    sacName = (habtype +'_SAC.csv')
    specEstName = (habtype + "_specEst.csv")
    assert( (habtype +'_SAC.csv') in sacs)
    assert( xUnits in ['ha','plots'])
    sac_i = pd.read_csv(sacName, index_col=0)
    sac_i['areaSampled'] = (sac_i.index * 0.09).to_list() 
    if xUnits == 'ha': X = sac_i['areaSampled'] 
    elif xUnits == 'plots': X = sac_i.index.to_list() 
    specEst_i = pd.read_csv(specEstName, index_col=0).loc['All']
    specEst_i.index = specEst_i.index.str.replace(".","_")
    ax.plot(X, sac_i['richness'], color=color)
    ax.set_ylabel('number of tree species')
    ax.fill_between(x=X,
                     y1=sac_i.richness - sac_i.sd,
                     y2=sac_i.richness + sac_i.sd,
                    alpha=0.4,
                    color=color,
                    )
    if printTitle: ax.set_title(f'{habtype}')
    if printChao:
        ax.text(1, (sac_i['richness'].max()-3),
                    f'Expected species richness = {specEst_i.chao.round()} +/- {specEst_i.chao_se.round()}', 
            )


sac_i = plotSACs('RG', xUnits='ha')

sac_i = plotSACs('RG', xUnits='plots')

sac_i

fig, ax = plt.subplots()
ax.plot(sac_i['areaSampled'], sac_i['richness'],
            color='g')

sac_i['richness']

sac_i['areaSampled']



plt.close('all')
fig, ax = plt.subplots()
plotSACs('BC', ax=ax, xUnits='ha', color='#E41A1C')
plotSACs('BS', ax=ax, xUnits='ha', color='#377EB8')
plotSACs('CLB', ax=ax, xUnits='ha', color='#4DAF4A')
plotSACs('RCA', ax=ax, xUnits='ha', color='#984EA3')
plotSACs('RG', ax=ax, xUnits='ha', color='#FF7F00')

BC_patch = mpatches.Patch(color='#E41A1C', label='BC', alpha=0.4)
BS_patch = mpatches.Patch(color='#377EB8', label='BS', alpha=0.4)
CLB_patch = mpatches.Patch(color='#4DAF4A', label='CLB', alpha=0.4)
RCA_patch = mpatches.Patch(color='#984EA3', label='RCA', alpha=0.4)
RG_patch = mpatches.Patch(color='#FF7F00', label='RG', alpha=0.4)
ax.legend(handles=[BC_patch, BS_patch, CLB_patch, RCA_patch, RG_patch])




sac_i.index * 0.09

plotSACs('RCA', ax=plt.gca(), color='r')

## great but we want to change the ticks to subplots:
## we want to show a tick for every hectare

## each one of our ticks if currently a plot
##  (0.09 ha / 1 subplot)
## we have 

ax = plt.gca()

tk = np.arange(0,6)*55.5
ot.se       6.934504
boot.se       6.934504


ax.set_xticks(tk)
## the ha values of this would be:
tklabs = np.arange(0,6)*5
tklabs[0] = 1
ax.set_xticklabels(tklabs)
ax.set_xlabel('Parceles medidas en hectareas (ha)')
ax.set_ylabel('número de especies de arboles')
ax.set_xlim(0,300)
ax.legend(loc='lower right')


## looks good


### make gis for permanent plot ###

## need to make a map of the permanent plot, 
## maybe colored by the tree genus or family
## can we make a georeffed polygon for the permanent plot?
## and data for the trees that Danilo worked?

## don't really have the coordinates for the permanent plot,
## so have to see if Ana has this, if not we make 
## our best guess. 

## so let's start with a map of danilo's results

## according to Roo, the numbering of our plots starts 
## in the NE corner, and moves directly south, 11 plots,
## with plots being 5 m apart in the N/S direction. 
## After 11 plots, the numbering starts over again in 
## the column to west, again in the north. 
## the columns are 10m apart in the east/west direction.
## the final, most southern plot in the western-most
## column was not sampled, so a corner is missing. 
## Can we plot this?

## an array would look something like this:
aa = np.arange(1,122).reshape(11,11).T
## if we want to look at it as we have in the past,  with n as down:
bb = np.flip(aa, axis=0)

## but if we want to plot, 

## we want an array of coordinate tuples, I think:



for i in range(0,110,10): print(i)

for i in range(0,55,5): print(i)

len(range(0,110,10))

len(range(0,55,5))


aa = []
for i in range(0,110,10):
    aa += [ Point(i,j) for j in range(0,55,5) ]

bb = gpd.GeoSeries(aa)
## can we remove our last one?
bb.drop(120, axis=0, inplace=True)
bb.index = range(1,121)

bb.plot()

## we need to attach data:

trees = pd.read_csv('Trees2014.csv', index_col='Numero')


## bind the two 

permGPD = gpd.GeoDataFrame(data=trees, geometry=bb)
permGPD.Genus.unique()
permGPD['genspp'] = permGPD.Genus + "_" + permGPD.Species

len(trees.Genus.unique()) ## 36 genera
len(trees.Familia.unique()) ## 25 families
len(genspp.unique()) ## 43 species

## generate random colors

def genXKCDColors(nu):
    Xcolors = random.choices(list(mcolors.XKCD_COLORS.values()), k=nu)
    cmap = mcolors.ListedColormap(Xcolors)
    return(cmap)
 

cmap=genXKCDColors(len(trees.Genus.unique()))
fig, ax = plt.subplots()
permGPD.plot(ax=ax, column='Genus', cmap=cmap)


## great. And for the Jena grant app, I'm going to also need a loscedros polygon 
## plus some landmarks to make sure we're not completely off, trails and rivers. 
## make a map for qgis 

##### check elevations of different forest types ####

## does it seem true that either old forest type can be shifted into 
## type I or II (disturbed forest type?)

## we need a dataframe with x-axis = type, y = elevation, of each PsubP site

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
elevLUDF = pd.concat([landUseForesttypeDF, envOnly], axis=1)[['landUse','forestType','elevacion']]


fig, ax = plt.subplots()
ax.scatter(elevLUDF.forestType, elevLUDF.elevacion)
## looks fine, but need to work it a little...

## set up colors for plotting, by historical land use
landuseColDict = {
          'BC':'#E41A1C',
          'BS':'#377EB8',
          'CLB':'#4DAF4A',
          'RCA':'#984EA3',
          'RG':'#FF7F00',
           }
landuseColrs = elevLUDF.landUse.apply(lambda x: landuseColDict[x])

plt.close('all')
fig, ax = plt.subplots()
Xraw, Yraw = elevLUDF.forestType, elevLUDF.elevacion
Xjitted = (Xraw
            .apply(np.random.normal, args=(0.08,))
            )
breakUpData = [ Yraw[Xraw == i].to_list() for i in Xraw.unique() ]
vio = ax.violinplot(breakUpData, showmeans=True)
ax.scatter(Xjitted, Yraw, color=landuseColrs, s=100)
ax.set_xticks([1,2,3,4])
ax.set_xticklabels(
        [ f'Type {i}' for i in range(1,5) ]
        )
ax.set_title('Forest type, history, and elevation')
ax.set_ylabel('Elevation')
ax.set_xlabel('Current forest type')
## color dictionary for forestType:
forestTypeColDict = {
                      1:'k',
                      2:'r',
                      3:'g',
                      4:'b',
                    }
## change violin patch color:
for i,j in enumerate(vio['bodies']):
    j.set_facecolors(forestTypeColDict[i+1])

## legend for land use history:
legend_patchesLU = [ Patch(color=j, label=i) for i,j in landuseColDict.items() ]
ax.legend(handles=legend_patchesLU,
            facecolor="white",
            edgecolor="black",
            loc="upper left",
            bbox_to_anchor=(1, 1))

plt.tight_layout()


## update notes with this. 
## seems like we still need to build 
## a logit model that predicts forest
## type by elevation

#### logistic regression between forest types

## should be pretty simple to make a logistic model
## of forest type as a function of elevation:

## in bayesAna environment again.

import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd
import pymc3 as pm
import arviz as az

import pymc3 as pm
import numpy as np
import pandas as pd
import theano.tensor as tt
import seaborn as sns
import scipy.stats as stats
from scipy.special import expit as logistic
import matplotlib.pyplot as plt
import arviz as az
plt.ion()

## there is an example in chapter4:

####### iris logistic example ########
dpath="/home/daniel/Documents/learn/Bayesian/martinBayesianAnalysis/Bayesian-Analysis-with-Python-Second-Edition/data"
iris = pd.read_csv(dpath+"/iris.csv")
df = iris.query("species == ('setosa', 'versicolor')")
y_0 = pd.Categorical(df['species']).codes
x_n = 'sepal_length' 
x_0 = df[x_n].values
x_c = x_0 - x_0.mean()

with pm.Model() as model_0:
    α = pm.Normal('α', mu=0, sd=10)
    β = pm.Normal('β', mu=0, sd=10)
    μ = α + pm.math.dot(x_c, β)    
    θ = pm.Deterministic('θ', pm.math.sigmoid(μ))
    bd = pm.Deterministic('bd', -α/β)
    yl = pm.Bernoulli('yl', p=θ, observed=y_0)
    trace_0 = pm.sample(1000)

theta = trace_0['θ'].mean(axis=0)
idx = np.argsort(x_c)
plt.plot(x_c[idx], theta[idx], color='C2', lw=3)
plt.vlines(trace_0['bd'].mean(), 0, 1, color='k')
bd_hpd = az.hpd(trace_0['bd'])
plt.fill_betweenx([0, 1], bd_hpd[0], bd_hpd[1], color='k', alpha=0.5)

plt.scatter(x_c, np.random.normal(y_0, 0.02),
            marker='.', color=[f'C{x}' for x in y_0])
az.plot_hpd(x_c, trace_0['θ'], color='C2')
plt.xlabel(x_n)
plt.ylabel('θ', rotation=0)
# use original scale for xticks
locs, _ = plt.xticks()
plt.xticks(locs, np.round(locs + x_0.mean(), 1))
plt.savefig('B11197_04_04.png', dpi=300)

#########

## works. can we do this with our data?

## the x axis will be elevation of site
## the y will be the probability of either 
## type III or type IV data

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
elevLUDF = pd.concat([landUseForesttypeDF, envOnly], axis=1)[['landUse','forestType','elevacion']]
df = elevLUDF.query("forestType > 2")



## switch the codes here to make a high θ mean
## a high probability of type 3 forest. 
## I think this is more intuitive, because this 
## is the forest associated higher elevations

## for colors, reuse the theme from elsewhere:

forestTypeColDict = {
                      1:'k',
                      2:'r',
                      3:'g',
                      4:'b',
                    }
colrs = df['forestType'].apply(lambda x: forestTypeColDict[x])
y_0 = pd.Series(index=df.index, dtype=int)
y_0[df['forestType'] == 3] = 1
x_n = 'elevacion' 
x_0 = df[x_n].values
x_c = x_0 - x_0.mean()

with pm.Model() as model_0:
    α = pm.Normal('α', mu=0, sd=10)
    β = pm.Normal('β', mu=0, sd=10)
    μ = α + pm.math.dot(x_c, β)    
    θ = pm.Deterministic('θ', pm.math.sigmoid(μ))
    bd = pm.Deterministic('bd', -α/β)
    yl = pm.Bernoulli('yl', p=θ, observed=y_0)
    trace_0 = pm.sample(2000, tune=1000)

plt.close('all')
theta = trace_0['θ'].mean(axis=0)
idx = np.argsort(x_c)
plt.plot(x_c[idx], theta[idx], color='k', lw=3, ls='--')
plt.vlines(trace_0['bd'].mean(), 0, 1, color='k')
bd_hpd = az.hpd(trace_0['bd'])
plt.fill_betweenx([0, 1], bd_hpd[0], bd_hpd[1], color='k', alpha=0.5)
plt.scatter(x_c, np.random.normal(y_0, 0.02),
            marker='.', s= 200, color=colrs)
az.plot_hpd(x_c, trace_0['θ'], color='y')
plt.xlabel(x_n)
plt.ylabel('θ', rotation=0)
# use original scale for xticks
locs, _ = plt.xticks()
plt.xticks(locs, np.round(locs + x_0.mean(), 1))

## great, use this
## examine the model a bit

ppc = pm.sample_posterior_predictive(trace_0, model=model_0)

help(pm.sample_posterior_predictive)

az.r2_score(y_0, ppc['yl']) ## wow, r2 = 0.93, +/- 0.13

az.r2_score(y_0, ppc['yl'].T) ## wow, r2 = 0.93, +/- 0.13

az.r2_score(y_0, ppc['yl'].mean(axis=0)) ## wow, r2 = 0.93, +/- 0.13

ppc['yl'].mean(axis=0).shape

## how do we check the predictions?

dir(ppc['yl'])

## our θ is the probability that a site is type 3
## how often are we right/wrong?

trace_0

len(trace_0)

trace_0.varnames

trace_0['α'].mean()

trace_0['β'].mean()

az.plot_trace(trace_0, var_names=['α', 'β'])

## this is what we need:
bb = pd.Series(dtype=int, index=df.index)

aa = trace_0['θ'].mean(axis=0)
bb[aa > 0.5] = 1
percentRight = (bb == y_0).sum()/len(y_0)
print(f'Out of {len(y_0)} we got {(bb == y_0).sum()} right') 
print(f'The model predicts the data {round(percentRight,3)*100}% of the time correctly')

np.argsort(x_c)

## update notebook:

##### surface for alternate stable states #######

## how to do a 3d surface plot with matplotlib?
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

## works
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
# Make data.
X = np.arange(-5, 5, 0.25)
Y = np.arange(-5, 5, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)
# Plot the surface.
surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)
ax.set_zlim(-1.01, 1.01)
fig.colorbar(surf, shrink=0.5, aspect=5)

## ok, tinker a bit:

plt.close('all')
fig, ax = plt.subplots(subplot_kw={"projection": "3d"})
# Make data.
X = np.arange(0.1, 8, 0.25)
Y = np.arange(0.1, 8, 0.25)
X, Y = np.meshgrid(X, Y)
Z = -1/(X-3 + Y-3)**2
#ax.elev=50
#ax.azim=230
##surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
##                       linewidth=0, antialiased=False)
surf = ax.plot_surface(X, Y, Z, linewidth=0, antialiased=False)

## don't think this will work. Too complicated. 
## I think a nice schematic with inkscape is probably the 
## way to go here.

##### model of clusters as function of habitat and elevation  #######

## so we have a strong predictor for predicting differences
## between the old forest types III and IV (elevation), and among the 
## anthropogenically disturbed site types I and II and old forest 
## types (land use history/habitat). Let's put them together in 
## one big model. 

## using the bayes setup (in bayesAna environment):

import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd
import pymc3 as pm
import scipy.spatial as sp
import geopandas as gpd
import theano.tensor as tt
#from theano import shared
import rasterio
import scipy.stats as stats
import arviz as az
from sklearn.linear_model import LinearRegression
from scipy import stats
plt.ion()

## we want our 
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
y_s = landUseForesttypeDF['forestType']
luDummy = (pd.get_dummies(landUseForesttypeDF, columns=['landUse']))
luDummy.drop(columns=['forestType'], inplace=True)
## elevation data
#aa = pd.concat([luDummy, envOnly.elevacion], axis=1)
#x_s = aa.values
#x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)
x_s = luDummy.values
x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)

with pm.Model() as model_s:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(5,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_s)
    trace_s = pm.sample(2000, init='adapt_diag')


## this works:
y_sMe = pd.Categorical(landUseForesttypeDF['forestType']).codes
luDummy = (pd.get_dummies(landUseForesttypeDF, columns=['landUse']))
luDummy.drop(columns=['forestType'], inplace=True)
x_sMe = luDummy.values
with pm.Model() as model_sMe:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(5,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_sMe, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_sMe)
    trace_sMe = pm.sample(2000, init='adapt_diag')
## this works ^ ##

## this seems to work, with elevation
y_sMe = pd.Categorical(landUseForesttypeDF['forestType']).codes
luDummy = (pd.get_dummies(landUseForesttypeDF, columns=['landUse']))
luDummy.drop(columns=['forestType'], inplace=True)
luElev = pd.concat([luDummy, envOnly.elevacion], axis=1)
x_s = luElev.values
x_s = (x_s - x_s.mean(axis=0)) / x_s.std(axis=0)

with pm.Model() as model_sMe:
    α = pm.Normal('α', mu=0, sd=5, shape=4)
    β = pm.Normal('β', mu=0, sd=5, shape=(6,4))
    μ = pm.Deterministic('μ', α + pm.math.dot(x_s, β))
    θ = tt.nnet.softmax(μ)
    yl = pm.Categorical('yl', p=θ, observed=y_sMe)
    trace_sMe = pm.sample(2000, init='adapt_diag')

## how do the intercepts look now?
colrs = ['k','r','g','b']
fig, ax = plt.subplots(figsize=(10,6))
for i in range(4):
    (pd.Series(trace_sMe['α'][:,i]
       .copy())
       .sort_values()
       .reset_index(drop=True)
       .plot.density(ax=ax, label=f'Forest Type {i+1}', color=colrs[i])
    )

ax.vlines(trace_sMe['α'].mean(0), ymin=0, ymax=0.15, colors=colrs, linestyles='dashed')
ax.set_xlim(-12,12)
ax.legend()
ax.set_title('y-intercepts of forest type')
## this moved the intercepts apart more, that's encouraging.

## visualize the coefficients
labs = luElev.columns.str.replace('landUse_','').to_list()
coeffDF = pd.DataFrame(trace_sMe['β'].mean(0),
                columns = [ f'ForestType{i}' for i in range(1,5) ],
                index  = labs,
                )

print(coeffDF)

plt.close('all')
## friendly plot of correlations:
coeffs = trace_sMe['β'].mean(0)
coeffLabs = coeffs.round(2)
forestType = [ f'{i}' for i in range(1,5) ]
predictors = luElev.columns.str.replace('landUse_','').to_list()
fig, ax = plt.subplots(figsize=(8,8))
im = ax.imshow(coeffs)
## fix ticks
ax.set_xticks(np.arange(len(forestType)))
ax.set_yticks(np.arange(len(predictors)))
ax.set_xticklabels(forestType)
ax.set_yticklabels(predictors)
# Loop over data dimensions and create text annotations.
for i in range(len(predictors)):
    for j in range(len(forestType)):
        text = ax.text(j, i, coeffLabs[i, j],
                        ha="center",
                        va="center",
                        fontsize=20, 
                        color="k")


ax.set_xlabel('Forest Type')

fig.tight_layout()


## check the predictions

data_predMe = trace_sMe['μ'].mean(0)
## this gets the posterior probabilities for each type of 
## of forest, so ebach PusbP site has an array with four 
## probabilities which should sum to one. Log-likelihoods
## so has to be exponentiated:
y_predMe = [np.exp(point)/np.sum(np.exp(point), axis=0)
          for point in data_predMe]
## then we take the forest type with the highest probability 
## for each, because these are the predictions for each site:
predictedForestType = pd.Series(np.argmax(y_predMe, axis=1), index=landUseForesttypeDF.index)
## reformat our observed into series:
observedForestType = pd.Series(y_sMe, index=landUseForesttypeDF.index)
## how often are we right?
(predictedForestType == observedForestType).sum()/len(predictedForestType)
## that bumps our predictive value up to 87% correct predictions


## R2?
ppc = pm.sample_posterior_predictive(trace_sMe, samples=5000, model=model_sMe)
az.r2_score(y_sMe, ppc['yl'])
## r2 - 0.88


####### update environmental permanovas ##########

## let's update the general permanovas to include 
## all the extra data 

## in R:

library(vegan)

envOnly <- read.csv('envOnly.csv', row.names='PsubP')
subParcelComm <- read.csv('subParcelComm.csv', row.names='PsubP')

all(row.names(envOnly) == row.names(subParcelComm))

adonis(subParcelComm ~ envOnly$elevacion*envOnly$habitat)

adonis(subParcelComm ~ envOnly$elevacion*envOnly$habitat)


for (i in 1:ncol(envOnly)){
    print(colnames(envOnly)[i])
    print(adonis(subParcelComm ~ envOnly[,i]))
}

###### model for watershed turnover #####

## we never built a bayesian model specifically for 
## turnover. How hard would this be?

import numpy as np 
import matplotlib.pyplot as plt; plt.ion()
import pandas as pd
import geopandas as gpd
import pymc3 as pm
import scipy.spatial as sp
import scipy.stats as stats
import theano.tensor as tt
#from theano import shared
import scipy.stats as stats
import arviz as az
from sklearn.linear_model import LinearRegression
from scipy import stats
from scipy.stats import norm
from statsmodels.stats.multicomp import pairwise_tukeyhsd

## first question to answer would be, can we recreate the diagram, no problems?:

psubpHopsDF= pd.read_csv('psubpHopsDF.csv', index_col='PsubP')
subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
psubpHopsNP = np.array(psubpHopsDF)
psubpHopsTriU = psubpHopsNP[np.triu_indices(psubpHopsNP.shape[0], k=1)]
## make the model from these two dist matrices
## I guess we can just make a model from these:
X, Y = psubpHopsTriU.reshape(-1,1), bcDist.reshape(-1,1)

fig, ax = plt.subplots()
ax.scatter(psubpHopsTriU, bcDist)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k')
ax.set_title('Overall Microcuenca Turnover')

## right, that's the shitty plot. The better one:

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)
aa = [ YS[XS == i].to_list() for i in np.unique(psubpHopsTriU) ]
fig, ax = plt.subplots(figsize=(10,10))
## that works...add points?
XSjitted = (XS
            .apply(np.random.normal, args=(0.03,))
            .apply( lambda x: x+1)
            )
ax.scatter(XSjitted, YS, alpha=0.15, color='red')
ax.violinplot(aa, showmeans=True)
_=ax.set_xticklabels(['',0,1,2,3,4])
ax.set_title('Overall Microcuenca Turnover')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_xlabel('distance in watershed hops')
[ np.mean(i) for i in aa ]

## works. Can we modify our other model to use watershed hops
## instead of absolute distance between comparisons?

## original ##
xx = physDist.copy()
with pm.Model() as model_gamma:
    γ = pm.Normal('γ', mu=1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, -γ + δ*xx))
    y_pred = pm.Gamma('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_g = pm.sample(init='adapt_diag')

## we need to change the x-variable, 
## and re-scale the variance, etc, to 

## our x is watershed hops, XS or x
## our y is BC dist

plt.scatter (XS,YS)

## toy plot to visualize priors:

plt.close('all')
fig, ax = plt.subplots()
n=200
kappa=0.3
mu = xx/(xx+kappa)
xx = np.linspace(0,4,n)
mu = pd.Series(xx/(xx+kappa))
## after doing a little arithmatic, let's try 
gamma = pd.Series(np.random.normal(loc=-1.6, scale=0.1, size=n))
## delta is hard, and the crux of what needs to change here.
delta = pd.Series(np.random.normal(loc=-0.2, scale=0.05, size=n))
ee = np.power(np.e, gamma + delta*xx)
error = ee.apply(lambda x:  np.random.normal(loc=0, scale=x))
ax.scatter(xx,mu+error)
ax.plot(xx,mu)
## something like that.
## use these for priors, try model  

xx = psubpHopsTriU
with pm.Model() as model_watershed:
    γ = pm.Normal('γ', mu=-1.6, sigma=0.1) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.2, sigma=0.05) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=0.3, sigma=0.1) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_g = pm.sample(init='adapt_diag')

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)
plt.scatter(XS,YS)

az.plot_trace(trace_g, var_names = ['γ','δ','κ'])

aa = pd.DataFrame({ "μ_m": trace_g['μ'].mean(0),
                    "ε_m": trace_g['ε'].mean(0),
                    "sheds": psubpHopsTriU,
                    "bcDist": bcDist})
model_Sort = aa.sort_values(by=['sheds'])

ppc = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_watershed)

fig, ax = plt.subplots(figsize=(8,8))

ax.scatter(model_Sort.sheds, model_Sort.bcDist, c='b', s=5)

model_Sort.sheds

ax.plot(model_Sort.sheds, 
        model_Sort.μ_m, 
        c='r', 
        linestyle= "solid", 
        linewidth=4)

## this doesn't work, probably because of the binning of data ###
az.plot_hpd(psubpHopsTriU, ppc['y_pred'], credible_interval=0.5, color='orange', ax=ax)
az.plot_hpd(psubpHopsTriU, ppc['y_pred'],  color='orange', ax=ax)
#################################################################

X, Y = model_Sort.sheds.to_numpy().reshape(-1,1), bcDist.reshape(-1,1)

ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=4, linestyle= "dotted")

## check the fit
ppc = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_watershed)
az.r2_score(bcDist, ppc['y_pred'])
## it's good, r2=.68

## how can we visualize this? 

ppc['y_pred'].shape

fig, ax = plt.subplots()
ax.scatter(model_Sort.sheds, model_Sort.bcDist, c='b', s=5)

## replot above nice violin plot, then...
ppc['y_pred'].mean(axis=0).shape
plt.scatter(xx+1,ppc['y_pred'].mean(axis=0))
## what about plotting the function itself, using posteriors

model_Sort

## the model performs well, but poorly in the first watershed
## so, what do we need to visualize this?
## also do we need to fix this? 

## maybe relax the mean and allow a simple linear model, 

plt.close('all')
fig, ax = plt.subplots()
n=200
kappa=0.3
xx = np.linspace(0,4,n)
m = 1/50
b = 0.8
mu = m*xx + b
ax.scatter(xx, mu)
ax.set_ylim(0,1.5)


## we want a constricting variance. Maybe let's try the 
## assymetric normal:

alpha=-2
γ = 0.2
δ = -0.04
α = 0.8
β = 0.02
mu = β*xx + α
ε = γ + δ*xx
error = stats.skewnorm.rvs([alpha], loc=0, scale=ε)
plt.close('all')
fig, ax = plt.subplots()
ax.scatter(xx, mu+error)
ax.plot(xx, mu)

## looks reasonable. Build a model?

xx = psubpHopsTriU 
with pm.Model() as model_skewNormal:
    γ = pm.Normal('γ', mu=0.2, sigma=0.1) ## initial spread of variance 
    δ = pm.Normal('δ', mu=-0.04, sigma=0.01) ## slope, = rate of tightening of variance
    α = pm.Normal('α', mu=0.8, sigma=0.2) ## intercept of mu, higher than 0.8 because skewNormal
    β = pm.Normal('β', mu=0.02, sigma = 0.01) ## slope of mu
    a = pm.Normal('a', mu=-2.0, sigma = 0.5) ## skewness parameter
    μ = pm.Deterministic('μ', β*xx + α)
    ε = pm.Deterministic('ε', γ + δ*xx)
    y_pred = pm.SkewNormal('y_pred', mu=μ, sd=ε, alpha=a, observed=bcDist)
    trace_sn = pm.sample(init='adapt_diag')


## how does that look?

aa = pd.DataFrame({ "μ_m": trace_sn['μ'].mean(0),
                    "ε_m": trace_sn['ε'].mean(0),
                    "sheds": psubpHopsTriU,
                    "bcDist": bcDist})
model_Sort = aa.sort_values(by=['sheds'])

trace_sn['δ'].mean()

fig, ax = plt.subplots()
ax.scatter(model_Sort.sheds, model_Sort.bcDist, c='b', s=5)
ax.scatter(model_Sort.sheds, model_Sort.μ_m, c='r', s=5)

ax.plot(model_Sort.sheds, 
        model_Sort.μ_m, 
        c='r', 
        linestyle= "solid", 
        linewidth=4)

az.plot_hpd(psubpHopsTriU, ppc['y_pred'], credible_interval=0.5, color='orange', ax=ax)

az.plot_hpd(psubpHopsTriU, ppc['y_pred'],  color='orange', ax=ax)

ppc_lin = pm.sample_posterior_predictive(trace_g, samples=2000, model=model_watershed)
az.r2_score(bcDist, ppc_lin['y_pred'])

## ? won't plot. sleep.
## so what do we need here? 
 
## I think what is missing is the violin-style plot, as done with the raw data.
## my confusion is how to visualize the variation. 
## should be governed by our posterior ε:

dir(trace_sn)

trace_sn.varnames

## we included ε in our sorted data frame, just for this reason.
## sometimes I am smarter than I think. Or luckier. Probably just luckier.

model_Sort.head()

model_Sort.tail()

model_Sort.ε_m

model_Sort

aa = model_Sort.groupby(by='sheds')
## that worked, right?
(aa.get_group(0).sheds == 0).all()
(aa.get_group(1).sheds == 1).all()
## yep
## as far as μ and ε, are these the same?
aa.agg('min') 
aa.agg('max')
## indeed...

## okay, but this gives a single
## error term that won't reflect the 
## assymetric dist (the skew) that we've
## intentionally introduced...

## really we need the HPDs for each of these
## spots...but the plot_hpd doesn't seem 
## geared for this, fails above

## backing up, we should be able to plot the
## modeled posterior distribution using the 
## scipy.stats dist functions themselves:

## so for sheds=1, our skewNorm dist should look like:  

aa = model_Sort.groupby(by='sheds')
skewNormPostSummary = aa.agg('mean') 

skewNormPostSummary

skewNormPostSummary


model_Sort.head()

model_Sort

help(stats.skewnorm)

stats.skewnorm.rvs([alpha], loc=0, scale=ε)

alpha=-1
loc=0.9
scale=0.1

## computing the mean for a skew is a bit complication, it's not μ.
## see here for the actual mean: https://docs.pymc.io/api/distributions/continuous.html#pymc3.distributions.continuous.SkewNormal

## I think we can brute force it, right? Generate a bunch of values 
## that represent our dist:

## our "x" is really our y, and it runs from 0 to ~1.5:
x = np.linspace(0,1.5,200)
alpha=-5
loc=0.9
scale=0.1
pdf = stats.skewnorm.pdf(x, [alpha], loc=loc, scale=scale)
plt.close('all')
fig, ax = plt.subplots()
ax.plot(x, pdf, label=r'$\mu$ = {}, $\sigma$ = {}, $\alpha$ = {}'.format(loc, scale, alpha))
ax.vlines(loc, ymin=0, ymax=8, color='r', linestyles='solid')
ax.vlines(skewnorm.mean(alpha, loc=loc, scale=scale), 
        ymin=0, ymax=8, color='k', linestyles='dashed')


aa = np.round(pdf, 2)
bb = aa[aa > 0]
stats.mode(bb)


## same with the binned mode. That long tail makes a difference, I guess. 

## can we calculate the mean analytically, does it agree with our empirical version?

## our mean is supposed to be:

## μ + ε*(2/np.pi)**(1/2)*α/(1+α**2)

## so in the case of sheds = 0:

skewNormPostSummary
mu = skewNormPostSummary.loc[0]['μ_m'].copy()
ee = skewNormPostSummary.loc[0]['ε_m'].copy()
alpha = trace_sn['a'].mean()  
skme0 = mu + ee * (2/np.pi)**(1/2) * alpha/((1+alpha**2)**(1/2))

print(skme0) ## 0.8497. 
## That seems more reasonable, fits the observed data. but not the calculated mean...
pdf.mean() ## 0.663333
## maybe this has something to do numerous zeros, rounding errors, etc
## just found a way to calculate moments in the scipy stats package:
skewnorm.stats([alpha], loc=mu, scale=ee, moments='m') ## yup, 0.8497
## awesome. 
## this also works:
skewnorm.mean(alpha, loc=mu, scale=ee)

## add this to our summary dataframe:

aa = model_Sort.groupby(by='sheds')
skewNormPostSummary = aa.agg('mean') 
skewNormPostSummary['postMean'] = pd.Series(skewnorm.mean(alpha, 
                                    loc=skewNormPostSummary['μ_m'], 
                                    scale=skewNormPostSummary['ε_m']))

skewNormPostSummary['μ_m']
skewNormPostSummary['ε_m']

skewNormPostSummary

## okay, what else can we do out of the box?
## we need a hpdi of some kind...this should
## be simple using the ppf or cdf functions

stats.skewnorm.cdf(1.0,[alpha], loc=mu, scale=ee)

stats.skewnorm.ppf(.05,[alpha], loc=mu, scale=ee)

stats.skewnorm.ppf(.95,[alpha], loc=mu, scale=ee)
stats.skewnorm.ppf(.99,[alpha], loc=mu, scale=ee)

## actually, not really, of course. 

## can we make a ton of points, then run the scipy.stats.bayes_mvs
## function on it?

aa = stats.skewnorm.rvs([alpha], loc=mu, scale=ee, size=4000)

stats.bayes_mvs(aa, alpha=0.9)
## nah. It is just trying to estimate the particular moments, so the mean
## but maybe we can feed this to az? 

## this works:
az.plot_posterior({'zoop':stats.skewnorm.rvs([alpha], loc=mu, scale=ee, size=4000)})
## but can I get the values themselves?

## 95%
pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], loc=mu, scale=ee, size=4000),
                credible_interval=0.95)

## how can we use this to make a series for our dataframe?
alpha = trace_sn['a'].mean()  

skewNormPostSummary['hpd95'] = skewNormPostSummary.apply(lambda x: 
                pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], 
                                                    loc=x['μ_m'], 
                                                    scale=x['ε_m'], 
                                                    size=4000),
                                                    credible_interval=0.95),
                        axis=1)

## check with ax

loc=0.981405
scale=0.135400
size=4000
plt.close('all')
aa = stats.skewnorm.rvs([alpha], 
                       loc=loc, 
                       scale=scale, 
                       size=size)
pm.stats.hpd(aa, credible_interval=0.94)
az.plot_posterior({'':aa},credible_interval=0.95)

## the credible intervals never quite agree between pm and arviviz...
## even using exactly the same sample... ?

## close enough. now what?


skewNormPostSummary

n=0
aa = az.plot_posterior({f'{n}':stats.skewnorm.rvs([alpha], 
                                loc=skewNormPostSummary['μ_m'][n], 
                                scale=skewNormPostSummary['ε_m'][n], 
                                size=4000)}, alpha=0.95)
skewNormPostSummary

## the hpd given by the ax.plot_posterior are different. Why? if we do this
## individually, still happens?

pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], 
                                    loc=x['μ_m'], 
                                    scale=x['ε_m'], 
                                    size=4000),
                                    credible_interval=0.95))


## 50%
pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], loc=mu, scale=ee, size=4000),
                credible_interval=0.50)


## I think that's what we needed. add these to the summary dataframe?

alpha = trace_sn['a'].mean()  
aa = model_Sort.groupby(by='sheds')
skewNormPostSummary = aa.agg('mean') 
skewNormPostSummary['postMean'] = pd.Series(skewnorm.mean(alpha, 
                                    loc=skewNormPostSummary['μ_m'], 
                                    scale=skewNormPostSummary['ε_m']))
skewNormPostSummary.columns = ['μ_m', 'ε_m', 'meanBCobserved', 'postMean']
skewNormPostSummary['hpd95'] = skewNormPostSummary.apply(lambda x: 
                pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], 
                                                    loc=x['μ_m'], 
                                                    scale=x['ε_m'], 
                                                    size=4000),
                                                    credible_interval=0.95),
                        axis=1)
skewNormPostSummary['hpd50'] = skewNormPostSummary.apply(lambda x: 
                pm.stats.hpd(ary=stats.skewnorm.rvs([alpha], 
                                                    loc=x['μ_m'], 
                                                    scale=x['ε_m'], 
                                                    size=4000),
                                                    credible_interval=0.50),
                        axis=1)


skewNormPostSummary

## how can we visualize this?

## the new plan is to do a violin of samples from the skew normal distribution 
## at each watershed hop 

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)
XSjitted = (XS
            .apply(np.random.normal, args=(0.03,))
            .apply( lambda x: x+1)
            )

bb = skewNormPostSummary.apply(lambda x: 
                    stats.skewnorm.rvs([alpha], 
                        loc=x['μ_m'], 
                        scale=x['ε_m'], 
                        size=40000).tolist(),
                    axis=1).to_list()
plt.close('all')
fig, ax = plt.subplots()
ax.violinplot(bb, showmeans=True, showextrema=False, label='zoop')
ax.scatter(XSjitted, YS, alpha=0.1, color='red')
ax.set_ylim(0,1.2)
_=ax.set_xticklabels(['',0,1,2,3,4])
## that works...add points?
ax.set_title('Overall Microcuenca Turnover')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_xlabel('distance in watershed hops')

## huh, our model doesn't actually predict much of a constriction 

## what if we repeat all that without constricting variance?:
## in variation with distance. But it does explain a lot of 
## variance.
## Is it any better than a horizontal line?

## dunno. But how about a simpler model without constricting variance:

xx = psubpHopsTriU 
with pm.Model() as model_skewNormalNC:
    α = pm.Normal('α', mu=0.8, sigma=0.2) ## intercept of mu,
    β = pm.Normal('β', mu=0.02, sigma = 0.01) ## slope of mu
    a = pm.Normal('a', mu=-2.0, sigma = 0.5) ## skewness parameter
    μ = pm.Deterministic('μ', β*xx + α)
    ε = pm.Normal('ε', mu=0.1, sd=0.05)
    y_pred = pm.SkewNormal('y_pred', mu=μ, sd=ε, alpha=a, observed=bcDist)
    trace_notConstrict = pm.sample(init='adapt_diag')

az.plot_trace(trace_notConstrict, ['α', 'β'])
az.plot_trace(trace_notConstrict, ['α', 'β'])

az.plot_posterior(trace_notConstrict)

aa = pd.DataFrame({ "μ_m": trace_notConstrict['μ'].mean(0),
                    "ε_m": trace_notConstrict['ε'].mean(0),
                    "sheds": psubpHopsTriU,
                    "bcDist": bcDist})
model_SortNC = aa.sort_values(by=['sheds'])
aa = model_SortNC.groupby(by='sheds')
skewNormNCPostSummary = aa.agg('mean') 
skewNormNCPostSummary['postMean'] = pd.Series(skewnorm.mean(alpha, 
                                    loc=skewNormNCPostSummary['μ_m'], 
                                    scale=skewNormNCPostSummary['ε_m']))
skewNormNCPostSummary['μ_m']
skewNormNCPostSummary['ε_m']


ppc_NC = pm.sample_posterior_predictive(trace_notConstrict, samples=2000, model=model_skewNormalNC)
az.r2_score(bcDist, ppc_NC['y_pred']) 
## r2=.01 shite

ppc_skew = pm.sample_posterior_predictive(trace_sn, samples=2000, model=model_skewNormal)
az.r2_score(bcDist, ppc_skew['y_pred'])
## r2=.02 also shite

ppc_skew['y_pred']

plt.scatter(psubpHopsTriU, bcDist)

## okay, unless we made a major mistake, there isn't a model here 
## to be built.
## it is possible that we made a major mistake, but can't find it

## the figure is still cool, and basically shows that every 
## watershed is really different. leave it at that. 

## what we might check while our heads are in this is whether 
## simple linear model might be a better fit than an assymptote,
## for our original physical distance (not watershed) comparisons:

specObs = pd.read_csv('specObs.csv', index_col='PsubP').sort_index()
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP').sort_index()
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
pts = (gpd.read_file('GIS/ana30meterPlots.geojson')
        .set_index('PsubP')
        .sort_index()
      )
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)

lw=5
fig, ax = plt.subplots(figsize=(10,10))
ax.scatter(physDist, bcDist, s=100, alpha=0.3)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=lw)
ax.set_title('Overall Turnover')
## any lines from our function work well here?
x = pd.Series(np.linspace(0,3700,500))
y1 = x.apply(lambda x: x/(200+x))
ax.plot(x,y1, color='red', linewidth=lw)

## so to use a linear model with decreasing variance?
## skewed normal for the variance...

xx = physDist.copy()
with pm.Model() as model_skewNormal:
    a = pm.Normal('a', mu=-2.0, sigma = 0.5) ## skewness parameter
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    α = pm.Normal('α', mu=0.9, sigma=0.2) ## intercept of mu,
    β = pm.Normal('β', mu=0.0001, sigma = 0.00001) ## slope of mu
    μ = pm.Deterministic('μ', α + β*xx )
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.SkewNormal('y_pred', mu=μ, sd=ε, alpha=a, observed=bcDist)
    trace_sn = pm.sample(init='adapt_diag')

ppc_skew['y_pred'].mean(axis=0).shape

aa = ppc_skew['y_pred'].mean(axis=0)
ax.plot(xx,aa, 'k')



model_skewNormal

alpha = trace_sn['a'].mean()
aa = pd.DataFrame({ "μ_m": trace_sn['μ'].mean(0),
                    "ε_m": trace_sn['ε'].mean(0),
                    "physDist": physDist,
                    "bcDist": bcDist})
model_skewSort = aa.sort_values(by=['physDist'])
## to get our means:

getSkewMean(mu,ee,a)

def getSkewMean(mu,ee,alpha):
    mean = mu + ee * (2/np.pi)**(1/2) * alpha/((1+alpha**2)**(1/2))
    return(mean)
model_skewSort['skewMean'] = model_skewSort.apply(lambda x: getSkewMean(mu=x['μ_m'].item(), 
                                           ee=x['ε_m'].item(), 
                                           alpha=trace_sn['a'].mean()),
                        axis=1)


plt.close('all')
fig, ax = plt.subplots()
ax.scatter(model_skewSort.physDist,model_skewSort.bcDist, c='b', s=5)
ax.plot(model_skewSort.physDist, 
        model_skewSort.skewMean, 
        c='r', 
        linestyle= "solid", 
        linewidth=4)
az.plot_hpd(physDist, ppc['y_pred'], credible_interval=0.5, color='orange', ax=ax)
az.plot_hpd(physDist, ppc['y_pred'],  color='orange', ax=ax)

X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=4, linestyle= "dotted")

## okay, that worked pretty well. Let's update notebook, and stop futzing with models?

ppc_skew = pm.sample_posterior_predictive(trace_sn, samples=2000, model=model_skewNormal)
az.r2_score(bcDist, ppc_skew['y_pred'])

## 0.06?? The fit looks great. What's the issue? 

## or not. Jeezus, I read that wrong. R2 is .06. I think something is wrong. We need another
## way to compare...p-values?

## we have the same dataset, physdist as our x, BCdist as our y. 

## So the plan (1) bring up old asymptote model, make sure R2 works
## if not, there is a code error somewhere...

## (2) let's compute some pvalues? What do we use?

## for both we need the old asymptote, brought back to life:

## asymptote:

xx = physDist.copy()
with pm.Model() as model_asymp:
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_asymp = pm.sample(init='adapt_diag')
 

az.plot_trace(trace_asymp, var_names = ['γ','δ','κ'])
trace_asymp['γ'].mean()
trace_asymp['δ'].mean()
trace_asymp['κ'].mean()

aa = pd.DataFrame({ "μ_m": trace_asymp['μ'].mean(0),
                    "ε_m": trace_asymp['ε'].mean(0),
                    "physDist": physDist,
                    "bcDist": bcDist})
asympSum = aa.sort_values(by=['physDist'])

asymp_pp = pm.sample_posterior_predictive(trace_asymp, samples=2000, model=model_asymp)

fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(asympSum.physDist, asympSum.bcDist, c='b', s=5)
ax.plot(asympSum.physDist, 
        asympSum.μ_m, 
        c='r', 
        linestyle= "solid", 
        linewidth=4)
az.plot_hpd(physDist, asymp_pp['y_pred'], credible_interval=0.5, color='orange', ax=ax)
az.plot_hpd(physDist, asymp_pp['y_pred'],  color='orange', ax=ax)
X, Y = physDist.reshape(-1,1), bcDist.reshape(-1,1)
ax.plot( X, LinearRegression().fit(X, Y).predict(X), c='k', linewidth=4, linestyle= "dotted")

az.r2_score(bcDist, asymp_pp['y_pred'])
## yeah, same. R2=.53

## linear:
xx = physDist.copy()
with pm.Model() as model_linSkew:
    a = pm.Normal('a', mu=-2.0, sigma = 0.5) ## skewness parameter
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    α = pm.Normal('α', mu=0.9, sigma=0.2) ## intercept of mu,
    β = pm.Normal('β', mu=0.0001, sigma = 0.00001) ## slope of mu
    μ = pm.Deterministic('μ', α + β*xx )
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.SkewNormal('y_pred', mu=μ, sd=ε, alpha=a, observed=bcDist)
    trace_linSkew = pm.sample(init='adapt_diag')

linSkew_pp = pm.sample_posterior_predictive(trace_linSkew, samples=2000, model=model_linSkew)
az.r2_score(bcDist, linSkew_pp['y_pred'])

## gotta sort it by X for nice plotting
aa = pd.DataFrame({ "μ_m": trace_linSkew['μ'].mean(0),
                    "ε_m": trace_linSkew['ε'].mean(0),
                    "physDist": physDist,
                    "bcDist": bcDist})
linSkewSum = aa.sort_values(by=['physDist'])
## mu isn't our mean anymore, with the skew
## to get the mean:

def getSkewMean(mu,ee,alpha):
    mean = mu + ee * (2/np.pi)**(1/2) * alpha/((1+alpha**2)**(1/2))
    return(mean)

linSkewSum['skewMean'] = linSkewSum.apply(lambda x: getSkewMean(mu=x['μ_m'].item(), 
                                           ee=x['ε_m'].item(), 
                                           alpha=trace_linSkew['a'].mean()),
                        axis=1)

fig, ax = plt.subplots(figsize=(8,8))
ax.scatter(linSkewSum.physDist, linSkewSum.bcDist, c='b', s=5)
az.plot_hpd(physDist, linSkew_pp['y_pred'], credible_interval=0.5, color='orange', ax=ax)
az.plot_hpd(physDist, linSkew_pp['y_pred'],  color='orange', ax=ax)
#ax.plot(physDist, linSkew_pp['y_pred'].mean(axis=0), 'ok') ## messy
## use the analytical mean instead:
ax.plot(linSkewSum['physDist'], linSkewSum['skewMean'], 'k')

## ok, so now how do we get some pvalues?

## seems like we want the distance between the oberved pt and 
## the predicted from each model...

## our predicted values from the asymp model are here:

asympPredMean = asymp_pp['y_pred'].mean(axis=0)
linPredMean = linSkew_pp['y_pred'].mean(axis=0)

## how far, on average, are these predictions from the observed?

## plot the dists of the difference:
aa = asympPredMean - bcDist
bb = linPredMean - bcDist
plt.close('all')

fig, ax = plt.subplots()
az.plot_kde(aa, plot_kwargs={'color':'b'}, label='asymptote model', ax=ax)
az.plot_kde(bb, plot_kwargs={'color':'r'}, label='linear model', ax=ax)
ax.vlines(0,0,5.5,linestyles='dashed', color='k', label='Observed value')

plt.close('all')

fig, ax = plt.subplots()
az.plot_kde(asympPredMean, plot_kwargs={'color':'b'}, label='asymptote model', ax=ax)
ax.vlines(bcDist.mean(),0,10,linestyles='dashed', color='k', label='Observed value')
fig, ax = plt.subplots()
az.plot_kde(linPredMean, plot_kwargs={'color':'r'}, label='linear model', ax=ax)
ax.vlines(bcDist.mean(),0,16,linestyles='dashed', color='k', label='Observed value')
ax.vlines(0,0,5.5,linestyles='dashed', color='k', label='Observed value')
  
## that looks good, but how do we convert to a pvalue? 
## we want to compute the probability that a model yields
## a more extreme (higher) value than the observed:

sum(asympPredMean > bcDist) / len(bcDist)  ## p=0.49
sum(linPredMean > bcDist) / len(bcDist) ## p=0.4109


## okay, well, stick with an asymptote model? 
## any other good checks we can do? 

## could try bayes factors...
## meh, really need to write this damn paper. 
## nobody cares about this model comparison. Just 
## write. 

## update notebook, sleep.

############### fix accumulation curves ############

import numpy as np
import pandas as pd

## somewhere I think I made an error in my arithmatic.
## above I say the subplot size is 0.09 ha. I'm not 
## sure where I got that. It looks like each plot had 
## a 30m diameter. I think I misunderstood the shape
## of the plot, and assumed this meant a 30x30=900m2 plot. 
## Which is 900/100000

900/10000 #= 0.09 

## so that's where that number came from, I guess. 

## really, it is a circle, so:

np.pi * (30/2)**2 #706.86 m2. 

np.pi * (30/2)**2 / 10000 ## 0.0707 ha

## so we're a bit off on our calculations. Can these be fixed in the notebook?
pi * (30/2)^2 / 10000 ## 0.0707 ha

## if we want a 5 ha primary tick mark:

## 5 ha = how many subplots:

pArea = np.pi * (30/2)**2 / 10000

5 / pArea ## 70.74

## 7

## notebook corrected. 


###### plot posteriors over priors ########

## it is recommended by Hobbs and Hooten to plot 
## the shift (if any) from prior distributions to 
## posteriors. I think the main instance where 
## we need this would be our Km from our 
## asymptotic model, this is the most ecologically
## relevant stat, with priors generated from the 
## lit. 

## so back up, rebuild the model:

specObs = pd.read_csv('specObs.csv', index_col='PsubP').sort_index()
envOnly = pd.read_csv('envOnly.csv', index_col='PsubP').sort_index()
subParcelComm = pd.read_csv("subParcelComm.csv", index_col='PsubP')
pts = (gpd.read_file('GIS/ana30meterPlots.geojson')
        .set_index('PsubP')
        .sort_index()
      )
d = {'X': pts.geometry.x, 'Y': pts.geometry.y}
physDist = sp.distance.pdist(pd.DataFrame(data=d), metric='euclidean')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')

xx = physDist.copy()
with pm.Model() as model_asymp:
    γ = pm.Normal('γ', mu=-1.5, sigma = 0.5) ## analagous to initial spread of variance (natural log of)
    δ = pm.Normal('δ', mu=-0.0006, sigma=0.005) ## slope, rate of tightening of variance
    κ = pm.Normal('κ', mu=200, sigma = 10) ## same old k
    μ = pm.Deterministic('μ', xx/(xx+κ))
    ε = pm.Deterministic('ε', np.power(np.e, γ + δ*xx))
    y_pred = pm.Normal('y_pred', mu=μ, sd=ε, observed=bcDist)
    trace_asymp = pm.sample(init='adapt_diag')

az.plot_trace(trace_asymp, var_names = ['γ','δ','κ'])

## works, great. Now to plot our posterior distribution of Km, 

trace_asymp.varnames

trace_asymp['κ'].shape

plt.close('all')

fig, ax = plt.subplots(figsize=(6,6))
az.plot_kde(trace_asymp['κ'], plot_kwargs={'color':'b'}, label='asymptote model', ax=ax) 
## and  to show our prior on there:

kapPrior = norm(loc = 200, scale = 10) ## norm from scipy stats
x = np.linspace(kapPrior.ppf(0.01),kapPrior.ppf(0.99), 100)
ax.plot(x, kapPrior.pdf(x),
       'r-', lw=2, alpha=0.6, label='norm pdf')
ax.set_xlabel(f'Km distance (m)')
ax.legend()

##  can we get the standard dev of our posterior? 
az.summary(trace_asymp, var_names=['κ', 'γ','δ'])

####### figure out boundary decision for forests  ########

envOnly = pd.read_csv('envOnly.csv', index_col='PsubP')
landUseForesttypeDF = pd.read_csv('landUseForesttypeDF.csv', index_col='site')
elevLUDF = pd.concat([landUseForesttypeDF, envOnly], axis=1)[['landUse','forestType','elevacion']]
df = elevLUDF.query("forestType > 2")

forestTypeColDict = {
                      1:'k',
                      2:'r',
                      3:'g',
                      4:'b',
                    }

colrs = df['forestType'].apply(lambda x: forestTypeColDict[x])

y_0 = pd.Series(index=df.index, dtype=int)
y_0[df['forestType'] == 3] = 1
x_n = 'elevacion'
x_0 = df[x_n].values
x_c = x_0 - x_0.mean()

with pm.Model() as model_0:
    α = pm.Normal('α', mu=0, sd=10)
    β = pm.Normal('β', mu=0, sd=10)
    μ = α + pm.math.dot(x_c, β)
    θ = pm.Deterministic('θ', pm.math.sigmoid(μ))
    bd = pm.Deterministic('bd', -α/β)
    yl = pm.Bernoulli('yl', p=θ, observed=y_0)
    trace_0 = pm.sample(2000)

theta = trace_0['θ'].mean(axis=0)
idx = np.argsort(x_c)
fig, ax = plt.subplots(figsize=(8,8))
ax.plot(x_c[idx], theta[idx], color='k', lw=3, ls='--')
ax.vlines(trace_0['bd'].mean(), 0, 1, color='k')
bd_hpd = az.hpd(trace_0['bd'])
ax.fill_betweenx([0, 1], bd_hpd[0], bd_hpd[1], color='k', alpha=0.5)
ax.scatter(x_c, np.random.normal(y_0, 0.02),
            marker='.', s= 200, color=colrs)

az.plot_hpd(x_c, trace_0['θ'], color='y')
ax.set_xlabel(x_n)
ax.set_ylabel('θ', rotation=0)
# use original scale for xticks
locs, _ = plt.xticks()
plt.xticks(locs, np.round(locs + x_0.mean(), 1))

ppc = pm.sample_posterior_predictive(trace_0, samples=3000, model=model_0)

help(pm.sample_posterior_predictive)

az.r2_score(y_0, ppc['yl'].mean(axis=0))


aa = trace_0['bd'].mean() + x_0.mean()

-49.762  20.871

round((-49.762) + x_0.mean(), 1)

round((-49.762) + x_0.mean())
round((20.871) + x_0.mean())

[ (az.hpd(trace_0['bd'], credible_interval=0.95)[i] + x_0.mean()) for i in [0,1] ] 

az.hpd(trace_0['bd'])[0] + x_0.mean()

az.hpd(trace_0['bd'])[1] + x_0.mean()

az.hpd(trace_0['bd'])[1]

az.hpd(trace_0['bd']) + x_0.mean()

############# calculating MEM contributions to R2 ##########

## MEM1
0.02642861 / 0.19002882

## MEM2 
(0.05221215 - 0.02642861)
(0.05221215 - 0.02642861) /  0.19002882

############# reforestation some extra calculations ########

(87967.35 - 71738.55) / 87967.35 ## 0.18448663055099424

(5210.19 - 5094.72) / 5094.72 ## 0.02266464104013554 

(6919.65 - 6564.96) / 6919.65

99.004610   93.929795


############### mean bc from watershed crossings #############

aa = [
0.8803880579382168,
0.8718442882110651,
0.8910368558226932,
0.881123053179186
]

np.mean(aa)

np.std(aa)

## need to recover our watershed groupings...

subParcelComm = pd.read_csv('subParcelComm.csv', index_col='PsubP')
psubpHopsDF= pd.read_csv('psubpHopsDF.csv', index_col='PsubP')
bcDist = sp.distance.pdist(subParcelComm, metric='brayCurtis')
psubpHopsNP = np.array(psubpHopsDF)
psubpHopsTriU = psubpHopsNP[np.triu_indices(psubpHopsNP.shape[0], k=1)]
X, Y = psubpHopsTriU.reshape(-1,1), bcDist.reshape(-1,1)

XS, YS = pd.Series(psubpHopsTriU), pd.Series(bcDist)
grouped = [ YS[XS == i].to_list() for i in np.unique(psubpHopsTriU) ]
fig, ax = plt.subplots(figsize=(10,10))
## that works...add points?
XSjitted = (XS
            .apply(np.random.normal, args=(0.03,))
            .apply( lambda x: x+1)
            )
ax.scatter(XSjitted, YS, alpha=0.15, color='red')
ax.violinplot(grouped, showmeans=True)
_=ax.set_xticklabels(['',0,1,2,3,4])
ax.set_title('Overall Microcuenca Turnover')
ax.set_ylabel('Bray-Curtis dissimilarity')
ax.set_xlabel('distance in watershed hops')

[ np.mean(i) for i in grouped ]

## is there a way to distinguish among the various groups of 
## comparisons? Can we just view the distributions?....

help(stats.f_oneway)

wat0, wat1, wat2, wat3, wat4 = grouped
stats.f_oneway(wat0, wat1, wat2, wat3, wat4)

## a tukey's test assume equal variance, I think, but we're not 
## too far from that:
tukey = pairwise_tukeyhsd(endog = YS, groups=XS, alpha=0.05)

help(pairwise_tukeyhsd)

print(tukey)

## yeah, well, that is pretty much what I'd think, the only 
## really clear message is that wat0 is different from all
## the others.  

## but I think we're violating some assumptions. Maybe a Kruskal 
## test is what we're looking for?

help(stats.kruskal)

stats.kruskal(wat0, wat1, wat2, wat3, wat4)
## still highly significant, basically same result anova

## and if we want to do a bayesian style multi-comparison?
## i think we would need to explore these distributions 
## more. They look log-normal-ish...might be simple...

## don't have the energy for this. If the editors ask for it,
## will be happy to do it. 

## for now, record the frequentist tests/model in the 
## notebook, and mention briefly in the manuscript. 

## report the mean, sd, sample size of all the groups. 

pd.DataFrame({
"meanBC": [ np.mean(i) for i in grouped ],
"stdDevBC": [ np.std(i) for i in grouped ],
"n": [ len(i) for i in grouped ]
})


## what is the combined average of all the groups besides
## the wat0 group?

zz = [1,2,3]
yy, *xx = zz

len(grouped[1:])


ungrouped = sum(grouped[1:], [])
np.mean(ungrouped)
np.std(ungrouped)


## how do we get std dev?


lengthz


sum(YS)/len(YS)


############## remake figs tables ##########

## here's the list:

fig. 1A overall SAC
fig. 1B 0.5 ha SAC
fig. 1C by-habitat SAC
fig. 2A asymptote bayes model (plus OLS)
fig. 2B linear bayes model (plus OLS)
fig. 3 hierarchical clustering with hab type
fig. 4 NMS of clusters and eco types
fig. 5 physical map
fig. 6 height diagram 
fig. 7 sig MEMs
fig. 8A Juvenile communities by historical land-use
fig. 8B Juvenile communities by ecological state
fig. 9 land cover cotacachi 
fig. 10 land cover Los Cedros
fig. 11 land cover Chontal
fig. 12 Schematic of equilibria

supp. fig. 1 PPC comparison for asymptote vs. linear
supp. fig. 2 Posterior shift for Km
supp. fig. 3 Clusters NMS
supp. fig. 4 predictions from land-use only model 
supp. fig. 5 y-intercepts from land-use only model
supp. fig. 6 logistic regresssion model
supp. fig. 7 y-intercepts combined model
supp. fig. 8 uncorrected juvenile NMS with outlier

table 1 diversity estimators for surveys + half-hectare
table 2 mean BC by watershed dataframe
table 3 tukeys results from by-watershed groupings 
table 4 indicator species analyses
table 5 mem correlations

supp. table 1 species lists (adult and juv, surveys + perm plot)
supp. table 2 by-land-use species diversity estimators

## so go through these and make sure we have something 
## pretty, accurate and useful for each

## fig. 1A overall SAC
## fig. 1B overall SAC
## these are nice enough for the moment, except for the 
## spanish labels. fix in notebook, export as svg.

mu, sd = 3,4

plt.rc('axes', titlesize=20)

plt.close('all')
fig, ax = plt.subplots()
ax.set_title(f"μ = {mu:3.2f}")

plt.rcParams.update(plt.rcParamsDefault)

plt.rcParamsDefault
#ax.set_title(f"μ = {mu:6.0f}")
## fig. 1C by-habitat SAC
## needs title

## fig. 2A asymptote bayes model (plus OLS)
## fig. 2B linear bayes model (plus OLS)
## the legend for these need hand editing, because the alpha issue with the hpds
## plus titles, matching axes, etc

## fig. 3(A,B?) hierarchical clustering, with plot name and type

## ugh, this is a mess. We need to revamp this figure creation:

library(stats)
library(vegan)

sPC <- read.csv('subParcelComm.csv', row.names='PsubP')
sPCBray <- vegdist(sPC)
sPC.ward <- hclust(sPCBray, method='ward.D2')
envOnly <- read.csv('envOnly.csv', row.names='PsubP')
pts <- read.csv('pts.csv', row.names='PsubP')

plot(sPC.ward)
k=4
clustGroups <- rect.hclust(sPC.ward, k=k, border=1:k)

## get habitat type labels for tree

envOnly[,'elevacion']

## to recreat our cluster group membership:

PsubP <- vector()
gr <- vector()
for (i in 1:k){
    cmem.i <- as.numeric(attr(clustGroups[[i]], "names"))
    PsubP <- c(PsubP, cmem.i)
    gr <- c(gr, rep(i, length(cmem.i)))
    }


## ugh, so we need to make sure the beastly combo plotting works, 
## or replace it. We can probably get rid of it and just show the 
## plots of the dendrograms...

lab <- vector(length = length(sPC.ward$labels))
for (i in 1:length(sPC.ward$labels)){
    ind <- which(row.names(envOnly)==sPC.ward$labels[i])
    lab[i] <- as.character(envOnly$habitat[ind])
}
plot(sPC.ward, labels=lab)
clustGroups <- rect.hclust(sPC.ward, k=k, border=1:k)

## print those out as svgs, combine the labels in inksape:

svg('graphics/clustPlotnumbers.svg', width=12)
plot(sPC.ward)
clustGroups <- rect.hclust(sPC.ward, k=k, border=1:k)
dev.off() 

svg('graphics/clustLanduse.svg', width=12)
plot(sPC.ward, labels=lab)
clustGroups <- rect.hclust(sPC.ward, k=k, border=1:k)
dev.off() 

## let's not spend too much time on this. Keep the ecological 
## type figure the other can be 

## fig. 4(A,B?) NMS of clusters and eco types
## I think we need to move the cluster-only NMS to supp figs.
## keep forest-type/habitat NMS

## fig. 5 physical map

anaPt = gpd.read_file('GIS/ana30meterPlots.geojson')
## our cluster info is here:
cGroup = pd.read_csv("cGroup.csv")
## add this cluster info in
anaPt = anaPt.merge(cGroup, on="PsubP")
## make a vector for coloring the clustering results:
cdik={
1:"k",
2:"r",
3:"g",
4:"b"
}
anaPt['clustCols'] = [ cdik[i] for i in anaPt.gr ]
## where is our habitat data?:
envOnly = pd.read_csv('envOnly.csv')
anaPt = anaPt.merge(envOnly, on="PsubP")
## set the habitat symbol
msym_dik={
'BC':'o',
'BS':'p',
'CLB':'s',
'RCA':'^',
'RG':'D',
}
anaPt['habSyms'] = [ msym_dik[i] for i in anaPt.habitat ]

lcPoly = gpd.read_file('GIS/lcPoly.geojson')
hydro = gpd.read_file('GIS/lcRiversStreams.geojson')
dem = rasterio.open("GIS/anaPlotDEM.tif")
paths = gpd.read_file('GIS/PathsLC.geojson')

fig, ax = plt.subplots(figsize=(10,10))
ax.ticklabel_format(useOffset=False, style='plain')
for i in anaPt.habitat.unique():
    anaPtI = anaPt[anaPt['habitat'] ==  i].reset_index()
    anaPtI.plot(color=anaPtI.clustCols,
                marker=anaPtI.habSyms[0],
                ax=ax)

hydro.plot(color='Blue', ax=ax)
paths.plot(color='red', linestyle='--', ax=ax)
lcPoly.boundary.plot(color='purple', linewidth=3, ax=ax)
rasterio.plot.show(dem, ax=ax)
fig, ax = plt.subplots(figsize=(10,10))

ax.imshow(dem, cmap='Greys_r')


## that didn't work, how can we get a color map?

plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
image_hidden = ax.imshow(dem.read()[0])
fig.colorbar(image_hidden, ax=ax)
ax.ticklabel_format(useOffset=False, style='plain')
rasterio.plot.show(dem, ax=ax)
## that works, but screws up the coordinate system so 
## that our geopandas won't plot over it.
## we could fix that programatically, but I think 
## it won't be elegant... let's just fix the resulting
## png in gimp or inkscape. 

## so to get the color bar: 
fig, ax = plt.subplots(figsize=(10,10))
image_hidden = ax.imshow(dem.read()[0])
fig.colorbar(image_hidden, ax=ax)
#plt.savefig('graphics/elevColorBar.svg')

## and to get the map:
plt.close('all')
fig, ax = plt.subplots(figsize=(10,10))
ax.ticklabel_format(useOffset=False, style='plain')
hydro.plot(color='Blue', ax=ax)
paths.plot(color='red', linestyle='--', ax=ax)
lcPoly.boundary.plot(color='purple', linewidth=3, ax=ax)
for i in anaPt.habitat.unique():
    anaPtI = anaPt[anaPt['habitat'] ==  i].reset_index()
    anaPtI.plot(color=anaPtI.clustCols,
                marker=anaPtI.habSyms[0],
                markersize=40,
                ax=ax, zorder=2,
                )
rasterio.plot.show(dem, ax=ax, zorder=1)

#plt.savefig('graphics/physMap.svg')

